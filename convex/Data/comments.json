[
  {
    "postTitle": "(Quite) A Few Words About Async",
    "comments": [
      {
        "author": "matklad",
        "content": "<p>Good overview! There’s one high-level thing I want to add here:</p>\n<p>It’s worth making a distinction between programming model and implementation strategy. For example, Go’s programming model with goroutines&amp;channels is supported by green threads in <code>go</code> implementation and by os threads in gccgo implementation. Similarly, modern Java has a single programming model, but lets the user choose between os and green threads implementations on a thread-by-thread basis. Rust has async-await programming model, uses state-machine transformation in compiler (“stackless coroutines”) as the implementation strategy, and allows users to choose between single-threaded or work-stealing executor. </p>\n<p>That’s the big promise of Zig’s new plan for async/await (which might, or might not work out!) — that there’s a single programming model, that can be supported by any of: </p>\n<ul>\n<li>A single, blocking OS thread,</li>\n<li>Blocking OS threads,</li>\n<li>User-space green threads (M:1 or M:N)</li>\n<li>Stackless coroutines (!)</li>\n</ul>\n<p>Although not all algorithms work under all models (some need real concurrency to avoid deadlocks, some rely on absence of parallelism to avoid data races), the idea is that <em>some</em> concurrent algorithms do work under any model (eg, count total number of <code>\\n</code> in two files), and that Zig’s whole-program compilation model actually allows expressing them as such without writing the code twice (again, speculative for now). </p>",
        "createdAt": "2025-07-09 17:29:42-0500",
        "replies": [
          {
            "author": "jmillikin",
            "content": "<blockquote>\n<p>Rust has async-await programming model, uses state-machine transformation in compiler (“stackless coroutines”) as the implementation strategy, </p>\n</blockquote>\n<p>Rust’s standard approach to concurrency is OS threads via the <a href=\"https://doc.rust-lang.org/std/thread/\" rel=\"ugc\"><code>std::thread</code></a> module. <code>async</code> is a less common approach used for cases where OS threads are unavailable (embedded / wasm) or too heavyweight.</p>\n<p>The popularity of <code>async</code> among some parts of the non-embedded crates.io ecosystem is IMO a historical aberration due to people used to working in JavaScript. </p>",
            "createdAt": "2025-07-09 21:16:41-0500",
            "replies": [
              {
                "author": "withoutboats",
                "content": "<p>I’m sure there’s a definition of “some parts” that makes that statement true, but the primary driver of async’s popularity in Rust is high performance network services for which using OS threads for concurrency is cost prohibitive, not mimicking JavaScript. </p>",
                "createdAt": "2025-07-10 03:57:25-0500",
                "replies": [
                  {
                    "author": "jmillikin",
                    "content": "<p>Anecdotally, among crates.io packages I’ve encountered that use <code>async</code> the overwhelming majority of them are doing so because they want some notion of “run this operation in the background and provide a <code>Promise</code>-like handle”. An example that comes to mind is <a href=\"https://crates.io/crates/zbus\" rel=\"ugc\"><code>zbus</code></a>, which due to its use case is almost never going to hit thread-related performance limits. </p>\n<p>IIRC one of the popular HTTP client libraries was also obligate <code>async</code> even if I just wanted to do the moral equivalent of <code>system(\"curl\")</code>, but that was ~a year ago and I can’t remember what it was called.</p>\n<p>Also anecdotal, but I’ve noticed a high correlation between use of <a href=\"https://crates.io/crates/async-trait\" rel=\"ugc\"><code>async-trait</code></a> and unnecessary <code>async</code> – the sorts of high-performance use cases that need to do their own userspace thread scheduling are <em>generally</em> not compatible with the amount of heap allocation implied by <code>async-trait</code>, so its presence in the dependency list is sort of a yellow flag.</p>",
                    "createdAt": "2025-07-10 06:40:40-0500",
                    "replies": [
                      {
                        "author": "withoutboats",
                        "content": "<p>I agree that libraries targeting desktop/CLI applications don’t usually need to use async Rust for performance reasons, though I’d say “run this operation in the Background provide a Promise-like handle” is a pretty reasonable thing for a user to want for reasons that aren’t just because of a connection to JavaScript. It’s genuinely a better API than threads and blocking functions. </p>",
                        "createdAt": "2025-07-10 11:59:30-0500",
                        "replies": []
                      }
                    ]
                  }
                ]
              },
              {
                "author": "matklad",
                "content": "<blockquote>\n<p>The popularity of async among some parts of the non-embedded crates.io ecosystem is IMO a historical aberration due to people used to working in JavaScript. </p>\n</blockquote>\n<p>This feels wildly incorrect. AWS is not a JavaScript shop, and they are (or at least were) footing the significant part of the bill behind Rust async story: </p>\n<p><a href=\"https://www.allthingsdistributed.com/2025/05/just-make-it-scale-an-aurora-dsql-story.html\" rel=\"ugc\">https://www.allthingsdistributed.com/2025/05/just-make-it-scale-an-aurora-dsql-story.html</a></p>\n<p><a href=\"https://aws.amazon.com/blogs/opensource/how-our-aws-rust-team-will-contribute-to-rusts-future-successes/\" rel=\"ugc\">https://aws.amazon.com/blogs/opensource/how-our-aws-rust-team-will-contribute-to-rusts-future-successes/</a></p>\n<blockquote>\n<p>Our team includes leaders in the Rust compiler, language design, and the Tokio stack, and these are the areas where we can make the most impact. </p>\n</blockquote>\n<p>And before that I think a lot of support was coming from Google Fuchsia, also not a JavaScript shop.</p>\n<p>While there’s certainly an “everything MUST be async” meme, the polar opposite of that meme isn’t any closer to reality.</p>",
                "createdAt": "2025-07-10 05:59:43-0500",
                "replies": [
                  {
                    "author": "jmillikin",
                    "content": "<p>I think you might have misread my post. These two facts are not in contradiction:</p>\n<ul>\n<li>\n<p>My claim: a frustratingly high percentage of the packages on crates.io that require <code>async</code> do so unnecessarily, and would be better served by using <code>std::thread</code>. When I look at how their code is written it’s clearly modeled after JavaScript idioms, with straight-line functions using <code>async</code> to perform concurrent operations (e.g. starting 2-3 HTTP requests then waiting for their results).</p>\n</li>\n<li>\n<p>Most (or all) of the feature development for <code>async</code> is funded by Amazon-scale users who have clear and compelling requirements for cooperative userspace scheduling in their high-performance network services. </p>\n</li>\n</ul>\n<p>To be clear, I am happy with <code>async</code> as implemented in the Rust compiler and I agree that it’s important for writing network code at larger scales – if I were writing an <code>nginx</code>-style HTTP server then I would definitely build on top of <code>async</code>. My complaint is aimed at the crates.io package ecosystem for using <code>async</code> in places where it doesn’t make sense. </p>",
                    "createdAt": "2025-07-10 06:53:22-0500",
                    "replies": [
                      {
                        "author": "fanf",
                        "content": "<p>What makes it a specifically JavaScript idiom, as opposed to the obvious way to write async code regardless of the programming language? </p>",
                        "createdAt": "2025-07-10 07:25:46-0500",
                        "replies": [
                          {
                            "author": "jmillikin",
                            "content": "<p>Because the idioms are pretty different between languages.</p>\n<p>In Go it’s idiomatic to use a channel (for single values) or <code>sync.WaitGroup</code> (for multiples), with inlined <code>go func() { ... }()</code> calls to spawn each operation. The <code>context.Context</code> type is used to propagate cancellation, timeout, and dynamically-scoped values (request IDs, etc).</p>\n<p>Java builds its modern concurrency API around thread pools, with <code>ExecutorService</code> providing methods to enqueue closures and get back a handle to wait for completion. There’s a built-in assumption that concurrency is limited to a bounded thread count. </p>\n<p>In Haskell there’s <code>MVar</code>, which is a single-element container that can be either empty or full. Reading an empty <code>MVar</code> blocks until a value is placed into it, like a capacity=1 queue. Cancellation and timeout are handled by being able to raise exceptions in spawned threads, which is “interesting” and pretty different from most other languages that require explicit synchronization for cancellation (e.g. Go’s <code>Context.Done()</code>). </p>\n<p>C/C++ traditionally used OS-specific APIs such as <code>pthread_join()</code> which let you pass a <code>void *</code> back from the thread on exit, and your code makes whatever you want out of it.</p>\n<p>In most of these languages the concurrency is directly exposed. The user has the <em>option</em> to write something higher-level (there are <code>Promise</code>- like libraries for every language mentioned) but fundamentally what the language provides is a way to create a separate call stack and either write into a shared location (<code>MVar</code>, channels) or pass back a value when the thread terminates (<code>Executor</code>, pthreads). </p>\n<hr>\n<p>JavaScript’s unique historical constraints on concurrency prevent it from providing a low-level concurrency API, so the standard includes <code>Promise</code> as its most primitive layer and provides <code>async</code> as a minor syntactic convenience on top of the fundamentally monadic structure. When someone who is used to working in JavaScript writes code in a different language they often reach for <code>Promise</code>-like APIs by instinct. </p>\n<p>Rust happens to be a language where there is no type named <code>Promise</code> in the standard library (it’s called <code>JoinHandle</code>), so those programmers find the similarly named <code>async</code> / <code>Future</code>, which is solving a different problem but has the same(-ish) name. Then they add a dependency on Tokio (etc) because <code>Future</code> by itself doesn’t add concurrency, and now their simple library that could have just been a few calls to <code>std::thread::spawn()</code> has a hard dependency on a userspace thread scheduler.</p>",
                            "createdAt": "2025-07-10 08:21:37-0500",
                            "replies": [
                              {
                                "author": "fanf",
                                "content": "<p>Sorry, I phrased the question badly. I wasn’t asking about concurrency in general, I meant to ask about async in languages that have a feature to automatically turn a function into a state machine with programmer-specified yield points. For example, Python and C# and C++. </p>\n<p>If someone writes concurrent code using C++ coroutines, does that mean they are using JavaScript idioms? Or (in the opposite direction) how can a programmer use Rust async without using JavaScript idioms? </p>",
                                "createdAt": "2025-07-10 10:06:19-0500",
                                "replies": [
                                  {
                                    "author": "jmillikin",
                                    "content": "<blockquote>\n<p>I meant to ask about async in languages that have a feature to automatically turn a function into a state machine with programmer-specified yield points. For example, Python and C# and C++. </p>\n</blockquote>\n<p>I think this question doesn’t make sense. The <code>async</code> keyword is doing different things in those languages. In Rust it desugars to a stack-allocated state machine, in JavaScript it desugars to monadic bind on <code>Promise</code> (i.e. callbacks). Python’s <code>async</code> is (IIRC) built on top of <code>__iter__()</code> which is dynamic in the expected way, and I’m not familiar with C# but I would guess that it’s similar to the other GC’d languages. </p>\n<blockquote>\n<p>If someone writes concurrent code using C++ coroutines, does that mean they are using JavaScript idioms?</p>\n</blockquote>\n<p>I don’t think so? Maybe I don’t understand what you’re asking, but what would be the path by which someone would answer “yes” here? </p>\n<blockquote>\n<p>Or (in the opposite direction) how can a programmer use Rust async without using JavaScript idioms?</p>\n</blockquote>\n<p>Build an API that lifts the I/O up out of the leaf functions, ideally with the event loop being defined by the user. Use <code>std::thread</code> (or a library on top of it) to spawn threads. Use <code>async</code> to interleave I/O operations within each thread. Avoid design decisions that require locals within an <code>async</code> function to be <code>Send + Sync</code> and/or heap-allocated.</p>\n<p>If your library has high-level auxiliary helper functions and their <code>async</code> versions rely on a specific runtime, then put those in a separate crate. This isn’t just for dependency pruning, it also helps verify that your core API and your helpers are cleaved at the right place – if you can’t write <code>httpclient-tokio</code> on top of <code>httpclient</code> then your users won’t be able to self-service their own helper functions. </p>",
                                    "createdAt": "2025-07-10 12:06:36-0500",
                                    "replies": []
                                  }
                                ]
                              }
                            ]
                          }
                        ]
                      },
                      {
                        "author": "matklad",
                        "content": "<p>Indeed, I misread your original point, apologies, and thanks for clarifying!</p>",
                        "createdAt": "2025-07-10 08:09:24-0500",
                        "replies": []
                      }
                    ]
                  }
                ]
              },
              {
                "author": "mk12",
                "content": "<p>I think this take is common - that async is overused where threads would work just fine. But I don’t really get it. I would flip it around - why use threads if I can use async instead? I find it easier to understand code that explicitly manipulates concurrent operations reified as promises/futures. For example the four main combinators in JS - all, allSettled, race, and any - and their equivalents in Rust make more sense to me than doing the same thing with threads and condition variables. I just feel like composing futures is a better, higher level API than coordinating threads. </p>",
                "createdAt": "2025-07-10 14:58:34-0500",
                "replies": []
              }
            ]
          }
        ]
      },
      {
        "author": "toothrot",
        "content": "<p>This was a great write-up. It was a good read, even though I was familiar with most parts. The OCaml part was super interesting. It was my favorite language in college, and I haven’t kept up with the multi core work. </p>",
        "createdAt": "2025-07-09 14:51:48-0500",
        "replies": []
      },
      {
        "author": "asb",
        "content": "<p>Related to the comments about the overheads of M:N scheduling, has anyone seen anything new at all re Google’s long-running work in this area? I think the last pieces I’m aware of are <a href=\"https://lkml.org/lkml/2020/7/22/1202\" rel=\"ugc\">https://lkml.org/lkml/2020/7/22/1202</a> and <a href=\"https://lore.kernel.org/lkml/20230328210754.2745989-1-posk@google.com/\" rel=\"ugc\">https://lore.kernel.org/lkml/20230328210754.2745989-1-posk@google.com/</a>\n</p>",
        "createdAt": "2025-07-10 02:21:52-0500",
        "replies": []
      },
      {
        "author": "icefox",
        "content": "<p>Something new in this area that might change the balance of tradeoffs is userspace interrupts: <a href=\"https://lwn.net/Articles/871113/\" rel=\"ugc\">https://lwn.net/Articles/871113/</a> That LWN article is pretty much everything I know about them though, and is several years old by now; anyone know if they’re being used in practice anywhere, and if they are actually as much of a silver bullet as they seem? </p>\n<p>Edit: I feel like M:N scheduling is a good sweet spot that deserves more play. If done well it’s almost as efficient as async/await, and it’s way easier to reason about. Downside is that doing it well usually means tying a lot of language and library runtime to it, and it also tends to make FFI difficult. I really like it as a 90% solution though. </p>",
        "createdAt": "2025-07-10 12:02:12-0500",
        "replies": []
      },
      {
        "author": "rtpg",
        "content": "<p>\n<span class=\"na\"> [Comment removed by author] </span>\n</p>",
        "createdAt": "2025-07-09 19:38:15-0500",
        "replies": []
      }
    ]
  },
  {
    "postTitle": "A Whirlwind Tutorial on Creating Really Teensy ELF Executables for Linux",
    "comments": [
      {
        "author": "rrampage",
        "content": "<p>Love this tutorial. I keep coming back to this to understand more about creating tiny ELFs. Made <a href=\"https://gist.github.com/rrampage/74586d0a0a451f43b546b169d460cb96\" rel=\"ugc\">a small gist</a> on creating a teensy ELF binary in ARM64 using just the assembler (GNU as) without needing the linker.</p>",
        "createdAt": "2025-07-10 11:00:25-0500",
        "replies": []
      },
      {
        "author": "breadbox",
        "content": "<p>This essay is now old enough to rent a car without a deposit, but to the best of my knowledge all of it still works. Whenever I upgrade to a new kernel, I check that the final 45-byte executable still works, and it always has. (I have had one of the intermediate programs break with updates to Linux, but they were always superficial issues that could be repaired without making significant changes to the essay.)</p>",
        "createdAt": "2025-07-10 12:49:41-0500",
        "replies": []
      }
    ]
  },
  {
    "postTitle": "C3 0.7.3 - Small improvements",
    "comments": [
      {
        "author": "makishimu",
        "content": "<p>Glad to see the work is going on, good for you! In the modern day of several “better C” projects available out there, I’d like to see some kind of more direct comparison with alternatives (I am thinking of Hare, Odin, C2, perhaps Zig). There are “What is C3?” and “Design Goals &amp; Background” articles (doc pages), but they don’t necessarily help me understanding benefits/disadvantages over the alternatives. A marketing person could say “give me selling points” (-: but I’m thinking of something more friendly and objective, e.g. <a href=\"https://prometheus.io/docs/introduction/comparison/\" rel=\"ugc\">Prometheus: Comparison to alternatives</a> or <a href=\"https://wiki.archlinux.org/title/Arch_compared_to_other_distributions\" rel=\"ugc\">Arch compared to other distributions</a>.</p>",
        "createdAt": "2025-07-09 05:14:26-0500",
        "replies": [
          {
            "author": "clerno",
            "content": "<p>There is this: <a href=\"https://c3-lang.org/faq/compare-languages/\" rel=\"ugc\">https://c3-lang.org/faq/compare-languages/</a></p>\n<p>No comparison to C2, because C2 was dead for a long time - up until about a year ago. No comparison to Hare because it has limited platform support, but comparisons to C/C++/Rust/Zig/Jai/Odin/D are there.</p>\n<p>It’s not for marketing, just for comparison.</p>\n<p>Do you find it good enough?</p>",
            "createdAt": "2025-07-09 07:14:30-0500",
            "replies": [
              {
                "author": "makishimu",
                "content": "<p>Yes, that’s a sort of an article I was looking for! Thank you. It’s a good start, but it certainly could elaborate on the difference beyond plain bullet points. Hyperlink to mentioned C3 feature doc pages too.</p>\n<p>Why is it (and a few other worthwhile pages) found deep down under FAQ, though? I think it should be among the first pages in some overview/about/design/concepts section in the documentation. On a side note, there is a school of thought that FAQ in general isn’t good documentation structure. It feels like a catch-all kitchen sink poorly structured category.</p>\n<p>Mentioning C2 and Hare, and explaining why they are missing, just like you did here, may be nice too. And since you go as far as comparing to Rust, C++, Jai, and D, perhaps consider Carbon and Nim too.</p>\n<p>I was joking about marketing.</p>",
                "createdAt": "2025-07-10 05:23:51-0500",
                "replies": [
                  {
                    "author": "clerno",
                    "content": "<p>Yes, the FAQ was actually kind of a placeholder to make up for the lack of structured tutorials and articles. I am thinking that as the community grows there will be people who can take what is there and mold it into something better over time.</p>\n<p>The actual hierarchy was overhauled a few times from a much simpler “readthedocs”-style page, and that’s obvious if you look at the docs closely with that in mind.</p>\n<p>People also don’t really agree on the priority of such comparisons. Should it be there up front? Or tucked away for those really interested? It depends a lot on where you come from I think. I personally think this is interesting, but others don’t agree.</p>\n<p>And then of course people don’t agree on my characterization. I had angry Zig users telling me I shouldn’t list async for Zig because it was temporarily removed, and I was uninformed and apparently making Zig look bad for doing so(!). So comparisons aren’t always popular!</p>",
                    "createdAt": "2025-07-10 08:54:41-0500",
                    "replies": []
                  }
                ]
              }
            ]
          },
          {
            "author": "srtcd424",
            "content": "<p>I think “how much do I have to relearn” is probably important, particularly for those of us who are older and losing our neuroplasticity. I’ve reached the point/age where I feel I don’t think there’s too many more “new things” I’m going to have capacity to learn, and I decided that I should aim for (1) a good declarative approach to system config management, and (2) a ‘modern’ programming language.</p>\n<p>NixOS has ended up being my pick for (1), I’m still a little unsure about (2). I think I’m going to go with rust, but something like C3 on the side to leverage my existing C experience might be worth it too. (Pretty much everything else changes the syntax too much, at which point I decided I might as well go the whole hog and learn rust anyway.)</p>",
            "createdAt": "2025-07-09 06:11:42-0500",
            "replies": [
              {
                "author": "clerno",
                "content": "<p>You’re very welcome to try it out. I like C very much myself and hope C3 is friendly to people who have a lot of C experience.</p>",
                "createdAt": "2025-07-09 07:38:22-0500",
                "replies": []
              }
            ]
          }
        ]
      },
      {
        "author": "clerno",
        "content": "<p>Full changelist</p>\n<strong>Changes / improvements</strong>\n<ul>\n<li>\n<code>$typefrom</code> now also accepts a constant string, and so works like <code>$evaltype</code>.</li>\n<li>\n<code>$evaltype</code> is deprecated in favour of <code>$typefrom</code>.</li>\n<li>Literal rules have changed, this makes <code>-0xFF</code> now a signed integer.</li>\n<li>Implicitly convert from constant typeid to Type in <code>$Type</code> assignment, and <code>$assignable</code>.</li>\n<li>Make $Type parameters accept constant typeid values.</li>\n<li>Deprecate <code>foo.#bar</code>.</li>\n<li>Allow inference across <code>&amp;&amp;</code> #2172.</li>\n<li>Added support for custom file extensions in project.json targets.</li>\n<li>\n<code>$eval</code> now also works with <code>@foo</code>, <code>#foo</code>, <code>$Foo</code> and <code>$foo</code> parameters #2114.</li>\n<li>\n<code>@sprintf</code> macro (based on the <code>$$sprintf</code> builtin) allows compile time format strings #1874.</li>\n<li>Improve error reports when encountering a broken “if-catch”.</li>\n<li>Add printf format to <code>$assert</code> and <code>$error</code> #2183.</li>\n<li>Make accepting arguments for <code>main</code> a bit more liberal, accepting <code>main(int argc, ZString* argv)</code>\n</li>\n<li>Make <code>$echo</code> and <code>@sprintf</code> correctly stringify compile time initializers and slices.</li>\n<li>Add <code>--sources</code> build option to add additional files to compile. #2097</li>\n<li>Support untyped second argument for operator overloading.</li>\n<li>The form-feed character ‘\\f’ is no longer valid white space.</li>\n<li>Show code that caused unreachable code #2207</li>\n<li>Allow generics over distinct types #2216.</li>\n<li>Support distrinct types as the base type of bitstructs. #2218</li>\n<li>Add hash::sha512 module to stdlib. #2227</li>\n<li>Compile time type assignment (eg <code>$Foo = int</code>) is no longer an expression.</li>\n<li>Add <code>@allow_deprecated</code> attribute to functions to selectively allow deprecated declarations #2223.</li>\n<li>Improve error message on pointer diff #2239.</li>\n<li>Compile-time comparison of constant vectors. #1575.</li>\n<li>$member.get supports bitstructs.</li>\n<li>$member.set for setting members without the *&amp; trick.</li>\n<li>Initial support for #1925, does not affect C compilation yet, and doesn’t try to link etc. Using “–emit-only”</li>\n</ul>\n<strong>Fixes</strong>\n<ul>\n<li>\n<code>-2147483648</code>, MIN literals work correctly.</li>\n<li>Splatting const slices would not be const. #2185</li>\n<li>Fixes to <code>$define</code> handling of binary ops.</li>\n<li>Fixes methodsof to pick up all sorts of extension methods. #2192</li>\n<li>\n<code>--lsp</code> sometimes does not emit end tag #2194.</li>\n<li>Improve Android termux detection.</li>\n<li>Update Android ABI.</li>\n<li>Fixes to <code>@format</code> checking #2199.</li>\n<li>Distinct versions of builtin types ignore @operator overloads #2204.</li>\n<li>@operator macro using untyped parameter causes compiler segfault #2200.</li>\n<li>Make <code>unreachable()</code> only panic in safe mode.</li>\n<li>\n<code>cflags</code> additions for targets was not handed properly. #2209</li>\n<li>\n<code>$echo</code> would suppress warning about unreachable code. #2205</li>\n<li>Correctly format ‘%c’ when given a width. #2199</li>\n<li>Fix to <code>is_array_or_slice_of_char</code> #2214.</li>\n<li>Method on array slice caused segfault #2211.</li>\n<li>In some cases, the compiler would dereference a compile time null. #2215</li>\n<li>Incorrect codegen if a macro ends with unreachable and is assigned to something. #2210</li>\n<li>Fix error for named arguments-order with compile-time arguments #2212</li>\n<li>Bug in AST copying would make operator overloading like <code>+=</code> compile incorrectly #2217.</li>\n<li>\n<code>$defined(#expr)</code> broken with binary. #2219</li>\n<li>Method ambiguity when importing parent module publicly in private submodule. #2208</li>\n<li>Linker errors when shadowing @local with public function #2198</li>\n<li>Bug when offsetting pointers of large structs using ++ and –.</li>\n<li>\n<code>x++</code> and <code>x--</code> works on pointer vectors #2222.</li>\n<li>\n<code>x += 1</code> and <code>x -= 1</code> works properly on pointer vectors #2222.</li>\n<li>Fixes to <code>x += { 1, 1 }</code> for enum and pointer vectors #2222.</li>\n<li>Linking fails on operator method imported as <code>@public</code> #2224.</li>\n<li>Lambda C-style vaargs were not properly rejected, leading to crash #2229.</li>\n<li>Incorrect handling of constant null fault causing compiler crash #2232.</li>\n<li>Overload resolution fixes to inline typedef #2226.</li>\n<li>\n<code>math::overflow_*</code> wrappers incorrectly don’t allow distinct integers #2221.</li>\n<li>Compiler segfault when using distinct type in attribute imported from other module #2234.</li>\n<li>Assert casting bitstruct to short/char #2237.</li>\n<li>\n<a href=\"https://lobste.rs/~tag\" rel=\"ugc\">@tag</a> didn’t work with members #2236.</li>\n<li>Assert comparing untyped lists #2240.</li>\n<li>Fix bugs relating to optional interface addr-of #2244.</li>\n<li>Compiler null pointer when building a static-lib with -o somedir/… #2246</li>\n<li>Segfault in the compiler when using a bitstruct constant defined using a cast with an operator #2248.</li>\n<li>Default assert() message drops parens #2249.</li>\n</ul>\n<strong>Stdlib changes</strong>\n<ul>\n<li>Deprecate <code>String.is_zstr</code> and <code>String.quick_zstr</code> #2188.</li>\n<li>Add comparison with <code>==</code> for ZString types.</li>\n<li>\n<code>is_array_or_slice_of_char</code> and <code>is_arrayptr_or_slice_of_char</code> are replaced by constant <code>@</code> variants.</li>\n<li>\n<code>@pool</code> now has an optional <code>reserve</code> parameter, some minor changes to the temp_allocator API</li>\n<li>io::struct_to_format now supports bitstructs.</li>\n<li>Add <code>String.escape</code>, <code>String.unescape</code> for escaping and unescaping a string.</li>\n</ul>",
        "createdAt": "2025-07-08 17:49:09-0500",
        "replies": []
      }
    ]
  },
  {
    "postTitle": "Closures for Hare",
    "comments": [
      {
        "author": "runxiyu",
        "content": "<p>Technically a community space but I judge this to be an unlikely target for brigading, hence this submission.</p>",
        "createdAt": "2025-07-09 21:10:40-0500",
        "replies": []
      },
      {
        "author": "spc476",
        "content": "<p>I’m wary of closures in a non-GCed language. Do you have to manually release any memory used by the closure? Or is it reclaimed automagically?</p>",
        "createdAt": "2025-07-10 02:10:05-0500",
        "replies": [
          {
            "author": "doug-moen",
            "content": "<p>The proof of concept “only supports stack-allocated closures”, from the OP.</p>",
            "createdAt": "2025-07-10 13:04:49-0500",
            "replies": []
          }
        ]
      }
    ]
  },
  {
    "postTitle": "Computer Scientists Figure Out How To Prove Lies",
    "comments": [
      {
        "author": "fanf",
        "content": "<blockquote>\n<p>The technique, called the Fiat-Shamir transformation, is useful not just in blockchains and cloud computing but also in many other cryptographic applications, such as the key exchanges that safeguard web transactions and encrypt text messages</p>\n</blockquote>\n<p>Huh, I had not heard of that before, and evidently I’m not going to learn how from Quanta because it’s too scared of jargon to write things like TLS or Ed25519.</p>\n<p>I’m going to read Matt Green’s blog posts about this paper instead <a href=\"https://blog.cryptographyengineering.com/2025/02/04/how-to-prove-false-statements-part-1/\" rel=\"ugc\">https://blog.cryptographyengineering.com/2025/02/04/how-to-prove-false-statements-part-1/</a></p>",
        "createdAt": "2025-07-10 10:49:54-0500",
        "replies": []
      }
    ]
  },
  {
    "postTitle": "CoverDrop: Blowing the Whistle Through A News App",
    "comments": [
      {
        "author": "marsavar",
        "content": "<p>Here are a couple of official announcements with more background, if anyone’s interested:</p>\n<ul>\n<li>\n<a href=\"https://www.cam.ac.uk/research/news/whistleblowing-tech-based-on-cambridge-research-launched-by-the-guardian\" rel=\"ugc\">Whistleblowing tech based on Cambridge research launched by the Guardian</a> (cam.ac.uk)</li>\n<li>\n<a href=\"https://www.theguardian.com/gnm-press-office/2025/jun/09/the-guardian-launches-secure-messaging-a-world-first-from-a-media-organisation-in-collaboration-with-the-university-of-cambridge\" rel=\"ugc\">The Guardian launches Secure Messaging, a world-first from a media organisation, in collaboration with the University of Cambridge</a> (theguardian.com)</li>\n</ul>\n<p>Source: I worked on this project, so excuse the shameless plug :-)</p>",
        "createdAt": "2025-07-10 12:21:47-0500",
        "replies": []
      },
      {
        "author": "jmiven",
        "content": "<p>\n<span class=\"na\"> [Comment removed by author] </span>\n</p>",
        "createdAt": "2025-07-10 10:50:29-0500",
        "replies": []
      }
    ]
  },
  {
    "postTitle": "Hare by Example",
    "comments": [
      {
        "author": "incanus",
        "content": "<p>This is nice! However the dark blue text is very difficult to see on the black background in the code examples. I’m not sure if you’re using a dark mode or something that inverts when you see it, but it’s very tough for me.</p>",
        "createdAt": "2025-07-09 14:54:26-0500",
        "replies": [
          {
            "author": "blainsmith",
            "content": "<p>I fixed the contrast issue in light mode.</p>",
            "createdAt": "2025-07-10 06:50:08-0500",
            "replies": []
          },
          {
            "author": "breadbox",
            "content": "<p>I think the dark-blue-on-black is quite readable, as long as the rest of the web page is in dark mode. If you’re viewing the page in light mode, the main page is white, which washes out the eye’s ability to see contrast in the black sections.</p>\n<p>Edit: never mind; it’s more than just the eye. The colors are indeed different depending on the light/dark mode. Under dark mode the color for the shell commands are a simple gray (#B8B8B8), while under light mode they are a deep blue (#261C7A) that is almost impossible to discern.</p>",
            "createdAt": "2025-07-09 15:53:35-0500",
            "replies": [
              {
                "author": "alterae",
                "content": "<p>yeah, link-blue against that backdrop is effectively illegible</p>",
                "createdAt": "2025-07-09 16:04:35-0500",
                "replies": []
              },
              {
                "author": "blainsmith",
                "content": "<p>Ahhh, now I see it. Odd that the shell code for that theme really does not look awesome in light mode. I will get that fixed!</p>",
                "createdAt": "2025-07-09 16:00:50-0500",
                "replies": []
              }
            ]
          },
          {
            "author": "blainsmith",
            "content": "<p>It was the <code>doom-one</code> theme from the chroma tool (<a href=\"https://xyproto.github.io/splash/docs/all.html\" rel=\"ugc\">https://xyproto.github.io/splash/docs/all.html</a>). I can explore other options, but they show up fine for me. You’re the first that raised the contrast issue. I appreciate it!</p>",
            "createdAt": "2025-07-09 15:42:43-0500",
            "replies": [
              {
                "author": "hoistbypetard",
                "content": "<p>This is how it shows up for me, too:</p>\n<p><a href=\"https://pasteboard.co/1RfS7ahqdX8u.png\" rel=\"ugc\">https://pasteboard.co/1RfS7ahqdX8u.png</a></p>\n<p>It happens with Firefox, Chrome, and Safari on Mac. My system appearance is set to light.</p>",
                "createdAt": "2025-07-09 17:11:08-0500",
                "replies": [
                  {
                    "author": "blainsmith",
                    "content": "<p>Thanks for the screenshot. I’ll get this addressed tomorrow!</p>",
                    "createdAt": "2025-07-09 18:00:29-0500",
                    "replies": []
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "author": "st3fan",
        "content": "<p>I would probably use Hare if it were available on macOS.</p>",
        "createdAt": "2025-07-09 17:11:39-0500",
        "replies": [
          {
            "author": "blainsmith",
            "content": "<p>Yeah, the author doesn’t plan to support macOS. They Re strictly sticking to open source operating systems.</p>\n<p>You can also run it on a VM with the correct architecture.</p>",
            "createdAt": "2025-07-09 18:18:32-0500",
            "replies": [
              {
                "author": "giann",
                "content": "<p>Opiniated people exhaust me. Why would you ignore such a large chunk of users? I mean they’re entitled to make any choice they want of course, but this seems a bit much.</p>",
                "createdAt": "2025-07-10 08:20:50-0500",
                "replies": [
                  {
                    "author": "mikedorf",
                    "content": "<p>Avoiding users that are misaligned with your project is a super useful project management strategy imo. It helps balance the costs/benefits of feature support and maintenance.</p>\n<p>Supporting MacOS outright is just an expensive and annoying endeavor for people that don’t care about using MacOS themselves - and that’s not an uncommon thing because there are mountains of software that doesn’t support it and probably never will.</p>\n<p>edit: reading <a href=\"https://harelang.org/documentation/faq.html#will-hare-support-windows-or-macos\" rel=\"ugc\">their FAQs</a> they have an extremely reasonable take: they are not implementing a mac/windows implementation, but the language is standardized so if someone wants to they can go build it.</p>",
                    "createdAt": "2025-07-10 09:29:30-0500",
                    "replies": []
                  },
                  {
                    "author": "andrewrk",
                    "content": "<p>It’s not so much an opinion as it is resisting the pressure to do a bunch of extra work from people who are neither your boss nor your source of income</p>",
                    "createdAt": "2025-07-10 15:08:31-0500",
                    "replies": []
                  }
                ]
              },
              {
                "author": "hoistbypetard",
                "content": "<p>I haven’t followed it closely, and haven’t tried the port myself, but I see from my notebook that the documentation <a href=\"https://github.com/hshq/harelang\" rel=\"ugc\">links an independent Darwin port</a>.</p>",
                "createdAt": "2025-07-09 18:37:22-0500",
                "replies": []
              }
            ]
          },
          {
            "author": "rtpg",
            "content": "<p><a href=\"https://github.com/hshq/harelang\" rel=\"ugc\">https://github.com/hshq/harelang</a> there is this mac fork.</p>\n<p>Of course the main project is making a stance to never officially support this, which is a stance. I understand not wanting to directly support it but I feel like even GNU ends up with Mac +Windows support code in their projects right?</p>",
            "createdAt": "2025-07-09 18:50:24-0500",
            "replies": [
              {
                "author": "nemith",
                "content": "<p>The Hare author also states that hare is intended to be a <a href=\"https://harelang.org/blog/2023-11-08-100-year-language/\" rel=\"ugc\">100-year language</a>. It will be interesting to see if these two goals are in conflict or not in the long term.</p>",
                "createdAt": "2025-07-09 20:17:55-0500",
                "replies": [
                  {
                    "author": "rtpg",
                    "content": "<p>One could bet on unix in general, I just feel like macs are what a lot of people end up with as personal computers (And the especially important demographic for early languages of “the laptop you bring while travelling so you have a 12 hour flight with the laptop on”)</p>",
                    "createdAt": "2025-07-09 22:11:12-0500",
                    "replies": []
                  },
                  {
                    "author": "caleb",
                    "content": "<p>or if the former goal is a precondition for the latter</p>",
                    "createdAt": "2025-07-09 23:17:35-0500",
                    "replies": []
                  },
                  {
                    "author": "zesterer",
                    "content": "<p>It’s a bizarre thing to advertise, to be honest. We don’t <em>want</em> 100-year languages. We already have enough 50-year languages and they’re a major ongoing cost for the entire industry. The goal should be to avoid being so arcane and contrarian and hostile to refactoring that longevity becomes inevitable. The best software tools are the ones I can get rid of.</p>",
                    "createdAt": "2025-07-10 10:33:27-0500",
                    "replies": []
                  }
                ]
              },
              {
                "author": "quasi_qua_quasi",
                "content": "<p>I vaguely remember that the emacs stance was that platform-specific code is fine as long as the feature it implements is also available on free platforms, or something like that.</p>",
                "createdAt": "2025-07-09 21:06:37-0500",
                "replies": [
                  {
                    "author": "danlamanna",
                    "content": "<p>I’m not sure if it’s consistently followed, but that was the basis for the emoji drama several years ago: <a href=\"http://xahlee.info/emacs/misc/emacs_macos_emoji.html\" rel=\"ugc\">http://xahlee.info/emacs/misc/emacs_macos_emoji.html</a>.</p>",
                    "createdAt": "2025-07-09 22:51:13-0500",
                    "replies": []
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "author": "moony",
        "content": "<p>I’d consider the language if it</p>\n<ul>\n<li>Acknowledged threads exist (they’re a fundamental primitive, processes are not sufficient)</li>\n<li>Acknowledged other platforms exist (I do not write code that nobody can then use; and rely on portability)</li>\n</ul>\n<p>As is, though, me writing anything in it would be actively harmful to people’s ability to freely use said code and telling them to “just port all of Hare” isn’t acceptable even if my daily driver is a Linux setup.</p>\n<p>The thing that baffles me is a “systems language” and “C alternative” that then doesn’t support atomics, locks, thread-safety of any kind, etc. Even the lowliest MCU often has two or more cores in this day and age and just doing message passing with decent efficiency at minimum needs locks! (Because no, sockets are not a good solution.)</p>\n<p>If i wrote a library in this, none of the things I do on the daily could even use it due to that. Not even a simple web server.</p>",
        "createdAt": "2025-07-10 09:41:38-0500",
        "replies": [
          {
            "author": "blainsmith",
            "content": "<p>All valid points given what you want in a language. Everyone will have their own opinion on language X. This post was to show some of those features in this one and not to get into debates on what sucks and what doesn’t. I am happy that you have your boundaries and requirements for languages you would choose to write in and I wish you luck in them as long as you enjoy yourself doing it.</p>",
            "createdAt": "2025-07-10 09:49:44-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "janerik",
        "content": "<p>I looked into Hare last year and played around with it. I should update my <a href=\"https://fnordig.de/2024/06/04/playground-for-hare/\" rel=\"ugc\">Hare playground</a> (and actually host it again), then integrate it with those examples. (And yes, that was mostly developed on macOS thanks to the port. Even fixed a few things in the Darwin port)</p>",
        "createdAt": "2025-07-10 02:38:44-0500",
        "replies": [
          {
            "author": "blainsmith",
            "content": "<p>Drew emailed me about Hare by Example giving feedback and mentioned your playground. We should collaborate and make this happen!</p>",
            "createdAt": "2025-07-10 06:10:49-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "mccd",
        "content": "<p>I’ve been writing a bit of C lately. As a language the only real pain to me is the lack of sum types and really painful string handling, so hare really interests me. If they add linear types then it is extremely lucrative.</p>\n<p>AFAIK there’s no formatter nor any lsp support for it yet though. At the very least I’ll wait for a formatter before I start using the language.</p>",
        "createdAt": "2025-07-10 04:00:33-0500",
        "replies": [
          {
            "author": "blainsmith",
            "content": "<p>There is some of that stuff. I use Helix as my editor and it has highlight support, but the LSP is definitely lacking.</p>",
            "createdAt": "2025-07-10 06:13:11-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "osa1",
        "content": "<p>I like the idea of inheriting (they call it “embedding”) struct fields: <a href=\"https://harebyexample.org/struct-sub-typing.html\" rel=\"ugc\">https://harebyexample.org/struct-sub-typing.html</a></p>\n<p>They don’t mention whether you can inherit fields from multiple structs, and what happens if you end up inheriting two fields with the same name. Technically you could allow multiple fields with the same name, but multiple inheritance gets tricky when in <code>S3</code> you inherit from two structs <code>S1</code> and <code>S2</code> which inherit from the same struct <code>S0</code>. You would have to duplicate the fields of <code>S0</code> as the code expecting <code>S1</code> and <code>S2</code> will expect <code>S0</code> fields in locations that may be different relative to <code>S1</code> and <code>S2</code> in <code>S3</code> if you de-duplicate the common inherited fields.</p>\n<p>I’m guessing they just don’t allow multiple inheritance/embedding?</p>",
        "createdAt": "2025-07-10 06:13:23-0500",
        "replies": [
          {
            "author": "blainsmith",
            "content": "<p>I actually haven’t tried that, but considering that Hare isn’t an OO language I’d suspect that inheritance the way you described would not work. Their more basic embedding is the “good enough” approach for most things most likely.</p>",
            "createdAt": "2025-07-10 06:17:20-0500",
            "replies": []
          },
          {
            "author": "typesanitizer",
            "content": "<p>This seems similar to Go, where methods on embedded structs are also available on the outer struct.</p>\n<p><a href=\"https://gobyexample.com/struct-embedding\" rel=\"ugc\">https://gobyexample.com/struct-embedding</a></p>",
            "createdAt": "2025-07-10 08:28:57-0500",
            "replies": [
              {
                "author": "osa1",
                "content": "<p>Didn’t know Go has the same feature, thanks.</p>\n<p>The “Go by example” page also doesn’t mention multiple embedding, but apparently it’s possible in Go: <a href=\"https://stackoverflow.com/questions/76396350/struct-embedding-and-ambiguity-resolution\" rel=\"ugc\">https://stackoverflow.com/questions/76396350/struct-embedding-and-ambiguity-resolution</a></p>",
                "createdAt": "2025-07-10 08:45:45-0500",
                "replies": []
              }
            ]
          },
          {
            "author": "gcupc",
            "content": "<p>The manual says:</p>\n<blockquote>\n<p>Pointers to structs enjoy a special feature in Hare: automatic sub-typing. Essentially, a pointer to one type can be automatically cast to a pointer of another type so long as the second type appears at the beginning of the first type.</p>\n</blockquote>\n<p>I assume that means that since you can’t have two types appear “at the beginning” of the first type, you can’t have multiple inheritance/embedding.</p>",
            "createdAt": "2025-07-10 09:43:20-0500",
            "replies": []
          }
        ]
      }
    ]
  },
  {
    "postTitle": "IBM Power11 hits the market this month",
    "comments": [
      {
        "author": "johnklos",
        "content": "<p>I want one!</p>\n<p>No, wait - if I really think about it, I don’t really want a multi-hundred (possibly thousand) watt machine that makes Sun rackmount hardware sound silent.</p>\n<p>The numbers don’t make any sense:</p>\n<blockquote>\n<p>256 SMT-8 Power11 cores with 2MB L2 each and up to 128GB of shared L3 (8GB per core)</p>\n</blockquote>\n<p>if each core has 8GB of L3, then that’d just be 16 cores, because 256 cores at 8GB per core would be 2048GB (in which case there’d be no need for system memory).</p>\n<p><strong>EDIT:</strong> I think it’s 16 physical processors (four sockets per machine and four nodes), each with 16 cores, and 16 physical processors with 8GB of L3 per physical processor would give 128GB. <strong>END EDIT</strong></p>\n<p>If this is a case of someone conflating cores with threads, then using “core” to refer to both physical cores and threads within the same sentence would be suspicious, or at least it’d feel “markety”.</p>\n<p>I wish IBM had enough sense to make a Raspberry Pi like device with several low power Power cores. People need to have them, to play with them, to run their own software on them if they’re ever to become more popular. The closest thing many of us have to contemporary Power is perhaps a PowerPC Mac mini or a Nintendo Wii, or perhaps a Power Mac G5. While I did have a Power S822 at a previous job, I’ve never otherwise seen in person a Power machine newer than a G5. It’s a shame, really.</p>",
        "createdAt": "2025-07-08 19:19:02-0500",
        "replies": [
          {
            "author": "classichasclass",
            "content": "<p>No, these really are cores. That’s a maxed out E1180 with all four nodes maximally populated. Your math in your edit is correct. I’m sure the cost would be eye-watering but never mind because IBM doesn’t sell to the hoi polloi anyway. :)</p>\n<p>The closest we’ve come to an OpenPOWER RPi is Microwatt and I really need to burn an FPGA device with the current build and check it out again. I’ve only run it in VHDL simulation.</p>\n<p>I would very much like a Power11 or something in that ballpark, preferably true OpenPOWER. I like my POWER9, but it was brand spanking new almost ten years ago. I’m now told there might be something on the horizon for Q1 2026, though that’s not official yet.</p>",
            "createdAt": "2025-07-08 19:58:32-0500",
            "replies": [
              {
                "author": "ocramz",
                "content": "<blockquote>\n<p>IBM doesn’t sell</p>\n</blockquote>\n<p>/s Jokes aside, these things are like bank mainframes I guess?</p>",
                "createdAt": "2025-07-09 04:39:52-0500",
                "replies": [
                  {
                    "author": "4ad",
                    "content": "<p>No, that is the IBM Z architecture. These machines are mostly used for Linux, although IBM i also runs on these processors.</p>",
                    "createdAt": "2025-07-09 05:50:55-0500",
                    "replies": []
                  }
                ]
              }
            ]
          },
          {
            "author": "calvin",
            "content": "<blockquote>\n<p>No, wait - if I really think about it, I don’t really want a multi-hundred (possibly thousand) watt machine that makes Sun rackmount hardware sound silent.</p>\n</blockquote>\n<p>It’s a server platform. I’m not sure what you expected otherwise.</p>\n<blockquote>\n<p>I wish IBM had enough sense to make a Raspberry Pi like device with several low power Power cores. People need to have them, to play with them, to run their own software on them if they’re ever to become more popular. The closest thing many of us have to contemporary Power is perhaps a PowerPC Mac mini or a Nintendo Wii, or perhaps a Power Mac G5. While I did have a Power S822 at a previous job, I’ve never otherwise seen in person a Power machine newer than a G5. It’s a shame, really.</p>\n</blockquote>\n<p>What is the market for a PowerPC Pi? People who want to collect exotic architectures? Not much point in that, nor is it a terribly large market.</p>\n<p>On the other hand, if you actually do want these, POWER8 (and especially older) systems are incredibly cheap from i.e. your ewaste recycler or eBay. I’m running a system I got with 1 TB of RAM and 192 threads for less than 600$. And unlike a G5, it’s not pathetic performance wise (i.e. it actually compares to Xeon IVB/HSW systems, contemporary high end Intel of the time).</p>",
            "createdAt": "2025-07-09 08:56:16-0500",
            "replies": [
              {
                "author": "ahelwer",
                "content": "<blockquote>\n<p>What is the market for a PowerPC Pi?</p>\n</blockquote>\n<p>Just speaking personally, someone who wants to try life with the Power architecture before dropping nearly $10k on a Talos Blackbird for all the open source firmware.</p>",
                "createdAt": "2025-07-09 09:49:17-0500",
                "replies": [
                  {
                    "author": "classichasclass",
                    "content": "<p>I get this point, and there was a time when IBM wanted PowerPC to compete everywhere. For at least part of that period, it seemed like it might actually happen.</p>\n<p>But IBM is out of the commodity computing market; I suspect they’ve concluded, and correctly, that’s it’s an unprofitable race to the bottom. They’ll make the chips if they’re paid to (see, for example, Nintendo), but if people want to buy a Power ISA machine at retail, they want someone else to sell it. If you don’t want a service contract, they don’t want to talk to you. (Embedded PPC lives in its own world, so I’m going to ignore it for this discussion.)</p>\n<p>This is really Raptor’s problem to solve because they <em>do</em> sell Power ISA at retail, and they are probably the largest vendor of such systems. They’ve been trying to with things like Arctic Tern, but what people really need is a Microwatt board sold pre-configured ready to run. That can be accomplished today on existing FPGA boards. But <a href=\"https://lobste.rs/~calvin\" rel=\"ugc\">@calvin</a> might also be right and there may well not be enough money in it either.</p>",
                    "createdAt": "2025-07-09 10:26:24-0500",
                    "replies": [
                      {
                        "author": "nortti",
                        "content": "<p>What kind of performance can you expect from a Microwatt FPGA implementation? It appears that you’d need a board costing at least a few hundred to run it, and unless that is able to give you decent performance I really can’t see that selling.</p>",
                        "createdAt": "2025-07-10 09:20:15-0500",
                        "replies": []
                      }
                    ]
                  }
                ]
              },
              {
                "author": "4ad",
                "content": "<blockquote>\n<p>I’m running a system I got with 1 TB of RAM and 192 threads for less than 600$</p>\n</blockquote>\n<p>What? How? Please share more details if possible! I can’t find anything like that on eBay. (Again it could be the problem of being in Europe).</p>",
                "createdAt": "2025-07-09 09:12:35-0500",
                "replies": [
                  {
                    "author": "calvin",
                    "content": "<p>Recyclers don’t know what they have (where’s the VGA port? can’t test this) and they struggle to move them versus x86 systems. You can get deals if you’re patient. What I neglected to mention is that 600$ POWER8 came with a second one for free because they wanted to get rid of it….</p>",
                    "createdAt": "2025-07-09 09:11:00-0500",
                    "replies": [
                      {
                        "author": "freddyb",
                        "content": "<p>That’s wild. Is the power draw crazy?</p>",
                        "createdAt": "2025-07-09 14:06:12-0500",
                        "replies": [
                          {
                            "author": "calvin",
                            "content": "<p>Checked Redfish, each PSU is pulling ~930W. Seems par for the course for high end servers from around then.</p>",
                            "createdAt": "2025-07-09 14:24:59-0500",
                            "replies": []
                          }
                        ]
                      }
                    ]
                  }
                ]
              },
              {
                "author": "johnklos",
                "content": "<blockquote>\n<p>It’s a server platform. I’m not sure what you expected otherwise.</p>\n</blockquote>\n<p>I expect servers that aren’t insanely more noisy than other servers. My Sun Fire V245 is bearable, but is significantly more noisy than my AlphaServer DS25. When I’d start the Power S822, even with the datacenter doors closed, people would be able to hear it from pretty far away and wondered if something was wrong. Luckily they do quiet down quite a bit when idle, but when those fans are moving, holy cow!</p>\n<blockquote>\n<p>What is the market for a PowerPC Pi? People who want to collect exotic architectures? Not much point in that, nor is it a terribly large market.</p>\n</blockquote>\n<p>It’d be nice if there was anything at the low end so people could see that the world isn’t just x86 and Arm.</p>\n<blockquote>\n<p>I’m running a system I got with 1 TB of RAM and 192 threads for less than 600$</p>\n</blockquote>\n<p>That’s pretty darned good! I bet it can run LLMs very decently, considering the memory bandwidth they have. If I had the space… and power… and energy to get NetBSD running on it…</p>",
                "createdAt": "2025-07-09 12:35:50-0500",
                "replies": [
                  {
                    "author": "calvin",
                    "content": "<blockquote>\n<p>I expect servers that aren’t insanely more noisy than other servers. My Sun Fire V245 is bearable, but is significantly more noisy than my AlphaServer DS25. When I’d start the Power S822, even with the datacenter doors closed, people would be able to hear it from pretty far away and wondered if something was wrong. Luckily they do quiet down quite a bit when idle, but when those fans are moving, holy cow!</p>\n</blockquote>\n<p>It’s a modern server, so compare it to something like i.e. a DL380 Gen8 w/ performance processors. I’m surprised any colocation place would be concerned, they likely deal with far noisier servers. Mine is colocated and after starting up, it’s no worse than any other server.</p>\n<blockquote>\n<p>It’d be nice if there was anything at the low end so people could see that the world isn’t just x86 and Arm.</p>\n</blockquote>\n<p>They make systems for the market that exists, not the market niche enthusiasts wish would exist.</p>\n<blockquote>\n<p>That’s pretty darned good! I bet it can run LLMs very decently, considering the memory bandwidth they have. If I had the space… and power… and energy to get NetBSD running on it…</p>\n</blockquote>\n<p>Neural network performance probably is not ideal except in SIMD, but there are much better ways to run that (i.e. a GPU - there are Nvidia drivers…).</p>\n<p>These don’t run NetBSD; there is a FreeBSD port, but <a href=\"https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=279137\" rel=\"ugc\">it doesn’t work with PowerVM</a>, only OPAL (which, means losing partitioning, a massive advantage these have). It’s funny how Power is effectively invisible to the altarch diehards in spite of it being far better performance and far better value than most of what they idolize.</p>",
                    "createdAt": "2025-07-09 14:26:57-0500",
                    "replies": [
                      {
                        "author": "johnklos",
                        "content": "<blockquote>\n<p>It’s a modern server, so compare it to something like i.e. a DL380 Gen8 w/ performance processors. I’m surprised any colocation place would be concerned</p>\n</blockquote>\n<p>I have no idea what a Dell sounds like these days. Dell quality is shit, from my experience, and from what I’ve heard from others who continue to have to deal with Dell, it hasn’t changed much.</p>\n<p>Who said anything about colo? I’d love a machine that didn’t require colo because of the amount of noise.</p>\n<blockquote>\n<p>Neural network performance probably is not ideal except in SIMD, but there are much better ways to run that (i.e. a GPU - there are Nvidia drivers…).</p>\n</blockquote>\n<p>That’s fine as long as you can squeeze your data in to 16 or 32 gigs. I don’t understand the fetishization of wanting to run something ten times faster, but with one tenth the working space, and ten times the setup time. I’d rather run slowly but not spend an entire day figuring out how to fit data in to a video card’s working space.</p>\n<blockquote>\n<p>These don’t run NetBSD</p>\n</blockquote>\n<p>Which is why at one point I wanted to spend the time and energy to get NetBSD running on it, but considering there is no low end aside from used high end, I don’t know if I want to do that.</p>\n<blockquote>\n<p>there is a FreeBSD port, but it doesn’t work with PowerVM, only OPAL (which, means losing partitioning, a massive advantage these have)</p>\n</blockquote>\n<p>I don’t know if everyone wants or needs to to virtualize or partition everything. Some of us just, you know, run software on an OS ;)</p>",
                        "createdAt": "2025-07-09 21:56:49-0500",
                        "replies": []
                      }
                    ]
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "author": "lorddimwit",
        "content": "<p>A more elegant architecture, from a more civilized age.</p>",
        "createdAt": "2025-07-09 00:13:13-0500",
        "replies": []
      }
    ]
  },
  {
    "postTitle": "Is the doc bot docs, or not?",
    "comments": [
      {
        "author": "carlana",
        "content": "<p>There should be a tag for documentation. That’s an important part of programming!</p>",
        "createdAt": "2025-07-09 10:09:00-0500",
        "replies": [
          {
            "author": "kaycebasques",
            "content": "<p>I always use <code>practices</code> but as a technical writer I would love to see a <code>docs</code> tag</p>",
            "createdAt": "2025-07-09 10:56:18-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "twotwotwo",
        "content": "<p>I don’t think anyone deploying a bot should position it as docs with current technology. Docs make promises–it’s a bug, either in the software or the docs, when they’re incorrect. And LLMs have inherent unreliability–they all have that “XYZ makes mistakes! Check the responses”, and really should have stronger language than that. Letting the LLM make promises on your behalf is inevitably making promises to users that will be broken.</p>\n<p>Unreliable things can still be handy, like Googling for forum posts can be, or trying your best guess. LLMs <em>are</em> reasonably effective at tracking down relevant info given a description that doesn’t have the right words to make a keyword search succeed, or at “translating” plain language to code snippets if trained on/given the needed info.</p>\n<p>In systems like this it could help to convey the uncertainty to the user and advice for how to deal with it (test the suggested code by…), to try to get it to decline to answer about likely question topics it has no info about (they will 100% <em>not</em> do this by default), and to include links to the real docs so there’s somewhere to go for reliable details.</p>",
        "createdAt": "2025-07-09 20:09:28-0500",
        "replies": []
      },
      {
        "author": "pta2002",
        "content": "<p>First (and last) time I used one of these bots, it was in my university, when I was trying to figure out how to sign up for my thesis. Turns out it just made something up and I almost lost my spot in the year.</p>\n<p>Thankfully (?) it seems like it made the same lie to everyone else, to the point that the administration ended up just accepting what the bot said that year, but after that they replaced the bot with a typical fixed-response + talk to a human. At least in this case the organization was fairly flexible. That won’t always happen.</p>",
        "createdAt": "2025-07-10 03:04:06-0500",
        "replies": []
      },
      {
        "author": "LunNova",
        "content": "<p>Bad docs are still docs, so probably.</p>\n<p>If the relevant part of codebase is small enough to fit in context paid models can do a pretty decent job at answering questions about it even if the documentation is missing or incorrect. I guess that’s only an option for open source projects, as any project that gives a doc bot in context sources or RAG access to them is quickly going to leak them. It’d be really expensive to run with a good enough model too.</p>",
        "createdAt": "2025-07-09 20:17:57-0500",
        "replies": []
      },
      {
        "author": "zie",
        "content": "<p>Normal docs, not AI generated are misinformed too though. But I’d agree that doc bot docs are docs.</p>",
        "createdAt": "2025-07-09 10:19:18-0500",
        "replies": [
          {
            "author": "gcupc",
            "content": "<p>I like the distinction ~twotwotwo made above: if the documentation and the program disagree, one of them has a bug that needs to be fixed. And no one is saying normal docs are never buggy, or even that they aren’t often buggy. But with an LLM-based doc bot, you can’t fix the bug; the possibility that it might just make things up is fundamental to the technology, and you can’t correct its understanding of the program, because it doesn’t have one.</p>",
            "createdAt": "2025-07-10 13:41:17-0500",
            "replies": []
          },
          {
            "author": "ollien",
            "content": "<p>I’d argue an important distinction is whether or not the docs are publicly available and able to be edited when mistakes are found. In this case, no audit is possible if it comes from an ephemeral chat.</p>",
            "createdAt": "2025-07-09 19:12:20-0500",
            "replies": [
              {
                "author": "zie",
                "content": "<p>Many(most?) docs from companies are not publicly available or able to be edited. I looked on Stripe’s github, and no docs repo existed.</p>\n<p>I think I get your point though, that it’s a lot easier to send in a bug report for docs when you can point at something and say see on page X it says Y. It’s a lot more annoying to do that with a bot (of any kind, AI or not). Send a screenshot I guess. Though that still doesn’t mean it can be reproduced and/or even fixed.</p>",
                "createdAt": "2025-07-10 09:00:43-0500",
                "replies": []
              }
            ]
          }
        ]
      }
    ]
  },
  {
    "postTitle": "Jai Demo and Design Explanation",
    "comments": [
      {
        "author": "damantisshrimp",
        "content": "<p>I like the focus on introspection and tooling.</p>",
        "createdAt": "2025-07-10 10:16:08-0500",
        "replies": []
      }
    ]
  },
  {
    "postTitle": "Kubernetes is not just for Black Friday",
    "comments": [
      {
        "author": "alper",
        "content": "<p>Standardization is good even if the standard itself isn’t perfect.</p>\n<p>I ran into somebody who tried the “most people don’t need Kubernetes, you should just get a fat Hetzner box and run your app there” until I told him the requests/s that we run and that was the end of that.</p>",
        "createdAt": "2025-07-10 04:50:43-0500",
        "replies": [
          {
            "author": "dijit",
            "content": "<p>Exceptions prove the rule though.</p>\n<p>The person you ran into was right, a pair of boxes that can take over for each other will get you exceptionally far.</p>\n<p>Kubernetes is what you should grab as soon as you expect to go beyond that.</p>\n<p>People forget that its not just developer time lost in managing things “the kubernetes way”, you also lose quite a bit of performance when dealing with clustering overheads. - this is true of basically all the hyperscale clustering technologies.</p>\n<p>FWIW, I’ve been in the industry since before the cloud was a thing and I was running (at peak) 1% of web traffic on bare metal at the time, with an application that was a fat pig- we had racks of machines for this and kubernetes would have helped a lot; I’ve also worked in places doing a 10th of that with even more hardware and a leaner application: because we are running abstractions on abstractions on abstractions and it sucks away a lot more than you think.</p>\n<p>(ps; I agree with your thesis that an imperfect standard can be better than nothing at all).</p>",
            "createdAt": "2025-07-10 05:39:20-0500",
            "replies": [
              {
                "author": "alper",
                "content": "<blockquote>\n<p>because we are running abstractions on abstractions on abstractions and it sucks away a lot more</p>\n</blockquote>\n<p>We are using NestJS which seems to be the “right” choice given that everybody else is also doing it but all the performance disappears into a morass of javascript abstractions all of which to me seem entirely pointless (Go web frameworks don’t seem to need any of this).</p>",
                "createdAt": "2025-07-10 07:56:33-0500",
                "replies": [
                  {
                    "author": "zladuric",
                    "content": "<p>I’ve been thinking about this as well.</p>\n<p>Early node rest APIs seemed crazy fast (compared to a lot of things), and stupid simple. Kinda like go.</p>\n<p>But I think it was used by people who understood JavaScript deeply enough, and could be “trusted” to know the tradeoffs - performance implications of adding abstractions of farmeworks vs correctness and code/team scaling implications of not using them. And also they knew when not to use JavaScript.</p>\n<p>And for the most part they’d be replacing PHP or python which was worse for these types of jobs, so most of the tradeoffs weren’t relevant anyway.</p>\n<p>But now we have millions of APIs running on node, and millions of devs who lack this deep understanding, or even the fact that there <em>are</em> performance things to worry about. And nobody cares anyway, wherever I ask for perf numbers, I <em>may</em> get an approximate answer by some poor DevOps guy holding it as together with spit and wire.</p>\n<p>So while it may seem pointless, it is also irrelevant, and nobody gives a damn.</p>",
                    "createdAt": "2025-07-10 09:03:05-0500",
                    "replies": []
                  }
                ]
              }
            ]
          },
          {
            "author": "peterbourgon",
            "content": "<blockquote>\n<p>until I told him the requests/s that we run</p>\n</blockquote>\n<p>Just out of curiosity: what RPS (approximately) do you serve, and in which language(s) is your app written?</p>\n<p>(I guess any kind of reasonable application, running on any kind of modern server, without any special optimizations, should be able to handle ~1k RPS at a bare minimum, or maybe around ~5k RPS in the normal case, and 10k+ RPS if you really know what you’re doing!)</p>",
            "createdAt": "2025-07-10 13:45:12-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "motet-a",
        "content": "<p>I agree with this post but I still have not figured out how to run something like k3s on something reasonably cheap. The <a href=\"https://docs.k3s.io/installation/requirements\" rel=\"ugc\">minimum requirements</a> say at least 2 cores, 2 GB RAM. I can totally afford a Raspberry Pi that would be vastly above those requirements (I already have one actually) but for reliability I really prefer not to self-host everything, some of my “home lab” services are used by others and I try to keep a decent quality of service. And “wasting” 2 GB of RAM and 2 cores on a rented VPS&nbsp;is not that cheap! Where I live, it’s basically $85 a year on on a quite affordable hosting service, and much more than that on AWS / GCP / Azure.</p>",
        "createdAt": "2025-07-10 03:58:19-0500",
        "replies": [
          {
            "author": "Vaelatern",
            "content": "<p>Try Nomad! It scales nicely down to very little going on.</p>",
            "createdAt": "2025-07-10 04:00:47-0500",
            "replies": [
              {
                "author": "zie",
                "content": "<p>In linux land, there is also Incus: <a href=\"https://linuxcontainers.org/incus/\" rel=\"ugc\">https://linuxcontainers.org/incus/</a> which supports OCI/docker too. It can scale from 1 node to many and is mostly just a wrapper around KVM, so not a lot of overhead.</p>",
                "createdAt": "2025-07-10 09:50:14-0500",
                "replies": [
                  {
                    "author": "bwbuhse",
                    "content": "<p>I admittedly don’t have it in a cluster, but +1 for Incus. I have it running on my home server with a mix of system containers and OCI images and it’s really nice/easy now whenever I decide to set a new service up.</p>",
                    "createdAt": "2025-07-10 10:56:43-0500",
                    "replies": []
                  }
                ]
              },
              {
                "author": "Verfeuil",
                "content": "<p>But with nomad, you lose the appeal of kubernetes: you need to deploy so many things to have the same ‘integrated’ workflow, and also pretty much no one ships a hcl for their software. I inherited a nomad cluster and i wish it was a kube one. For example, I have a several hundred line hcl file for harbor that I don’t want to touch.</p>",
                "createdAt": "2025-07-10 06:41:49-0500",
                "replies": [
                  {
                    "author": "zie",
                    "content": "<p>Not wanting to touch stuff is kind of normal for inheritance though. :)</p>\n<p>We run Nomad at $WORK, and it works fine. The killer feature is it’s quite easy to reason about, since the scope is bounded, unlike k8s which generally is not (bounded scope or easy to reason about).</p>",
                    "createdAt": "2025-07-10 09:53:19-0500",
                    "replies": []
                  }
                ]
              }
            ]
          },
          {
            "author": "aarroyoc",
            "content": "<p>There’s k0s which lists only 1 core and 1GB RAM as minimum requirements, but I have not tried it</p>",
            "createdAt": "2025-07-10 04:33:33-0500",
            "replies": [
              {
                "author": "shurup",
                "content": "<p>In my understanding, the most minimal Kubernetes distribution is this new project by Portainer: <a href=\"https://github.com/portainer/kubesolo\" rel=\"ugc\">KubeSolo</a>. It targets specific use cases (e.g., IoT) and comes with a few limitations, though…</p>",
                "createdAt": "2025-07-10 04:36:29-0500",
                "replies": []
              }
            ]
          },
          {
            "author": "CobyPear",
            "content": "<p>I had a k3s cluster on some raspberry pi clones, and it couldn’t support all of the services i wanted to run. So it’s possible to run on lighter weight machines, but heavily dependent on the workload.</p>",
            "createdAt": "2025-07-10 15:06:07-0500",
            "replies": []
          },
          {
            "author": "wink",
            "content": "<p>As you use $ the latency would probably suck but I think netcup (and maybe Hetzner, OVH) you could maybe cover it with 50-55 EUR, but I think 4/month is the hard lower limit without getting any good deals. (No I did not check USD vs EUR on this day, for these comparisons it’s always been 1=1 for me ;))</p>",
            "createdAt": "2025-07-10 09:16:48-0500",
            "replies": [
              {
                "author": "motet-a",
                "content": "<p>Well actually I use both EUR and OVH! I converted amounts to USD in my last comment. I never tried Hetzner. But yeah, €4/month is basically the lower limit.</p>",
                "createdAt": "2025-07-10 10:10:40-0500",
                "replies": []
              }
            ]
          }
        ]
      },
      {
        "author": "kqr",
        "content": "<p>If the author reads this: you showed the example of a Synapse setup to indicate how complicated Ansible was, yet an anonymous three-nginx setup to show how simple Kubernetes is. I feel a little cheated. Could we get an apples-for-apples comparison?</p>",
        "createdAt": "2025-07-10 13:56:43-0500",
        "replies": [
          {
            "author": "Thib",
            "content": "<p>Author here, that’s a fair question. To get an apples-for-apples comparison, you should look at the keycloak example.</p>\n<p>Element (the company) built <a href=\"https://artifacthub.io/packages/helm/element-server-suite-community/matrix-stack\" rel=\"ugc\">a helm chart to deploy synapse</a> and the principle is the same as in the Keycloak the example I used.</p>\n<p>I’ll grant you that using the Element provided chart instead of Keycloak would have been clearer :)</p>",
            "createdAt": "2025-07-10 15:06:15-0500",
            "replies": [
              {
                "author": "kqr",
                "content": "<p>Thanks, that makes sense. Your article echoes many of the beliefs I have around Kubernetes for hobby configurations but which I’ve never been able to express since I lack the experience of running Kubernetesified services as a side project.</p>",
                "createdAt": "2025-07-10 15:25:51-0500",
                "replies": []
              }
            ]
          }
        ]
      }
    ]
  },
  {
    "postTitle": "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity",
    "comments": [
      {
        "author": "steveklabnik",
        "content": "<p>Loving this. It’s well written, and is very clear about what they are claiming and not claiming.</p>\n<blockquote>\n<p>Most issues were completed in February and March 2025, before models like Claude 4 Opus or Gemini 2.5 Pro were released.</p>\n</blockquote>\n<p>I’d love to see this done again, for various reasons, but this is a big one. My opinion of models pre and post May 2025 are very different, and I’d be curious to see if that’s reflected in another experiment.</p>",
        "createdAt": "2025-07-10 13:37:36-0500",
        "replies": []
      },
      {
        "author": "Johz",
        "content": "<p>The key takeaways, as I understand it:</p>\n<ul>\n<li>AI slowed developers down rather than speeding them up in this specific study.</li>\n<li>The developers estimated that they would be more productive with AI, and still felt that they were more productive with AI once the exercises were over, even though they weren’t.</li>\n<li>The study is surprisingly well thought through for this kind of DX research, but still very limited and the authors caution against extrapolating the results to developers in general.</li>\n</ul>",
        "createdAt": "2025-07-10 12:32:47-0500",
        "replies": [
          {
            "author": "gcupc",
            "content": "<p>I like that the authors are very specific and clear about what they don’t claim. That should forestall a lot of low-quality criticism, though it may cause some people to dismiss the study entirely.</p>\n<p>The most interesting finding to me is #2: developers <em>felt</em> they were more productive with AI, but they were actually less productive. I think this tells us what we can make of all the anecdotal evidence reported in programmer fora. It also reminds me of how studies have shown keyboard controls <em>feel</em> faster than mouse controls, even when they are slower.</p>",
            "createdAt": "2025-07-10 13:17:18-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "technomancy",
        "content": "<blockquote>\n<p>Surprisingly, we find that when developers use AI tools, they take 19% longer than without—AI makes them slower</p>\n</blockquote>\n<p>If you’re surprised by this, I don’t know what to say; you haven’t been paying much attention.</p>\n<p>Also this doesn’t account for the time wasted on things like dealing with slop patches and bug reports: <a href=\"https://socket.dev/blog/django-joins-curl-in-pushing-back-on-ai-slop-security-reports\" rel=\"ugc\">https://socket.dev/blog/django-joins-curl-in-pushing-back-on-ai-slop-security-reports</a></p>",
        "createdAt": "2025-07-10 13:20:23-0500",
        "replies": []
      }
    ]
  },
  {
    "postTitle": "MicroHs, a tiny Haskell Compiler",
    "comments": [
      {
        "author": "fanf",
        "content": "<p>Lennart’s talks are good, and his one starts with a fun demo.</p>\n<p>There have been <a href=\"https://lobste.rs/search?q=microhs&amp;what=comments&amp;order=newest\" rel=\"ugc\">several recent comments mentioning MicroHS</a> and its github repo was <a href=\"https://lobste.rs/s/lceket/microhs_haskell_implemented_with\" rel=\"ugc\">linked but not discussed last year</a></p>",
        "createdAt": "2025-07-09 19:10:12-0500",
        "replies": [
          {
            "author": "pom",
            "content": "<p>I did not know about MicroHS before seeing this presentation, thanks for the link to previous comments. What I found the most interesting here was the use of combinators as a compilation target which appears to still be viable—I had read about it In <a href=\"https://simon.peytonjones.org/slpj-book-1987/\" rel=\"ugc\">Simon Peyton Jones’s book</a> before but it seemed more like an academic curiosity.</p>",
            "createdAt": "2025-07-10 01:30:34-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "osa1",
        "content": "<p>In “specialization and dictionary elimination” he shows that you can reify the dictionaries with a built-in named <code>Dict</code>, but I wonder how first class the reified dictionaries are really. For example can you change the methods? If I have a <code>Dict</code> representing <code>Show [Int]</code>, can I change how the <code>Int</code>s are shown by changing the <code>Show Int</code> captured by <code>Show a =&gt; Show [a]</code>?</p>\n<p>I’m also wondering what kind of format he uses for the cache/package database. Is it an actual database, like SQLite or similar, or a custom format? What kind of queries/updated does it allow?</p>",
        "createdAt": "2025-07-10 02:49:52-0500",
        "replies": [
          {
            "author": "iand675",
            "content": "<p>It’s not really specific to MicroHS, but what you’re describing in the first question is sort of <a href=\"https://www.schoolofhaskell.com/user/thoughtpolice/using-reflection#dynamically-constructing-type-class-instances\" rel=\"ugc\">a thing people do</a> using the reflection library, although I think it tends to require a newtype wrapper to shim in the instance.</p>",
            "createdAt": "2025-07-10 03:39:43-0500",
            "replies": []
          },
          {
            "author": "chriswarbo",
            "content": "<blockquote>\n</blockquote>\n<p><a href=\"https://hackage.haskell.org/package/MicroHs\" rel=\"ugc\">It’s on Hackage</a>; there’s no Haddock documentation, but clicking the “[browse]” link in the “Downloads” section shows <a href=\"https://hackage.haskell.org/package/MicroHs-0.13.0.0/src/\" rel=\"ugc\">a file listing</a></p>\n<p>Looking at <a href=\"https://hackage.haskell.org/package/MicroHs-0.13.0.0/src/src/MicroHs/CompileCache.hs\" rel=\"ugc\">the CompileCache module</a>, it seems to be using a custom format based around key/value maps and assoc-lists (compressed with LZ77).</p>",
            "createdAt": "2025-07-10 08:47:46-0500",
            "replies": []
          }
        ]
      }
    ]
  },
  {
    "postTitle": "Monitoring my Homelab, Simply",
    "comments": [
      {
        "author": "sigmonsez",
        "content": "<p>The claim is one dependency, the go stdlib but then goes on to call various cloud services ntfy.sh and healthchecks.io. maybe they means code dependency?</p>\n<p>I’ve setted on Uptime Kuma and just looking at a dashboard. I don’t even want to be paged =P</p>",
        "createdAt": "2025-07-10 11:46:53-0500",
        "replies": []
      },
      {
        "author": "rtpg",
        "content": "<p>I’ve recently gotten my home server spun up and between this and the systemd chatter I’m getting loads of useful ideas.</p>\n<p>Now I just want to find some programs to run that actually cause the server to look busy.</p>",
        "createdAt": "2025-07-10 07:39:38-0500",
        "replies": []
      },
      {
        "author": "nelson",
        "content": "<p>I get a lot of mileage from updown.io and healthchecks.io. I haven’t felt a need for custom extras like this though.</p>\n<p>I really should set up ntfy.sh. Right now I’m using email and Gotify. Gotify is really not good but it’s what Proxmox supports. They added web hooks recently though, time to revisit.</p>",
        "createdAt": "2025-07-10 10:12:46-0500",
        "replies": [
          {
            "author": "zie",
            "content": "<p>ntfy.sh is awesome, def. worth the setup for me.</p>",
            "createdAt": "2025-07-10 10:26:12-0500",
            "replies": []
          },
          {
            "author": "driib",
            "content": "<p>Thanks for the idea! Just set up a ntfy.sh target successfully on my Proxmox:</p>\n<ul>\n<li>Target: webhook</li>\n<li>Type: POST</li>\n<li>Address: <code>https://ntfy.sh/REDACTED</code>\n</li>\n<li>Headers: <ul>\n<li>Title: <code>{{title}}</code>\n</li>\n</ul>\n</li>\n<li>Body:</li>\n</ul>\n<pre><code>[{{severity}}] {{message}} </code></pre>",
            "createdAt": "2025-07-10 13:10:16-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "mrexodia",
        "content": "<p>My setup is somewhat similar. I have a Cloudflare worker on a 5 minute cronjob. The worker connects to api.mydomain.lan/health.php via Zero Trust and the results are posted to a Discord channel with a webhook.</p>\n<p>The health.php script checks a bunch of services running on my Proxmox instance and returns an error that gets forwarded to the Discord channel directly.</p>\n<p>So far I found out that my ISP is extremely unreliable and that power outages happen pretty frequently, otherwise things are working very smoothly 😅</p>",
        "createdAt": "2025-07-10 10:45:35-0500",
        "replies": []
      },
      {
        "author": "makishimu",
        "content": "<p>My pseudo-homelab is like 1.75 boxes. Recently I’ve been looking into setting up some monitoring to watch basics like the temperature, storage usage, etc. So I was reminded about this post: <a href=\"https://lobste.rs/s/uzjmam/mon_stupid_simple_monitoring\" rel=\"ugc\">μMon: Stupid simple monitoring</a>. Although it focuses rather on observability than alerting.</p>",
        "createdAt": "2025-07-10 11:20:12-0500",
        "replies": []
      },
      {
        "author": "j3s",
        "content": "<p>nice! i currently use prometheus &amp; blackbox_exporter for just about everything, but have had trouble with exporters breaking backwards compatibility without warning - invalidating my dashboards and alerts silently. something like this is appealing – i love that i can see it working for another 10 years without many changes.</p>",
        "createdAt": "2025-07-10 11:52:46-0500",
        "replies": []
      },
      {
        "author": "driib",
        "content": "<p>I am actually attracted to an idea of an all-in-one OTEL tool for the homelab. For example, Signoz. Maybe it won’t tick all the boxes for the enterprise procurement but good to have something small yet capable of ingesting anything useful for observability (not everything!). I haven’t tried it yet, mostly looking through its release notes and waiting until it stabilizes enough so that every release doesn’t include 10 bugfixes.</p>\n<p>For now I settled on Uptime Kuma + ntfy.sh and, sometimes, netdata, to have a few days worth of graphs to understand the trends when looking into root causes.</p>",
        "createdAt": "2025-07-10 11:59:20-0500",
        "replies": []
      },
      {
        "author": "dsr",
        "content": "<p>The thing that I value most in a monitoring system is that it does the job, and the thing that I value second most highly is that it not cause more work by itself.</p>\n<p>This looks like it can score highly on both parameters for small systems, so huzzah.</p>",
        "createdAt": "2025-07-10 14:28:30-0500",
        "replies": []
      }
    ]
  },
  {
    "postTitle": "Multi-player Durable Stream Playground",
    "comments": [
      {
        "author": "WilhelmVonWeiner",
        "content": "<p>You only submit links about S2, is posting to Lobsters really getting that many conversions? <a href=\"https://lobste.rs/s/mkyrvt/amazon_time_sync_service_now_supports#c_6sv0w9\" rel=\"ugc\">Clearly you have interest in the technology</a>, so why not foster conversation around that rather than treat Lobsters as a marketing channel?</p>",
        "createdAt": "2025-07-10 11:24:50-0500",
        "replies": [
          {
            "author": "shikhar",
            "content": "<p>That’s fair, I’ll try to share/participate more than just S2.</p>",
            "createdAt": "2025-07-10 11:30:41-0500",
            "replies": []
          }
        ]
      }
    ]
  },
  {
    "postTitle": "Oregon Programming Languages Summer School (OPLSS) 2025: Types, Logic, and Formal Methods",
    "comments": [
      {
        "author": "mplant",
        "content": "<p>I’ve only watched a few videos that have come out of OPLSS but they are always a treat - extremely approachable dives into incredibly deep and interesting areas of PLT. Very excited to go through this one! I should really visit one year.</p>",
        "createdAt": "2025-07-09 12:07:41-0500",
        "replies": [
          {
            "author": "gasche",
            "content": "<p>Better be quick while there are still some people willing to come to the US.</p>",
            "createdAt": "2025-07-09 15:26:18-0500",
            "replies": [
              {
                "author": "mplant",
                "content": "<p>well, uh, since I missed this one, and they happen annually, the quickest I can be is probably not quick enough!</p>",
                "createdAt": "2025-07-10 08:27:49-0500",
                "replies": []
              }
            ]
          }
        ]
      }
    ]
  },
  {
    "postTitle": "Phrase origin: Why do we “call” functions?",
    "comments": [
      {
        "author": "fanf",
        "content": "<p><a href=\"https://archive.org/details/programsforelect00wilk\" rel=\"ugc\">Wilkes, Wheeler, Gill</a> uses the phrase “call in” to invoke a subroutine.</p>\n<p>Note the Internet Archive has the 1957 edition; I don’t know how much it differs from the 1951 edition. The preface says, “little of the original material has been taken over unchanged,” so it isn’t a reliable indication of the terminology used in 1951. (but see the edit below)</p>\n<p>Page 31 has:</p>\n<blockquote>\n<p>… if, as a result of some error on the part of the programmer, the order Z F does not get overwritten, the machine will stop at once. This could happen if the subroutine were not called in correctly.</p>\n</blockquote>\n<blockquote>\n<p>It will be noted that a closed subroutine can be called in from any part of the program, without restriction. In particular, one subroutine can call in another subroutine.</p>\n</blockquote>\n<p>See also the program on page 33.</p>\n<p><em>(edit)</em></p>\n<p>I couldn’t find a paper from that era that’s easily available to read, but <a href=\"https://chiphack.org/talks/edsac-part-2.pdf\" rel=\"ugc\">here’s a presentation with many pictures of artefacts from EDSAC’s early years</a>. It has a couple of pages from the 1950 “report on the preparation of programmes for the EDSAC and the use of the library of subroutines” which shows a subroutine listing with a comment saying “call in auxiliary sub-routine”.</p>",
        "createdAt": "2025-07-09 12:25:32-0500",
        "replies": [
          {
            "author": "breadbox",
            "content": "<p>Looking at a copy of the 1951 edition, I was unable to find an analogue of the section you quoted from. This part appears to be new, or at least substantially rewritten. However, I can verify that “called in” (or “called into”) is used throughout, starting with the definition of a “closed subroutine” on Page 22:</p>\n<blockquote>\n<p>A “closed” subroutine is one which is called into use by a special group of of orders incorporated in the master routine or main program.</p>\n</blockquote>\n<p>One quote in particular I found compelling is on Page 28:</p>\n<blockquote>\n<p>If this is considered undesirable, M1 may be called in in such a way that the first order or psueo-order of the next componement goes into the next available location, whether it be odd or even.</p>\n</blockquote>\n<p>The doubling of the preposition, to my mind, makes it clear that “called in” is considered to be a set phrase, a term in and of itself.</p>",
            "createdAt": "2025-07-09 15:00:41-0500",
            "replies": [
              {
                "author": "fanf",
                "content": "<p>Thanks! (where did you find a copy?!)</p>\n<p>I was worried that when revising the book they had updated it to contemporary terminology (as well as talking about computers other than EDSAC). It’s interesting that they revised the introductory material, but the terminology remained the same!</p>\n<p>I guess from the similarity between the book and the “report on the preparation of programmes” that the report was the lab’s working manual, and the book was a published version for a wider audience. And the second edition of the book was based on a later version of the lab’s manual.</p>",
                "createdAt": "2025-07-09 18:46:59-0500",
                "replies": [
                  {
                    "author": "breadbox",
                    "content": "<p>It’s just sitting around on g.books. Must be an oversight, as the book is absolutely still under copyright. (It would have been 24 years into its original 28-year period when US copyright was extended to 75 years.) Normally the big G is quite conservative about the books it makes fully available (I’ve had many many times where a book I wanted to access was snippet-view only even though it was clearly over 95 years old), so I’m guessing they would fix it at once if e.g. the publisher were to complain. If you’re interested, grab a copy of the scan now.</p>",
                    "createdAt": "2025-07-10 00:29:36-0500",
                    "replies": []
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "author": "tanami",
        "content": "<p>I picture it like a phone call where the caller hangs up and then the execution returns to that context.</p>",
        "createdAt": "2025-07-09 04:55:49-0500",
        "replies": []
      },
      {
        "author": "xfbs",
        "content": "<p>One interesting thing: I’m not a native english speaker. So, when I learned vocabulary for software engineering, it is often not with the context of what the words mean “normally”. So there is a lot of vocabulary that I use daily, and I never question why it is called the way it is.</p>\n<p>And sometimes, that leads to an interesting backstory. Or a mental visualization of what the term represents. There’s a number of terms where I’ve realized this. I wish I had taken a note of them. But I’ve never thought about the expression “calling a function”. Very interesting read!</p>\n<p>This also leads to some funny incidents when you translate english computer science terms into your native language. Like: library, dictionary, stack, heap. In German, we say <em>aufrufen</em> for a function, rather than <em>anrufen</em> (anrufen is to call, like to call someone. Aufrufen is to call, but more like calling out for a name. Like at McDonalds, when they call out for whoever has the order number 1337).</p>",
        "createdAt": "2025-07-09 07:41:07-0500",
        "replies": [
          {
            "author": "lorddimwit",
            "content": "<p>I’m not a native French speaker but I remember when I first read Harry Potter in French (20 years ago now), and to “cast a spell” is to “jeter” it, to throw it. And then I remembered in English that “cast” is a synonym for “throw” (but with different nuance) and I was enlightened.</p>",
            "createdAt": "2025-07-09 12:10:20-0500",
            "replies": [
              {
                "author": "hobbified",
                "content": "<p>On that note: long before radio or TV, “broadcast” meant to scatter seeds widely by basically tossing them over the ground, instead of planting them neatly in rows. Broad + cast = throw all over the place.</p>",
                "createdAt": "2025-07-09 23:21:36-0500",
                "replies": []
              }
            ]
          }
        ]
      },
      {
        "author": "nathell",
        "content": "<p>It would be interesting to compare this in different languages.</p>\n<p>In Polish, ‘to call’ as if by telephone is most commonly <em>dzwonić</em> (lit. ‘to ring’); there’s also the more literary and dated <em>telefonować</em> (to telephone).</p>\n<p>‘To call’ a function is <em>wywoływać</em> or, colloquially, <em>wołać funkcję</em>. That’s the same verb you would use for <em>calling out</em> someone on a ham radio, or to describe an action <em>resulting in</em> an effect. It’s also possible to <em>wywoływać demona</em> (to summon a demon).</p>",
        "createdAt": "2025-07-09 08:02:39-0500",
        "replies": [
          {
            "author": "andyferris",
            "content": "<p>I like the idea of “summoning” a subroutine :)</p>",
            "createdAt": "2025-07-09 18:47:34-0500",
            "replies": []
          },
          {
            "author": "datarama",
            "content": "<p>In Danish, it’s the same thing. To call with a telephone is “at ringe” (“to ring”), to call a function or someone on a radio is is “at kalde” (“to call”), and to summon a demon is “at hidkalde” (“to call here”, though its connotations are more like English “to call forth”) - if you summon your dog or your child it’s “kalde”, not “hidkalde”.</p>",
            "createdAt": "2025-07-10 04:40:06-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "ansible-rs",
        "content": "<p>Nice.</p>\n<p>Reminds me of when I learned why we call log files “log files”.</p>\n<p><a href=\"https://www.quora.com/Why-are-computer-logfiles-called-so\" rel=\"ugc\">https://www.quora.com/Why-are-computer-logfiles-called-so</a></p>",
        "createdAt": "2025-07-10 13:57:18-0500",
        "replies": []
      },
      {
        "author": "ZicZacBee",
        "content": "<p>The early use of “call in” makes me think more of “to call in an expert” or “we are going to need to call in a plumber” or “the manager called in a pinch hitter”.</p>",
        "createdAt": "2025-07-10 10:01:38-0500",
        "replies": []
      },
      {
        "author": "sjamaan",
        "content": "<p>An impressive bit of archeology!</p>",
        "createdAt": "2025-07-09 01:55:17-0500",
        "replies": []
      }
    ]
  },
  {
    "postTitle": "Stop putting nix setup in your checked-in .envrc files",
    "comments": [
      {
        "author": "samhh",
        "content": "<p>Committing personal files has been a pet peeve of mine for years that not many other people share. The issues it causes are just distant enough for people not to care.</p>\n<p>VS Code configs in particular are interesting to me because there is value in team members choosing to share and sync them. It’s like there’s a missing abstraction layer in VCS. My brain is telling me submodules but that’s never (?) actually the right answer.</p>",
        "createdAt": "2025-07-07 14:03:44-0500",
        "replies": [
          {
            "author": "dpc_pw",
            "content": "<blockquote>\n<p>Committing personal files has been a pet peeve of mine for year</p>\n</blockquote>\n<p>This is the mistake that most people complaining about it make. <code>.direnv</code> with <code>use nix</code> is not necessarily “personal file”. Just like building the project with <code>make</code>, or <code>bazel</code>, requiring Nix to set up a dev environment is a technical choice. I don’t see people complaining that the project committed in <code>Makefile</code> required to build it as “committing personal files”. Can you create your own Makefile with personal commands? Yes. Does it mean every Makefile is a “personal file”? No.</p>\n<p>Nix with flake dev shell gives a solid, automatic, near-reproducible dev shell, that can be build on to automatically setup commit hooks, linters, formatters and what not for very smooth user experience. It cuts time required supporting newcomers asking for help setting up dev shell, documenting it, and so on, and that fact that it’s all in <code>.envrc</code> automatically advertises it’s availability (with ability to reject it).</p>",
            "createdAt": "2025-07-07 15:33:14-0500",
            "replies": [
              {
                "author": "samhh",
                "content": "<p>You’re conflating Nix and direnv.</p>\n<p>Users with Nix but not direnv, or who use direnv differently, can enter the same dev environment - it’s actually defined in Nix - via <code>nix develop</code>.</p>\n<p>Users who then wish to use nix-direnv or similar can simply <code>echo 'use flake' &gt; .envrc &amp;&amp; direnv allow</code>. I personally take advantage of this in some repos to use direnv’s <code>dotenv</code> where others prefer not to.</p>\n<p>Ultimately if it’s your repo you can do whatever you want, but the likes of direnv and commit hooks are highly prescriptive for a communal repo, akin to dictating the choice of (underlying) shell or OS. Especially considering the cost to you is a one-liner the first time you clone.</p>",
                "createdAt": "2025-07-08 04:46:26-0500",
                "replies": []
              },
              {
                "author": "offby1",
                "content": "<p>You are conflating build instructions with a developer usability tool, to the detriment of users of the latter. You’re also maybe a bit over-invested in trying to tell other people how to use a tool that they find perfectly functional when it isn’t combined with your personal preferences. Perhaps give that some thought instead of telling every other direnv user that they’re holding it wrong.</p>",
                "createdAt": "2025-07-07 15:48:23-0500",
                "replies": [
                  {
                    "author": "dpc_pw",
                    "content": "<p>\n<span class=\"na\"> [Comment removed by moderator pushcx: The sarcasm is over-the-top here.] </span>\n</p>",
                    "createdAt": "2025-07-07 18:54:39-0500",
                    "replies": []
                  }
                ]
              }
            ]
          },
          {
            "author": "xfbs",
            "content": "<p>I agree. I don’t accept editor-specific files in repos. There is <code>.editorconfig</code> for that. One exception is (sadly) that there can be projects that can only be built by a specific editor, like iOS XCode projects or Visual Studio projects. Then you have no choice really.</p>\n<p>Maybe there should be a standard like a <code>.editor</code> folder in repos with an <code>vscode.json</code> and a <code>vim.conf</code> and a <code>zed.json</code> and some way you can override it? Although that seems intrusive. Editor configs tend to be quite personal. You only really want to configure things like code style preferences.</p>",
            "createdAt": "2025-07-08 04:55:00-0500",
            "replies": []
          },
          {
            "author": "altano",
            "content": "<p>For VSCode your project should have a .vscode/settings.json file committed to the repository that only includes uncontroversial, universal project settings you share with others (eg disabling conflicting lint extensions, or marking certain codegen directories read-only). For example, <a href=\"https://github.com/altano/npm-packages/blob/main/.vscode/settings.json\" rel=\"ugc\">https://github.com/altano/npm-packages/blob/main/.vscode/settings.json</a>. You can also recommend extensions the project benefits from in .vscode/extensions.json.</p>\n<p>I’ve noticed this directory is almost always git ignored but that’s definitely subverting the intent. .vscode is for committing and sharing settings, and settings you shouldn’t share go in your user settings.</p>",
            "createdAt": "2025-07-07 15:12:38-0500",
            "replies": [
              {
                "author": "samhh",
                "content": "<p>But does that not prevent users from then having their own individual per-repo settings?</p>\n<p>What if I’m in a project with three other people who all want a heavy extension but I don’t, but I still want to share things like debug configs?</p>",
                "createdAt": "2025-07-08 04:42:07-0500",
                "replies": [
                  {
                    "author": "matklad",
                    "content": "<p>Yeah, that’s a missing layer of settings. Curiously, IJ gets this part right: it has global settings, (vcs) shared project settings, and local project settings.</p>\n<p>Though, the layout of .idea was very messy back in the day, it contained both configuration and state, and you needed to know which subset of .idea to commit. And, yeah, the whole thing that setting are mostly GUI rather than a user-editable config file….</p>",
                    "createdAt": "2025-07-08 09:30:53-0500",
                    "replies": [
                      {
                        "author": "hoistbypetard",
                        "content": "<p>It’s still rather messy, IMO. But they include a <code>.gitignore</code> inside <code>.idea</code> now which has made me happy to add it to my repos even when I plan to share them, without spending energy looking up which individual files I should add.</p>",
                        "createdAt": "2025-07-08 10:37:17-0500",
                        "replies": []
                      },
                      {
                        "author": "tomjakubowski",
                        "content": "<p>It isn’t a missing layer, it’s there: .code-workspace files.</p>",
                        "createdAt": "2025-07-08 13:55:05-0500",
                        "replies": [
                          {
                            "author": "altano",
                            "content": "<p>These aren’t as good because they are about more than just settings, eg you have to open the project different when using them, etc. I usually advise just not putting settings in the project if you can’t get 100% agreement.</p>",
                            "createdAt": "2025-07-08 14:19:39-0500",
                            "replies": [
                              {
                                "author": "tomjakubowski",
                                "content": "<p>I just keep a directory of .code-workspace files for my most used projects. Pretty often those .code-workspace files will open a multi-root workspace (another obscure vscode feature I wish had more publicity). It’s fine for my needs.</p>",
                                "createdAt": "2025-07-08 20:16:56-0500",
                                "replies": []
                              }
                            ]
                          },
                          {
                            "author": "matklad",
                            "content": "<p>TIL, thanks!</p>",
                            "createdAt": "2025-07-08 15:03:20-0500",
                            "replies": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "author": "tomjakubowski",
                    "content": "<p>You can save a .code-workspace file in vscode which has whatever config you want in it, and then open that workspace file to work on your project. They’ll be merged with the config from .vscode/settings.json. This is one way to keep controversial settings out of .vscode/settings.json.</p>\n<p><a href=\"https://code.visualstudio.com/docs/editing/workspaces/multi-root-workspaces#_settings\" rel=\"ugc\">https://code.visualstudio.com/docs/editing/workspaces/multi-root-workspaces#_settings</a></p>",
                    "createdAt": "2025-07-08 13:51:12-0500",
                    "replies": []
                  }
                ]
              }
            ]
          },
          {
            "author": "dmbaturin",
            "content": "<p>I think the same applies to VCS hooks. You can’t casually allow <em>every</em> remote repository to inject scripts into your setup. But there’s still value in sharing hooks between people, and there’s no ready abstraction for it, to my knowledge.</p>",
            "createdAt": "2025-07-07 14:20:52-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "dpc_pw",
        "content": "<p>Nix dev env is the only supported dev env in my projects and if you don’t like it, don’t use it. I am the main developer and maintainer, so the project structure is serving <em>me</em>, not some random anon with opinions.</p>\n<p>Instead of trying to change how I work with my own project, it would be more productive to ask direnv project for features that would allow you to cleanly opt-out (if <code>direnv deny</code> is not already enough). Possibly <code>.direnv.local</code> + <code>direnv allow --only-local</code> would allow you to write out your own preferred config and ignore the recommended and supported defaults, so we can all be happy.</p>",
        "createdAt": "2025-07-07 14:23:12-0500",
        "replies": [
          {
            "author": "offby1",
            "content": "<p>Given that direnv’s own developers don’t commit their <code>.envrc</code> it is possible that they think otherwise.</p>",
            "createdAt": "2025-07-07 14:21:23-0500",
            "replies": [
              {
                "author": "dpc_pw",
                "content": "<p>I’m happy to use direnv how I see fit without worrying about what direnv developers think.</p>",
                "createdAt": "2025-07-07 14:26:39-0500",
                "replies": [
                  {
                    "author": "offby1",
                    "content": "<p>Fair enough. I’m happy to post an article about my opinion without worrying about what you think of it. I pretty comprehensively disagree with your position, as do direnv’s developers <em>and</em> some of the nix contributors. By all means persist, though. I won’t stop you.</p>",
                    "createdAt": "2025-07-07 14:41:31-0500",
                    "replies": [
                      {
                        "author": "mtlynch",
                        "content": "<blockquote>\n<p>Fair enough. I’m happy to post an article about my opinion without worrying about what you think of it.</p>\n</blockquote>\n<p>Isn’t the purpose of sharing your article on Lobsters to get feedback about it?</p>\n<p>I think dpc_pw’s criticisms are fair, and the responses you’re giving them are pretty snarky and nonconstructive.</p>\n<p>I think your article (and this discussion) would be more effective if you offered a path for maintainers to achieve what they want with the <code>.envrc</code> that’s compatible with what you want.</p>",
                        "createdAt": "2025-07-07 16:22:20-0500",
                        "replies": [
                          {
                            "author": "offby1",
                            "content": "<p>Your point about proposing a solution is valid. I’ve updated my post with a positive recommendation. As to my tone, eh, I’m not feeling myself as the source here; this started with dpc saying they intended to ignore my opinions and direnv’s developers’ opinions. I don’t feel like I escalated that at all, so I’m not gonna accept that particular criticism.</p>",
                            "createdAt": "2025-07-07 17:24:13-0500",
                            "replies": [
                              {
                                "author": "kxli",
                                "content": "<blockquote>\n<p>I don’t feel like I escalated that at all</p>\n</blockquote>\n<p>I agree with you and I’m also surprised that you were called out for your tone, when the reply said “not some random anon with opinions” and “without worrying about what direnv developers think” which is not directly rude, but still feels snarky. Especially compared to other replies to this post which also disagreed but did it in a much <em>softer</em> way.</p>",
                                "createdAt": "2025-07-08 06:15:52-0500",
                                "replies": []
                              }
                            ]
                          }
                        ]
                      },
                      {
                        "author": "dpc_pw",
                        "content": "<p>Seriously though, you’re barking up the wrong tree. The conflict here is the ownership of <code>.envrc</code>. Does it belong to “the project”, or each individual contributor. But it’s an artificial and easy to resolve conflict - <code>direnv</code> could just support both, and allow users to work with it.</p>",
                        "createdAt": "2025-07-07 15:21:15-0500",
                        "replies": [
                          {
                            "author": "offby1",
                            "content": "<p>Would you consider them having explicitly rejected that approach as a reason to reconsider your assumptions? <a href=\"https://github.com/direnv/direnv/issues/556\" rel=\"ugc\">Because they have</a>.</p>",
                            "createdAt": "2025-07-07 15:49:47-0500",
                            "replies": [
                              {
                                "author": "natkr",
                                "content": "<p>We must’ve been reading different issue threads or something.</p>\n<p>In the one you linked, zimbatm (original dev and the only actual org member involved) initially mentioned a design for local config files that is pretty similar to what <a href=\"https://lobste.rs/~dpc_pw\" rel=\"ugc\">@dpc_pw</a> suggested but quickly bowed out of the discussion… and a bunch of randos going back and forth. The closest I can see would be <a href=\"https://github.com/direnv/direnv/issues/556#issuecomment-2291475941\" rel=\"ugc\">gvolpe’s plea</a>, but that’s far from “explicitly rejected by direnv’s developers”.</p>",
                                "createdAt": "2025-07-07 18:46:02-0500",
                                "replies": []
                              },
                              {
                                "author": "dpc_pw",
                                "content": "<p>I really don’t care.</p>\n<p>If it bothered me much, I would just fork and add this feature, <a href=\"https://github.com/helix-editor/helix/pull/4847\" rel=\"ugc\">just like I do e.g. for Helix</a>. It’s open source. Anyone can ask claude code to impl this feature, and it would probably one-shot it.</p>\n<p>I really don’t want to be getting in another side-project now, but I bet I could rewrite the whole thing in Rust in under 1k lines, and e.g. fix <a href=\"https://github.com/direnv/direnv/issues/1461\" rel=\"ugc\">the hangs I’ve reported in the past myself</a>.</p>",
                                "createdAt": "2025-07-07 19:21:17-0500",
                                "replies": []
                              }
                            ]
                          },
                          {
                            "author": "peterbourgon",
                            "content": "<blockquote>\n<p>The conflict here is the ownership of .envrc. Does it belong to “the project”, or each individual contributor.</p>\n</blockquote>\n<p>I don’t think there’s any ambiguity here – if you check in an .envrc then, by definition, it belongs to the project. And that’s because <code>direnv</code> – currently, and apparently very much by design – will, by definition, always (attempt to) evaluate any/every .envrc file it encounters, by default.</p>",
                            "createdAt": "2025-07-08 10:25:05-0500",
                            "replies": [
                              {
                                "author": "rovaughn",
                                "content": "<p>To be clear, by “will, by definition, always (attempt to) evaluate any/every .envrc file it encounters, by default.”, does that include when it initially notices a .envrc but refuses to load it and prompts the user to explicitly <code>direnv deny</code> or <code>direnv allow</code>? I don’t think I’ve ever seen it automatically load an .envrc I didn’t explicitly allow.</p>",
                                "createdAt": "2025-07-08 19:21:31-0500",
                                "replies": [
                                  {
                                    "author": "peterbourgon",
                                    "content": "<blockquote>\n<p>To be clear, by “will, by definition, always (attempt to) evaluate any/every .envrc file it encounters, by default.”, does that include when it initially notices a .envrc but refuses to load it and prompts the user to explicitly direnv deny or direnv allow?</p>\n</blockquote>\n<p>Yes, but maybe I didn’t phrase what I was trying to say particularly well. The original question was about whether a checked-in .envrc “belongs” to the project itself, or to each individual contributor. My point was that direnv always and unavoidably <em>considers</em> every .envrc file it encounters. When you clone a repo locally, you can direnv deny and opt-out of evaluating the file, or direnv allow and opt-in to evaluating it, but whatever you decide, the outcome is both automatic and all-or-nothing – you evaluate the .envrc every time you cd into the dir, and you don’t get to pick which parts of the .envrc that you want to evaluate, you always evaluate the whole thing. And because the .envrc checked in, you can’t make any changes to that file that are just for you, they’re gonna be seen and evaluated by everyone. That combination of automatic and complete consideration/evaluation, and the file being checked-in, means it pretty clearly “belongs” to the repo as a whole, rather than to any individual contributor.</p>",
                                    "createdAt": "2025-07-09 12:21:21-0500",
                                    "replies": []
                                  }
                                ]
                              }
                            ]
                          }
                        ]
                      }
                    ]
                  }
                ]
              },
              {
                "author": "kranzes",
                "content": "<p>Btw, direnv’s own developers quite literally commit their Nix .envrc basically everywhere. I guess you’re just not familiar with their other projects.</p>",
                "createdAt": "2025-07-08 05:49:39-0500",
                "replies": []
              }
            ]
          },
          {
            "author": "chinmay",
            "content": "<p>i don’t even use nix and i agree, i will commit whatever i please to my project</p>",
            "createdAt": "2025-07-07 23:23:09-0500",
            "replies": []
          },
          {
            "author": "Psentee",
            "content": "<blockquote>\n<p>Nix dev env is the only supported dev env in my projects and if you don’t like it, don’t use it. I am the main developer and maintainer, so the project structure is serving me, not some random anon with opinions.</p>\n</blockquote>\n<p>Fine. But keep your dev env away from <em>my</em> shell. Let me customize it (say, I want to use your setup, but add one or two packages I like on top of it, or fine-tune env vars). Give me the setup, but don’t force me to use it (I know I can <code>direnv deny</code> it, this still won’t let me put my own <code>.envrc</code> there (it would probably source yours anyway and customize it, IF YOU WOULD ONLY LET ME DO THAT)). &lt;/rant&gt;</p>",
            "createdAt": "2025-07-08 03:26:38-0500",
            "replies": [
              {
                "author": "dpc_pw",
                "content": "<blockquote>\n<p>But keep your dev env away from my shell.</p>\n</blockquote>\n<ol>\n<li>Install software that does a thing.</li>\n<li>Clone my repo.</li>\n<li>Complain that software is doing a thing, “keep your repo away from my shell”.</li>\n</ol>",
                "createdAt": "2025-07-08 11:31:36-0500",
                "replies": []
              },
              {
                "author": "Rovanion",
                "content": "<p>Why not? Just put it there and don’t check it in.</p>",
                "createdAt": "2025-07-08 05:31:37-0500",
                "replies": [
                  {
                    "author": "Psentee",
                    "content": "<p>It’s annoying to never have <code>git status</code> clean, persistent junk in <code>git diff</code>, not being able to just <code>git add .</code>, and so on.</p>",
                    "createdAt": "2025-07-08 07:52:25-0500",
                    "replies": [
                      {
                        "author": "Rovanion",
                        "content": "<p>Perhaps a solution then could be to ask the maintainer about adding it to the ´.gitignore´.</p>",
                        "createdAt": "2025-07-08 09:32:49-0500",
                        "replies": [
                          {
                            "author": "pushcx",
                            "content": "<p>The Lobsters <a href=\"https://github.com/lobsters/lobsters/blob/884164b4f5147587a96aa28ec0352af9638b9228/.gitignore#L3\" rel=\"ugc\">.gitignore</a> explains how devs can configure their own setup so the project doesn’t have to take on this distraction.</p>",
                            "createdAt": "2025-07-08 11:28:33-0500",
                            "replies": [
                              {
                                "author": "hoistbypetard",
                                "content": "<p>Using the global ignore file like that gets to be a challenge for files that you want checked in for some projects but not for others. I’ve found using multiple <code>config</code> files as described <a href=\"https://thecomalley.github.io/git-configuration\" rel=\"ugc\">here</a> to be a good solution for having different emails associated with different repos while having a decent default for all of them, and it’d absolutely work for ignore as well.</p>",
                                "createdAt": "2025-07-08 13:01:41-0500",
                                "replies": [
                                  {
                                    "author": "tomjakubowski",
                                    "content": "<p>You can also do it on a per-repo basis, by editing .git/info/exclude.</p>",
                                    "createdAt": "2025-07-08 13:42:21-0500",
                                    "replies": [
                                      {
                                        "author": "hoistbypetard",
                                        "content": "<p>Yeah, I absolutely do that. I was excited about this approach because it lets you have groups of repositories configured the same way without depending on upstream pre-configuration.</p>",
                                        "createdAt": "2025-07-08 14:09:32-0500",
                                        "replies": []
                                      }
                                    ]
                                  }
                                ]
                              },
                              {
                                "author": "Rovanion",
                                "content": "<p>Yeah that’s even better, thanks for the idea!</p>",
                                "createdAt": "2025-07-08 18:52:12-0500",
                                "replies": []
                              }
                            ]
                          }
                        ]
                      }
                    ]
                  }
                ]
              }
            ]
          },
          {
            "author": "rvrb",
            "content": "<p>on top of this, if something like this is enough to keep you from contributing, I think I probably didn’t want you to contribute in the first place.</p>",
            "createdAt": "2025-07-07 16:22:15-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "wucke13",
        "content": "<p>As an avid Nix user I often wondered how maintainers feel when I as an outside, small-time contributor ask for a different shebang or the removal of hard-coded <code>/bin/rm</code> et al. in scripts to improve my quality of life when using <em>their</em> tool/software/repo on <em>my</em> Nix based environment.</p>\n<p>Now with the roles reversed, I default to feeling my self on the moral high ground, why would <em>I</em> have to cater to the QoL needs of <em>someone else</em> when my way is clearly <em>superior</em> (at least from my point of view)?</p>\n<p>(This is just my first impulse, I fully understand that this is not a balanced, reasonable standpoint)</p>\n<p>I can rationalize quite well why this specific feedback still does not apply to me and my repos, so sorry OP, I will likely not comply. However, I learned something, broadened my horizon, so to say. Thank your for causing that.</p>",
        "createdAt": "2025-07-07 17:47:56-0500",
        "replies": [
          {
            "author": "Psentee",
            "content": "<p>FWIW, fixing the shebang (assuming you mean e.g. <code>#!/usr/bin/env bash</code> instead of <code>#!/bin/bash</code>) fixes it also for *BSD users (where bash isn’t in base system and it’s usually <code>/usr/local/bin/bash</code> from ports or something like that) and for some MacOS users (MacOS ships with <code>/bin/bash</code>, but many homebrew users would prefer to use homebrew’s bash which is <code>/usr/local/bin/bash</code> on intel and <code>/opt/homebrew/bin/bash</code> on ARM, either way it will be on <code>$PATH</code> and <code>env</code> will pick the right one)</p>",
            "createdAt": "2025-07-08 03:44:52-0500",
            "replies": []
          },
          {
            "author": "ritualmachine",
            "content": "<p>The difference between the two is that in the one instance, you are working to make software <strong>more</strong> robust, being able to handle environments that the authors did not originally intend to be able to work under.</p>\n<p>Depending on a FOSS’ dev’s outlook, this could be very graciously accepted. It’s fallen out of fashion in this iteration of the zeitgeist, but software being deployable to unexpected/unplanned environments used to be something folks wore with pride.</p>\n<p>And on the other hand, you yourself are explicitly setting up your build system to work within a more limited environment, one where you don’t have to forsee as many hiccups because for one to use it they’ll have already bought in to all the same assumptions about an environment that you have.</p>",
            "createdAt": "2025-07-08 10:23:31-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "zimbatm",
        "content": "<p>Hey, direnv author here.</p>\n<p>I agree with OP’s observation; it creates a conflict when both the user and the project wants to use direnv differently.</p>\n<p>My initial position was that people should not commit their <code>.envrc</code>. That didn’t work. It’s too useful to be able to clone a repo and have your <code>.envrc</code> with it. If you look at my more recent projects, you can see me doing it as well.</p>\n<p>The proper solution is probably to add another layer of <code>.envrc</code> for the local user, that can override whatever is checked in the repository. Something like this idea: <a href=\"https://github.com/direnv/direnv/issues/556#issuecomment-576710201\" rel=\"ugc\">https://github.com/direnv/direnv/issues/556#issuecomment-576710201</a></p>\n<p>Let me know if you’re interested to work on something like this. It could be a nice feature to add.</p>",
        "createdAt": "2025-07-08 11:31:41-0500",
        "replies": [
          {
            "author": "offby1",
            "content": "<p>I’m curious how an out of tree <code>.envrc</code> would handle conflicts with an in-tree <code>.envrc</code>; I didn’t see a link to a design for the out of tree one, outside of your comment, but I’d be interested in at least going over the usecases.</p>\n<p>(also, thank you for what is probably the single most significant developer usability tool in my entire toolbelt; direnv is amazing)</p>",
            "createdAt": "2025-07-08 11:52:09-0500",
            "replies": [
              {
                "author": "zimbatm",
                "content": "<p>Assuming the user should be in control of their system, that dictates if we have “user-envrc” and “repo-envrc” (let’s ignore their location for now), the “user-envrc” should take precedence.</p>\n<p>That also means that if the user wants to load the repo-envrc, they can still <code>source_env</code> the repo-envrc. And control what they want to change before and after the repo-envrc.</p>\n<p>In terms of naming the user-envrc; <code>.envrc.private</code> and <code>.envrc.local</code> are widely used already, and direnv takes back-compat seriously. That’s why it would be easier to use a naming scheme outside of the current repository, like ~/.config/direnv/user/&lt;hash(path)&gt;<code>, and add </code>direnv edit –user` to keep this convenient to use.</p>\n<p>(thanks!)</p>",
                "createdAt": "2025-07-08 12:34:06-0500",
                "replies": []
              }
            ]
          }
        ]
      },
      {
        "author": "adriano",
        "content": "<p>I can hardly disagree with this more. First and foremost, project maintainers are allowed to make choices that not everyone agrees with. Second, it seems almost disingenuous not to mention that you have to <code>direnv allow</code> before <code>.envrc</code> affects your shell at all. And finally, nix is a dev dependency that you’ve chosen not to install, as is your choice. But that doesn’t absolve you of the following outcome:</p>\n<blockquote>\n<p>My editor reads .envrc files, so I see errors there. My shell does, too, so that fails.</p>\n</blockquote>\n<p>You said earlier that you like making open source contributions. As such, I’m sure you can understand how much toil and developer support nix and direnv help maintainers avoid, by ensuring developers all share the same environments (gave or take a bit).</p>\n<p>I’ll recommend this as your future first contribution. If a project’s <code>.envrc</code> is missing a <code>has nix</code> or similar check for necessary tooling, then it should be corrected.</p>\n<pre><code>if ! has nix; then echo \"Directions/URLs/Wikis on how non-nix users should set up their development environment. And if someone like yourself feels so strongly about contributing to a project that requires nix as a dev dependency, but not install it, then contribute your own code that solves all the problems that nix does, and run it here.\" fi </code></pre>\n<p>Denying nix users permission to use <code>.envrc</code> as they see fit is not the solution. Focus your frustrations on a solution that doesn’t malign project maintainers.</p>",
        "createdAt": "2025-07-08 08:31:32-0500",
        "replies": []
      },
      {
        "author": "raito",
        "content": "<p>Would something like:</p>\n<pre><code>if command -v nix; then use nix fi </code></pre>\n<p>be better?</p>",
        "createdAt": "2025-07-07 14:29:52-0500",
        "replies": [
          {
            "author": "tonyfinn",
            "content": "<p>direnv has a <a href=\"https://direnv.net/man/direnv-stdlib.1.html#codehas-ltcommandgtcode\" rel=\"ugc\"><code>has nix</code></a> (or other binary) which I’ve started using for this.</p>\n<p>The <code>.envrc</code> also sets variables for the “default/dev” database setup and a few other things so stuff works out of the box, though I do also use <code>source_env_if_exists .envrc.local</code> if people want to add their own gitignored extra config.</p>",
            "createdAt": "2025-07-07 14:42:48-0500",
            "replies": [
              {
                "author": "bdesham",
                "content": "<p>Yeah, I feel like <code>source_env_if_exists .envrc.personal</code> is the best solution here. That way, the “base” envrc can set any environment variables that are truly shared between all developers, and the local or personal envrc files can do anything else an individual user wants, without affecting anyone else.</p>",
                "createdAt": "2025-07-07 15:27:06-0500",
                "replies": [
                  {
                    "author": "offby1",
                    "content": "<p>The huge risk there is that those alternative sourced files aren’t hashed by direnv to watch for malicious upstream files; if an upstream is compromised and starts providing one of those, it’ll be loaded immediately, potentially executing arbitrary code.</p>",
                    "createdAt": "2025-07-07 15:46:25-0500",
                    "replies": []
                  }
                ]
              }
            ]
          },
          {
            "author": "offby1",
            "content": "<p>For my part? No, I don’t want the upstream repository to be opinionated about what my <em>local</em> development experience is. I’m not them, they’re not me, and <code>.envrc</code> and its effects are very personal to each developer, in my experience. A <code>.envrc.sample</code> makes sense; <code>.envrc</code> does not.</p>",
            "createdAt": "2025-07-07 14:39:50-0500",
            "replies": [
              {
                "author": "benjajaja",
                "content": "<p>So it’s not about nix then, it’s about <code>.envrc</code>.</p>\n<p>Unless you want to classify things “allowed” there, which is quite murky.</p>",
                "createdAt": "2025-07-08 04:40:57-0500",
                "replies": [
                  {
                    "author": "offby1",
                    "content": "<p>It’s about <code>.envrc</code>, but nix projects are 100% of the cases where I’ve seen <code>.envrc</code> committed; that may not be globally accurate but I have not seen exceptions.</p>",
                    "createdAt": "2025-07-08 11:47:18-0500",
                    "replies": [
                      {
                        "author": "benjajaja",
                        "content": "<p>Fair point.</p>\n<p>As a nix user, it’s cool to be able to clone a project and having everything work immediately. But cloning and needing to copy one .envrc.sample would not be a problem at all - after all on some projects you already need to do that for .env also.</p>\n<p>I will consider just providing an .envrc.sample in my next OSS projects.</p>",
                        "createdAt": "2025-07-08 14:08:44-0500",
                        "replies": []
                      }
                    ]
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "author": "rvrb",
        "content": "<p>wait a minute. direnv has an explicit whitelisting process. the only way I can imagine that this would become a problem is if you were blindly whitelisting and executing envrc files from third-party repositories.</p>",
        "createdAt": "2025-07-07 16:34:55-0500",
        "replies": [
          {
            "author": "quasi_qua_quasi",
            "content": "<p>It’s not a problem of trust, rather that if your .envrc has <code>use nix</code> then anyone who doesn’t have it installed is going to see errors in anything that tries to load and run the envrc. And an error in envrc generally means “something is actively wrong”.</p>",
            "createdAt": "2025-07-07 16:38:34-0500",
            "replies": [
              {
                "author": "rvrb",
                "content": "<p>well, by whitelisting you’ve also consented to using the repository’s development environment. complaining about it then breaking when you don’t have the required tooling seems misguided.</p>\n<p>however, on looking into this, I do tend to agree with the OP that <code>.envrc</code> should not be committed for one reason only: because direnv provides no clear way for the OP to define their own overrides. that sucks.</p>\n<p>we should probably just be committing an <code>.envrc.example</code> and symlinking it, or something.</p>\n<p>ideally, though, direnv could configure local overrides.</p>",
                "createdAt": "2025-07-07 16:46:37-0500",
                "replies": []
              }
            ]
          }
        ]
      },
      {
        "author": "xfbs",
        "content": "<p>I <strong>hate</strong> using direnv. We use it at work. I cd into a directory — maybe on purpose, maybe I’m just looking around — direnv runs. Nix downloads 2 GB from the cache. I’m blocked on doing anything useful for 2 minutes. Instead of me controlling the machine, it controls me (I am slightly exaggerating). But yeah, I feel like typing <code>nix develop</code> is not hard and I would rather do it explicitly.</p>\n<p>My solution? Commit all of the <code>.direnv</code> files you like! I just don’t have it installed. At least on my personal machine. Problem solved.</p>\n<p>I feel like direnv can be useful for people that do Python. But I think that is more of a symptom of a problem. When you use Cargo, it will download and install dependencies in your repo, not system-wide. To get Python to play nice, you need to trick it into thinking it can install things system-wide but contain it in a virtual environment. So to do anything useful, you need to enter that environment. I’m not sure if I am oversimplifying but is that a reason why these tools are popular?</p>",
        "createdAt": "2025-07-08 04:49:32-0500",
        "replies": [
          {
            "author": "MoonlitKnight",
            "content": "<blockquote>\n<p>My solution? Commit all of the .direnv files you like! I just don’t have it installed. At least on my personal machine. Problem solved.</p>\n</blockquote>\n<p>Wouldn’t the same thing be achieved by not calling <code>direnv allow</code> in the directory in question?</p>",
            "createdAt": "2025-07-08 09:32:05-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "Tenzer",
        "content": "<p>This is one of those cases where developers should use their global <code>~/.config/git/ignore</code> file to ignore <code>.envrc</code> files in all repositories so they don’t ever get included. The same goes for settings files for whatever editor people choose to use. It’s not up to the git ignore files in every repository you touch to have entries for whichever editor you decide to use.</p>\n<p>I wish people would use the global git ignore file more frequently in order to not pollute repositories they work in with their local dev preferences.</p>",
        "createdAt": "2025-07-07 17:19:25-0500",
        "replies": [
          {
            "author": "hoistbypetard",
            "content": "<p>An adjacent thing: I use different email addresses for different groups of repositories. Until very recently, I accommodated this by only setting <code>user.name</code> but not <code>user.email</code> globally, instead using per-repository configuration. The pitfall of this is my first commit in a repo is often first attributed to <code>user@machine</code> instead of a valid email, until I amend that.</p>\n<p>I recently learned <a href=\"https://thecomalley.github.io/git-configuration\" rel=\"ugc\">about the includeIf directive for git config</a>. That completely killed this particular papercut for me. And it looks like it’d be a good way to have a different ignore file for groups of projects. The main reason I want that is because I do globally ignore <code>.env</code> files, accept in one tree where I am specifically accumulating various useful ones and not publishing it anywhere. Until you mentioned this global ignore file, it hadn’t occurred to me that I could give ignore the same treatment.</p>",
            "createdAt": "2025-07-08 08:48:37-0500",
            "replies": []
          },
          {
            "author": "xfbs",
            "content": "<p>That’s a good point actually. I don no make use of this. And maybe I should set it up in a sensible way.</p>\n<p>Semi-related: I configired vim to not write swap files because I don’t want to accidentally commit them.</p>",
            "createdAt": "2025-07-08 04:58:11-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "dxo",
        "content": "<p>I don’t understand, don’t you have to actively opt in to use the checked in envrc? Can’t you just ignore the ones you don’t like?</p>",
        "createdAt": "2025-07-08 01:12:20-0500",
        "replies": [
          {
            "author": "Psentee",
            "content": "<p>Yeah, but when I want to use my own <code>.envrc</code> or modify the vendor’s one (because it’s my shell, not vendor’s), I’ll have to tiptoe around a permanently modified file in git: never have clean <code>git status</code>, junk in <code>git diff</code>, conflicts on pull when upstream modifies their <code>.envrc</code>, can’t use <code>git add .</code>, and so on.</p>",
            "createdAt": "2025-07-08 03:49:23-0500",
            "replies": [
              {
                "author": "dxo",
                "content": "<p>it’s a single line: use flake, and for my projects that have a checked in .envrc, nix is such an integral part of the build system that using anything else is going to make your life annoying enough that one little dirty file in the repo is honestly the least of your problems.</p>\n<p>I’m sure I’m missing something, but I have never personally wanted to put anything in a .envrc except “use flake”. What are you putting in there beyond that?</p>",
                "createdAt": "2025-07-08 04:22:20-0500",
                "replies": [
                  {
                    "author": "Psentee",
                    "content": "<p>Some extra packages that you haven’t put in your flake’s dev dependencies – say, not every python project mandates specifically pyright &amp; ruff, and I run these anyway for my own convenience.</p>\n<p>Customize env vars – say, for Rust projects, it’s <code>RUST_BACKTRACE=full</code> (it’s not for build, it’s a preference for local development, so its place is not in flake) &amp; <code>RUST_LOG=info,module_i_am_working_on=trace</code> (tuned for the thing I’m working on, not an always-right default).</p>\n<p>Since we’re with Rust projects, I like to have things like <code>cargo-expand</code>, <code>rust-analyzer</code>, <code>cargo-nextest</code> (which is just a runner, so I can use it with regular tests).</p>\n<p>With emacs direnv integration + LSP, when the source files are not inside direnv, they “lose” their direnv context. When Rust deps are in <code>~/.cargo</code> or Python’s virtualenv is in <code>~/.py-venv-manager-of-the-week/</code>, I can jump-to-definition from my source, but then I’m outside of the direnv and lose LSP. So I’m configuring whatever language I’m working with to put dependencies inside of direnv – for Rust, it’s <code>export CARGO_HOME=\"$(dirname \"$(realpath \"${BASH_SOURCE[0]}\")\")/target/.cargo\"</code> in <code>.envrc</code>. That’s a quirk of my personal setup and a result of how I manage my workstation, not something that every contributor must be happy with.</p>\n<p>Before I discovered <code>zsh-completion-sync</code> plugin, I had some <code>FPATH</code> manipulation to get autocompletion for commands added to my path by direnv.</p>\n<p>I have <code>WINIT_UNIX_BACKEND=wayland</code> for some projects to prevent it from trying to use XWayland.</p>\n<p>Sometimes I use <code>use flake . --override-input nixpkgs nixpkgs</code> to make it use nixpkgs that my nixos uses (fairly recent) to avoid having fifteen different revisions of nixpkgs and all the nested deps cluttering my nix store.</p>\n<p>All of the above are my personal preferences, taken out of actual <code>.envrc</code> files on my workstation, and none of these are something that would belong in the mandatory <code>.envrc</code> for all the contributors. Some more hypotheticals:</p>\n<p>I might want to add a prefix to my PS1 to clearly hint which project I’m working on.</p>\n<p>I might add a <code>source_up</code> to include some common configuration for a bunch of projects.</p>\n<p>I might want to make my computer go “ding” whenever I <code>cd</code> into your project. That’s entirely valid thing for me to want.</p>\n<p>I might want to set <code>GITHUB_TOKEN</code> to my work account creds rather then my personal account, or the other way around.</p>\n<p>I might want to set <code>GIT_{AUTHOR,COMMITTER}_{NAME,EMAIL}</code> to my personal data, work data, one of my pseudonyms, etc. Override <code>GIT_SSH_COMMAND</code> to force it to use the right private key. And so on.</p>\n<p>What goes in my shell when I <code>cd</code> anywhere is my configuration, not yours. I’d appreciate your <code>.envrc.example</code> (and probably <code>source_env</code> it, but if it’s one line I’d just put the line there). I appreciate that <code>use flake</code> is an integral part of the build system, and I’m going to have it in my <code>.envrc</code>, but I also want the rest of my tools to work right.</p>\n<p>And maybe I’m missing something, but I can’t see how any of the above would “make my life annoying” (except maybe <code>--override-input nixpkgs</code> if your project depends on some specific version, but that’s my risk and commenting it out when it breaks or when I want to do a final check is easy enough).</p>\n<p>And I want to be able to do it specifically because you as a maintainer don’t know how I’m configuring my shell, and certainly you don’t know better than me how to tune my workstation for my needs. It’s obviously your right as a maintainer to make it harder for me to contribute – if I care enough I’ll work around it, but definitely won’t feel welcome as a contributor or respected as a professional if you insist that your baseline is all the configuration I’d ever need.</p>\n<p>I believe that the best compromise is to use <code>.envrc.example</code> and add <code>.envrc</code> to <code>.gitignore</code> (one <code>ln -s</code> per git clone is not that much of an overhead), but if you insist on not letting me <code>--override-input</code>, at least add a <code>source_env_if_exists .envrc.local</code> to let me do the rest. It won’t change anything for you, and will allow me to use my quirky little workstation the way I want.</p>",
                    "createdAt": "2025-07-08 06:59:27-0500",
                    "replies": [
                      {
                        "author": "offby1",
                        "content": "<p>Thank you! I considered providing a similar list myself, but wasn’t sure if it would be convincing or cause people to glaze over my post. Your list is not a lot different from mine, it turns out, except I also do several of your hypotheticals as well :D</p>",
                        "createdAt": "2025-07-08 09:01:43-0500",
                        "replies": []
                      },
                      {
                        "author": "bdesham",
                        "content": "<blockquote>\n<p>I might want to make my computer go “ding” whenever I cd into your project. That’s entirely valid thing for me to want.</p>\n</blockquote>\n<p>Here’s a fun thing to add to your .envrc if you’re on macOS:</p>\n<pre><code>osascript -e \"say \\\"It's time to make the donuts\\\"\" </code></pre>",
                        "createdAt": "2025-07-08 08:47:12-0500",
                        "replies": []
                      },
                      {
                        "author": "bdesham",
                        "content": "<p>(OK, serious reply this time…)</p>\n<blockquote>\n<p>Before I discovered zsh-completion-sync plugin, I had some FPATH manipulation to get autocompletion for commands added to my path by direnv.</p>\n</blockquote>\n<p>Do you mind saying how you had the <code>FPATH</code> manipulation set up? Inspired by your comment, I’m trying to do the same thing, but it’s tricky because <code>FPATH</code> isn’t a normal variable, and if direnv tries to <code>unset</code> it then I lose my shell completions entirely.</p>",
                        "createdAt": "2025-07-10 08:53:16-0500",
                        "replies": [
                          {
                            "author": "Psentee",
                            "content": "<p>TBH, not sure. Maybe you need to <code>export</code> your FPATH in the first place? I never did it consciously, but maybe some part of oh-my-zsh or other snippets I accumulated did. It somehow worked, but I still had to rerun <code>compinit</code>. It was <em>very</em> clunky.</p>\n<p><a href=\"https://github.com/BronzeDeer/zsh-completion-sync\" rel=\"ugc\">https://github.com/BronzeDeer/zsh-completion-sync</a> does work and I can recommend this approach instead – <code>use flake</code> adds the devshell’s packages to <code>XDG_DATA_DIRS</code>, which is picked up by zsh-completion-sync</p>\n<p>ETA my <code>FPATH</code> snippet, found it in history:</p>\n<pre><code>if [[ -n \"$FPATH\" ]]; then export FPATH=\"${FPATH}:$(dirname $(dirname $(realpath $(which cargo))))/share/zsh/site-functions\" fi </code></pre>",
                            "createdAt": "2025-07-10 09:04:44-0500",
                            "replies": [
                              {
                                "author": "bdesham",
                                "content": "<p>Thanks very much!</p>",
                                "createdAt": "2025-07-10 11:08:25-0500",
                                "replies": []
                              }
                            ]
                          }
                        ]
                      }
                    ]
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "author": "Psentee",
        "content": "<p>My approach is to check in my <code>.envrc</code> (or a version without some most blatant personalization) as <code>.envrc.example</code>, and adding <code>.envrc</code> to <code>.gitignore</code>. You can symlink the example to <code>.envrc</code>, you can source it from <code>.envrc</code> and add more settings, or ignore it completely and use your own (or none at all). And it doesn’t try to change your environment without you explicitly doing that (your shell configuration is yours, not mine)</p>",
        "createdAt": "2025-07-08 03:29:25-0500",
        "replies": []
      },
      {
        "author": "kerem",
        "content": "<p>I completely agree with the author. .envrc itself is in the global .gitignore for me. It does not need to be in the repositories. That isn’t where it belongs.</p>",
        "createdAt": "2025-07-08 15:43:49-0500",
        "replies": []
      },
      {
        "author": "sigmonsez",
        "content": "<p>seems like a bit of old-man-yells-at-cloud.gif <a href=\"https://tenor.com/search/old-man-yells-at-cloud-gifs\" rel=\"ugc\">https://tenor.com/search/old-man-yells-at-cloud-gifs</a></p>",
        "createdAt": "2025-07-07 16:00:20-0500",
        "replies": [
          {
            "author": "offby1",
            "content": "<p>You are <em>not</em> wrong. I’m hoping to convince someone. Anyone. Even one change will help.</p>",
            "createdAt": "2025-07-07 23:21:16-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "darkone23",
        "content": "<p>If you are lovingly crafting .envrc files you should share them with the rest of the world.</p>\n<p>We already commit things like lock files and linter configs - is committing the developer shell much different?</p>",
        "createdAt": "2025-07-09 23:25:03-0500",
        "replies": []
      },
      {
        "author": "mccd",
        "content": "<p>Feels like the answer is just that projects should add an if check before use nix and source a .envrc.local that is gitignored? I dont think many devs would reject such a PR.</p>",
        "createdAt": "2025-07-07 17:24:49-0500",
        "replies": [
          {
            "author": "offby1",
            "content": "<p>Adding <code>source_env_if_exists .envrc.local</code> opens a security hole, since that file will not be checked for content on changes, making it a vector for remote code execution on clone.</p>",
            "createdAt": "2025-07-07 17:31:04-0500",
            "replies": [
              {
                "author": "mccd",
                "content": "<p>That same risk exists with nix shells right? I don’t need to reallow direnv when I change shell.nix.</p>",
                "createdAt": "2025-07-07 17:40:40-0500",
                "replies": []
              },
              {
                "author": "sjamaan",
                "content": "<p>But that file would bit gitignored, so the only place it can come from is the person who cloned the repo, right?</p>",
                "createdAt": "2025-07-08 07:40:20-0500",
                "replies": [
                  {
                    "author": "bdesham",
                    "content": "<p><code>git add -f</code> allows you to add a file to version control even if it matches one of your gitignore rules :-|</p>\n<p>(And it kind of makes sense that Git allows this… otherwise, how would it handle the case that you add file X to the repo, and then change the gitignore to cover file X? I suppose Git could refuse to commit a change to the gitignore that would cover existing files, but that’s not what it does.)</p>",
                    "createdAt": "2025-07-08 08:43:48-0500",
                    "replies": []
                  }
                ]
              },
              {
                "author": "knl",
                "content": "<p>How would this attack vector work? If you clone someone’s repo containing <code>.envrc</code>, you’re exposed to the same issue, no?</p>",
                "createdAt": "2025-07-08 02:17:50-0500",
                "replies": [
                  {
                    "author": "ehamberg",
                    "content": "<p>No. direnv would prompt you to allow it before it’s used.</p>",
                    "createdAt": "2025-07-08 03:25:49-0500",
                    "replies": []
                  }
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  {
    "postTitle": "systemd has been a complete, utter, unmitigated success",
    "comments": [
      {
        "author": "pmc",
        "content": "<p>I’m glad there’s someone else out there who unashamedly likes systemd. Sure, it has its problems, but it solves so many other obtuse problems that I’m happy to deal with its quirks.</p>",
        "createdAt": "2025-07-08 19:34:40-0500",
        "replies": [
          {
            "author": "ricardbejarano",
            "content": "<p>I actually think it’s the silent majority, as usual on post-social-media internet.</p>",
            "createdAt": "2025-07-09 06:33:59-0500",
            "replies": [
              {
                "author": "gerikson",
                "content": "<p>This is something that’s very common about customer satisfaction. When stuff you pay for works, you don’t say that. No-one is posting “man, I am so happy I have clean water and working electricity today!” (even if maybe they should). People complain when stuff isn’t working, and sometimes they spontanously state when something gets fixed in a good way. Stuff that works is just taken for granted.</p>",
                "createdAt": "2025-07-09 07:24:30-0500",
                "replies": [
                  {
                    "author": "viraptor",
                    "content": "<blockquote>\n<p>No-one is posting “man, I am so happy I have clean water and working electricity today!” (even if maybe they should).</p>\n</blockquote>\n<p>And people who do, have a great YouTube opportunity. TechnologyConnetions hit serious popularity after the dishwasher video. MapMen like maps. Etc. … Maybe we need someone who’s really into Linux syscalls or something like that.</p>",
                    "createdAt": "2025-07-09 09:25:50-0500",
                    "replies": [
                      {
                        "author": "bityard",
                        "content": "<p>Alec’s video are hit-or-miss for me lately. I never know whether I’m going to get one one with the tone of, “gee isn’t this thing we take (or took) for granted pretty nifty? I’d love to share it with you” or “you are all idiots for not knowing this thing that I know and therefore I’m going to spend the next 40 minutes preaching to you while being obnoxiously smug about it the whole time.”</p>",
                        "createdAt": "2025-07-09 11:25:55-0500",
                        "replies": []
                      }
                    ]
                  }
                ]
              },
              {
                "author": "lann",
                "content": "<p>The <em>silent</em> majority didn’t spend much time thinking about sysv init and doesn’t spend much time thinking about systemd. You learn to do your job with what you’re given. It is fine. Here comes a migration; you are annoyed for a few days/weeks/months and then you’ve learned the new thing. It is fine.</p>",
                "createdAt": "2025-07-10 07:17:09-0500",
                "replies": []
              }
            ]
          }
        ]
      },
      {
        "author": "quasi_qua_quasi",
        "content": "<blockquote>\n<p>Look, I respect that <code>stvpidcvnt111111</code> has a right to their opinion,</p>\n</blockquote>\n<p>I was going to say that making up a username for people you disagree with is crude behavior, especially putting a slur in it, but that really is that person’s username. Huh.</p>",
        "createdAt": "2025-07-09 01:02:45-0500",
        "replies": [
          {
            "author": "ficd",
            "content": "<p>I was thinking the exact same thing. I had to do a double take and check the source!</p>",
            "createdAt": "2025-07-10 15:17:49-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "kylewlacy",
        "content": "<blockquote>\n<p>I don’t think the ini-style configuration format is a panacea (I like Dhall), but that’s another olive branch from systemd authors to system administrators: it doesn’t require a turing-complete configuration format or domain-specific language. You can generally understand what this means when you read it and how to change it: […]</p>\n</blockquote>\n<p>I think the syntax for systemd units is… okay. For me it’s pretty easy to <em>read</em>, but I have to consult the manual every time I need to <em>write</em> a unit (or even for some edits, depending on what I’m changing). It’s still weird to me that array-like fields (like <code>Env</code>) are just duplicated to represent multiple items– which can get Especially Weird with overrides. I’m never sure why dependencies go in <code>[Unit]</code> vs <code>[Install</code>], and I’ll never remember how <code>oneshot</code> vs. <code>simple</code> services work. I really wish I didn’t have to make both a <code>.service</code>\n<em>and</em>\n<code>.timer</code> file just to e.g. schedule a shell script.</p>\n<p>But these are all pretty minor nitpicks, I’m really happy with systemd overall! There’s also a lot of good built-in tooling that easily balances out with my mild syntax complaints… like <code>systemd edit</code>, <code>systemd cat</code>, etc.</p>\n<p>Oh, and I wish more distros used <code>systemd-boot</code> by default. I haven’t actually used it yet, but I’m jealous of how simple it looks compared to grub. Plus it supports TPM-based unlocks with very little friction!</p>",
        "createdAt": "2025-07-09 03:22:43-0500",
        "replies": [
          {
            "author": "intelfx",
            "content": "<p>I realize this isn’t the point of your comment; however…</p>\n<blockquote>\n<p>I’m never sure why dependencies go in [Unit] vs [Install],</p>\n</blockquote>\n<p><code>[Unit]</code> is for <strong>regular</strong> dependencies: <code>Wants=</code>, <code>Requires=</code>, <code>Before=</code>, <code>After=</code>. Read as: <em>“this unit Wants that unit, Requires another unit”</em>.</p>\n<p><code>[Install]</code> is for <strong>reverse</strong> dependencies which are <strong>only</strong> applied when you enable the unit: <code>WantedBy=</code>, <code>RequiredBy=</code>, <code>Also=</code>. Read as: <em>“WHEN ENABLED, this unit is WantedBy that unit, Also enables another unit”</em>.</p>\n<blockquote>\n<p>and I’ll never remember how oneshot vs. simple services work</p>\n</blockquote>\n<ul>\n<li>\n<code>simple</code>: start the binary, assume it <em>keeps going indefinitely</em>, forget about it, continue.</li>\n<li>\n<code>oneshot</code>: start the binary, assume it <em>does one thing and exits</em>, wait for it to exit, only then continue.</li>\n</ul>",
            "createdAt": "2025-07-09 16:16:05-0500",
            "replies": [
              {
                "author": "terinjokes",
                "content": "<p>I’ll also note you probably don’t want “simple”, you want “exec” in almost all use cases.</p>\n<blockquote>\n<p>The exec type is similar to simple, but the service manager will consider the unit started immediately after the main service binary has been executed. The service manager will delay starting of follow-up units until that point. (Or in other words: simple proceeds with further jobs right after fork() returns, while exec will not proceed before both fork() and execve() in the service process succeeded.)</p>\n</blockquote>\n<p>For daemons you write yourself supporting type “notify” isn’t difficult and brings improvements by waiting to run dependent units until after this service has started and stabilized. (And, if you can, “notify-reload” to also gracefully handle the reload workflow)</p>",
                "createdAt": "2025-07-10 11:15:35-0500",
                "replies": []
              }
            ]
          },
          {
            "author": "ksynwa",
            "content": "<blockquote>\n<p>I really wish I didn’t have to make both a .service and .timer file just to e.g. schedule a shell script.</p>\n</blockquote>\n<p>This is why I still use cron for scheduled tasks. The process for creating timers seems abstruse to me as of now.</p>",
            "createdAt": "2025-07-09 03:36:19-0500",
            "replies": [
              {
                "author": "yawaramin",
                "content": "<p>Separation of concerns. The service unit takes care of running the job with the required isolation/security levels. The timer unit takes care of running it at the right times.</p>",
                "createdAt": "2025-07-09 10:18:08-0500",
                "replies": []
              },
              {
                "author": "cultpony",
                "content": "<p>There is systemd-crontab-generator, which ingests your system’s crontab and generates runtime .timer and .service definitions for it. You get the benefit for both; have a crontab with one place where it all lives and also having it all be native to systemd. Plus you can now start the crontab scripts without needing to wait for them to trigger, since it’s now a proper systemd unit.</p>",
                "createdAt": "2025-07-09 11:39:27-0500",
                "replies": [
                  {
                    "author": "strugee",
                    "content": "<p>This doesn’t fully work your distro sets up and likes to use <code>/etc/cron.{hourly,daily,weekly,monthly,yearly}</code>, which at least Debian does.</p>\n<p>Not saying it’s not useful, just that it isn’t a silver bullet.</p>",
                    "createdAt": "2025-07-10 13:52:39-0500",
                    "replies": []
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "author": "pointlessone",
        "content": "<p>I wonder how many people who like systemd started using Linux before it came about. I mean not just started but learned how it works and made a somewhat informed choice of an init.</p>\n<p>I wonder because I find systemd extremely opaque. It does a lot and I have no idea how it does it or why it does it in the first place. And it doesn’t look approachable to learn the intricacies of it. Its repo clocks in over 1.6 m lines of code. Yess not all of it is the actual code that goes into the binary but even if it’s just a third it’s hell of a reading.</p>\n<p>My preference is OpenRC. It’s definitely less feature-rich than systemd. But anyone can learn it through and through. It’s under 32k lines of code. It doesn’t provide socket activation or logging but it does <em>its</em> job.</p>\n<p>So has anyone learned an older init and then looked at systemd and said “yes, this is better”?</p>",
        "createdAt": "2025-07-09 03:12:11-0500",
        "replies": [
          {
            "author": "rweir",
            "content": "<blockquote>\n<p>I wonder how many people who like systemd started using Linux before it came about.</p>\n</blockquote>\n<p>lots? I’m definitely one of them, I was a working sysadmin and programmer long before systemd existed, and I’m extremely happy to:</p>\n<ul>\n<li>not have to write shitty shell scripts to start services anymore</li>\n<li>have services actually be reliably stoppable by the init system</li>\n<li>be able to trivially contain the privileges of services</li>\n<li>be easily able to search logs on a machine (<code>journalctl -S\"-1 day\" --grep \"thing_I_want\"</code> is much nicer than trying to decompress logs and grep for some dates before getting to my pattern)</li>\n<li>to have all that coupled to the <code>cron</code> system and the privilege-separating socket handling system</li>\n<li>to be able to specify dependencies between things in a structured manner, instead of changing the integer in a symlink</li>\n<li>be able to easily override most settings from upstream or distro unit files which are easily automatable, in the days of init scripts one would have to hope the script provided hooks or vars for whatever you wanted to do, or hack the shell script itself, which is extremely annoying from a configuration management system</li>\n</ul>\n<p><code>systemd-networkd</code> is also pretty nice for simple configs, it handles declaratively much more than <code>ifupdown</code> does, so all my interface hook scripts are gone and replaced with some lines in an ini file.</p>\n<p>all the above makes my life easier and makes it so much easier that I do more - even if something doesn’t have a systemd unit file, it’s only 2 minutes work to write one that will integrate with all the above and be Good, so I do, instead of being lazy and using a docker image or just not bothering at all. and now that sandboxing is “add a line to a config file”, I actually bother to do that for things.</p>\n<p>it’s not all perfect and I don’t think I’ll ever really like ini files, but it actually exists and makes my life much easier.</p>",
            "createdAt": "2025-07-09 05:50:16-0500",
            "replies": [
              {
                "author": "sjsadowski",
                "content": "<p>I too am in the “long before systemd” group.</p>\n<p>Having gone through this in real time, I also think that people are forgetting that there were competitors to systemd (Upstart? runit?) that failed for various reasons - not necessarily because they were bad but because of people weighing the technical merits of replacing sysvinit with something and what the scope should be of that thing, as well as the overall expectation of supporting contributors to help solve the problems as they’ve come up.</p>\n<p>I don’t love systemd, nor do I hate systemd. It is a thing that I work with, and it does a decent to manage, it’s decently documented, and I can expect consistent results. That’s fine by me.</p>",
                "createdAt": "2025-07-09 07:10:24-0500",
                "replies": [
                  {
                    "author": "swehren",
                    "content": "<blockquote>\n<p>I don’t love systemd, nor do I hate systemd. It is a thing that I work with, and it does a decent to manage, it’s decently documented, and I can expect consistent results. That’s fine by me.</p>\n</blockquote>\n<p>I feel exactly the same. My only complaint was years ago when it was initially deployed to whatever distro I was using at the time (maybe Arch, I don’t recall), and did not give consistent results, but actively broke things on my system. Maybe that’s arch’s fault maybe that’s systemd’s fault maybe it’s my fault, but that sour taste definitely put me in the hater camp for awhile.</p>\n<blockquote>\n<p>not necessarily because they were bad but because of people weighing the technical merits of replacing sysvinit with something and what the scope should be of that thing</p>\n</blockquote>\n<p>I think this is the other fair remaining criticism, although the ship has thoroughly sailed – does systemd need to be as large as it is or is there a smaller, simpler solution that would address the majority of the advantages?</p>",
                    "createdAt": "2025-07-09 09:56:24-0500",
                    "replies": [
                      {
                        "author": "wink",
                        "content": "<p>I only became somewhat of a hater by being called a hater because people ignored my valid criticism of “it simply works worse than before/it solves problems I personally never had”.</p>\n<p>That was many years ago and my problems have mostly been solved, so I’m not a hater anymore. Or maybe just accepted that it’s here to stay.</p>\n<p>X vs Wayland has that same energy. People who had problems with X have less now and people who were happy with X but Wayland (as distros deploy it, or certain DEs) doesn’t match their uses.</p>",
                        "createdAt": "2025-07-10 09:32:18-0500",
                        "replies": []
                      }
                    ]
                  }
                ]
              }
            ]
          },
          {
            "author": "worr",
            "content": "<blockquote>\n<p>So has anyone learned an older init and then looked at systemd and said “yes, this is better”?</p>\n</blockquote>\n<p>Yeah, I’ve used Linux in some capacity since 2005ish, and systemd is 100% a welcome improvement. I remember getting some exposure to Solaris’ SMF in 2008ish and thinking “holy shit, I wish Linux had anything like this,” although I had admittedly never tried to write a service manifest. I’ve used FreeBSD and OpenBSD quite a bit, and the latter showed me what I now consider the only reasonable init scripts (however, from my experience, I think init scripts are a bad design period). On Linux, I’ve used sysvinit, OpenRC and runit, all bad in different ways. I don’t care too much about socket activation, but having actual reliable service tracking and management is a huge win imo.</p>\n<p>With all that experience, I feel qualified in saying that I really like what systemd does, and while I’m not gonna say it’s been the most perfect transition or that some of the individual components don’t have flaws, I’m overall much happier with it than The Old Ways™.</p>",
            "createdAt": "2025-07-09 06:26:23-0500",
            "replies": [
              {
                "author": "calumleslie",
                "content": "<p>I too remember SMF! I worked on software that deployed to many Unices in around 2006 and I also remember SMF being a nice step up from traditional init scripts. I do remember it being syntactically clunky (XML I think?) but it was predictable and declarative (looking) in much the same sort of way as systemd tries to be.</p>",
                "createdAt": "2025-07-09 16:46:14-0500",
                "replies": [
                  {
                    "author": "aconbere",
                    "content": "<p>Yeah, this was my experience too. I’d been using linux and freebsd in a professional capacity since 2005 and started a job using Solaris in 2008 or so. I recall being super impressed with SMF, Zones, and ZFS.</p>\n<p>When SystemD got announced and there was so much hand wringing I was baffled. Having experienced SMF I was there thinking… “but this is clearly better!”</p>",
                    "createdAt": "2025-07-10 10:01:00-0500",
                    "replies": []
                  },
                  {
                    "author": "worr",
                    "content": "<p>Yup, XML, which is why I included the caveat about never writing a manifest 😅</p>\n<p>I’ll take an ini file over a big XML file any day</p>",
                    "createdAt": "2025-07-09 17:06:09-0500",
                    "replies": []
                  }
                ]
              }
            ]
          },
          {
            "author": "srtcd424",
            "content": "<blockquote>\n<p>So has anyone learned an older init and then looked at systemd and said “yes, this is better”?</p>\n</blockquote>\n<p>Hell yes. I’ve been using Linux since ‘94, so I’ve seen pretty much everything. I will admit that when systemd first appeared on the scene I was incredibly grumpy about it, mainly because as its tentacles slowly spread into the various distros, it gradually invalidated all my muscle memory, and similar things.</p>\n<p>One day I realised that it just wasn’t going to go away, and so basically cleared a couple of weeks in my schedule to read ALL the docs and do lots of experimentation. About the same time, I was either P2V-ing or V2C-ing a bunch of systems (I forget which), and the fact that neither sysvinit or upstart could easily answer the simple question “which services failed to boot?” tipped me over into being a systemd convert.</p>\n<p>TBF, I think the distros have often rolled out new systemd features before they’re quite ready, which probably doesn’t help with its reputation. resolved, for example, used to cause me a fair bit of grief a few years ago - now things have settled down, and I find myself preferring it.</p>\n<p>I don’t hate the systemd codebase either. Perhaps it’s because I’m about the same age as Lennart, but the terse style is pretty close to what I prefer. Having said that, it’s probably not a bad idea to start introducing rust, both for its reliability advantages, and to keep the next generation of coders down interested.</p>\n<p>I do still occasionally think that there must be a theoretically cleaner design out there, but I’m not sure what it is, and I’ve not seen any sensible suggestions. Realistically systemd has added so many useful features it would seem to daft to start over. Having said that, I can see some specific use cases might want something lighter, e.g. perhaps the embedded space, or intentionally simple desktops etc. I’ve not looked closely at Chimera Linux or <a href=\"https://github.com/davmac314/dinit\" rel=\"ugc\">dinit</a>, but it seems like they’re making a sensible simplification v compatibility trade-off, and I definitely wish both projects well, even if I’m not likely to end up as a user.</p>",
            "createdAt": "2025-07-09 06:03:33-0500",
            "replies": [
              {
                "author": "wink",
                "content": "<blockquote>\n<p>TBF, I think the distros have often rolled out new systemd features before they’re quite ready, which probably doesn’t help with its reputation.</p>\n</blockquote>\n<p>100%</p>\n<p>Times I had a laptop not shut down because anything was hanging endlessly, before systemd: 0</p>\n<p>Times I had a laptop not shut down because of systemd (first couple years): very often, different distros, different hw</p>",
                "createdAt": "2025-07-10 09:34:27-0500",
                "replies": []
              }
            ]
          },
          {
            "author": "kwas",
            "content": "<blockquote>\n<p>So has anyone learned an older init and then looked at systemd and said “yes, this is better”?</p>\n</blockquote>\n<p>I was an Arch user when the initscripts to systemd transition happened. I’ve read the manpages for the latter, said “neat”, and moved on. Then RHEL jumped from upstart to systemd, and I appreciated the swap to something I already was comfortable with.</p>\n<p>Previous designs either felt under-documented or too niche (each major distro having their own) for me to ever try to write a service file myself.</p>",
            "createdAt": "2025-07-09 04:11:37-0500",
            "replies": []
          },
          {
            "author": "jfb",
            "content": "<p>Sure. I started with Unix in the late 80s/early 90s and systemd is imperfect and clunky and so much better than what came before it beggars the imagination.</p>",
            "createdAt": "2025-07-09 07:04:36-0500",
            "replies": []
          },
          {
            "author": "cks",
            "content": "<p>I’ve been a profession sysadmin for a long time and I’ve run Unix systems using most init systems, including Solaris SMF, classic System V init, and so on (not the modern BSD init systems, though; I have low exposure to them). Systemd has always been a clear improvement with multiple quality of life improvements around things like portable (across Linux distributions and distribution versions) and easily written unit files, providing clean environments for service restarts, cgroups providing reliable tracking of ‘what is part of this thing’, and so on. The systemd authors also put in a lot of work being backward compatible with classical Linux System V style init and other classical Linux elements, to the point where you could not just use SysV init scripts as-is in your systemd init (and you still can) but also use systemd drop in overrides to affect their operation.</p>\n<p>Could all of this have been done in a simpler init system? Sure, probably. Did it actually get done in any other init system? No. No one showed up to do the work of implementing similar quality of life improvements. That’s why people were willing to adopt systemd over, say, Upstart (which was the initial System V init replacement in multiple Linux distributions).</p>\n<p>It’s pretty easy to write a basic init system (I’ve done it, long ago when even a basic split rc.d style one was better than the old-school 4.x BSD /etc/rc alternative). It’s much harder to write a production level full replacement, especially if you want more advanced features like cgroup based tracking of processes and clean service restarts.</p>",
            "createdAt": "2025-07-09 13:25:37-0500",
            "replies": []
          },
          {
            "author": "madhadron",
            "content": "<blockquote>\n<p>So has anyone learned an older init and then looked at systemd and said “yes, this is better”?</p>\n</blockquote>\n<p>Yup. I came to Linux in the mid-1990’s. But people forget that many of the commercial Unix systems had already ditched old style Unix init. Systemd is hardly the first time we’ve gone down this path. The wonder is just how long Linux persisted in using SysV init.</p>\n<blockquote>\n<p>It doesn’t provide socket activation or logging but it does its job.</p>\n</blockquote>\n<p>The disconnect here is that you can either look only at an init system and say “its job is…” or you can look at the operating system and say, “This should provide…” For many of the things you want to provide, you have to integrate pieces that, in older and cruder systems, could stay entirely separate.</p>\n<p>My gripe to date about systemd is that there isn’t a ubiquitous, robust, built-in way to ship journald to other servers. I occasionally think about writing it, but the fact that log shipping isn’t now a built-in is a real miss.</p>",
            "createdAt": "2025-07-09 15:57:01-0500",
            "replies": [
              {
                "author": "inverse",
                "content": "<blockquote>\n<p>My gripe to date about systemd is that there isn’t a ubiquitous, robust, built-in way to ship journald to other servers. I occasionally think about writing it, but the fact that log shipping isn’t now a built-in is a real miss.</p>\n</blockquote>\n<p><a href=\"https://www.freedesktop.org/software/systemd/man/latest/systemd-journal-upload.service.html\" rel=\"ugc\">This functionality has existed since v239</a> and systemd has been able to serve logs over HTTP since v198</p>",
                "createdAt": "2025-07-10 10:09:40-0500",
                "replies": [
                  {
                    "author": "madhadron",
                    "content": "<p>Oh, cool! Looks like it’s just http, though, not udp , so clearly some work still to do.</p>",
                    "createdAt": "2025-07-10 13:17:20-0500",
                    "replies": []
                  }
                ]
              }
            ]
          },
          {
            "author": "gerikson",
            "content": "<blockquote>\n<p>I wonder how many people who like systemd started using Linux before it came about.</p>\n</blockquote>\n<p>I think the most interesting point here is “using Linux”.</p>\n<p>I’ve had an active Linux system near me since 1998 or so. But I have been the classic single user - I have never had to maintain a multi-user system. I’ve also always used apt-based systems.</p>\n<p>To me, the question of what kind of init my system uses has basically never been an issue. The packaging system has handled installation of software (including server software) transparently.</p>\n<p>I think I started tinkering with systemd scripts when I got an RPi and had to do some stuff specific to that platform. The community there was basically all post systemd and for them the tips and tricks for getting stuff to work was just … systemd.</p>\n<p>Today, if you’re a fulltime computer wrangler, you don’t deploy server packages - you deploy entire “computers” using containers. How those “computers” interact internally to make sure your desired functionality appears is opaque, and you don’t need to tinker with it unless it’s really messed up.</p>",
            "createdAt": "2025-07-09 03:42:15-0500",
            "replies": [
              {
                "author": "benjajaja",
                "content": "<p>It’s the same as with other stuff: in the old days it was enough, because stuff was simpler. Just start <code>X -configure</code> and then tweak /etc/X11/xorg.conf. Today there way too many subsystems involved for it to be that simple. And it wasn’t easy either, just simple as in not so complex.</p>",
                "createdAt": "2025-07-09 05:31:57-0500",
                "replies": [
                  {
                    "author": "gerikson",
                    "content": "<p>For those times I’ve used Linux with a graphical system it has Just Worked, no <code>configure</code> needed. First was probably a laptop in 1999 or so, the 386 I started with couldn’t run X 😉</p>",
                    "createdAt": "2025-07-09 07:26:43-0500",
                    "replies": [
                      {
                        "author": "jcelerier",
                        "content": "<p>my first foray into Linux, with a boxed red hat linux set, ended up in failure which, a couple decades later, finally made sense: the kernel that came with it at the time just did not support the GPU I had in this machine at all (geforce II MX maybe?), so I would never have been able to run X however I tried. But I was like 10 years old so not motivated to go through the entire manual at this point.</p>\n<p>Then mandrake linux came and it just worked &lt;3</p>",
                        "createdAt": "2025-07-09 18:32:28-0500",
                        "replies": [
                          {
                            "author": "pimeys",
                            "content": "<p>About the same time or a bit earlier. Matrix Millennium GPU. Worked fine after I found the correct random numbers to my XFree config.</p>",
                            "createdAt": "2025-07-10 13:50:42-0500",
                            "replies": []
                          }
                        ]
                      },
                      {
                        "author": "nonagoninf",
                        "content": "<p>I started using Linux in 1994 and setting up XFree86 certainly required running <code>xf86config</code>. Early teenage me was quite worried about damaging something by setting the wrong modelines. Luckily the graphics card of our 386 was supported (I think it was a Tseng Labs ET4000).</p>",
                        "createdAt": "2025-07-10 01:27:58-0500",
                        "replies": []
                      }
                    ]
                  }
                ]
              }
            ]
          },
          {
            "author": "kokada",
            "content": "<p>Like others in the thread, me too. I started using Arch probably one or two years before systemd replaced it, and I was one of the earlier adopters (when Arch started to have systemd in community repository but still not the default) exactly because the old solution (based in BSD init) was bad.</p>\n<p>I mean, every other week I had some kind of system boot failure because I put things in the wrong order in <code>services=()</code> inside <code>/etc/rc.conf</code>. Also trying to figure out how to parallelize services was “fun” (you could append or prepend a <code>!</code> to make the service start in the background so it wouldn’t block other services to start, but this only worked if you initialized all dependencies for that service in a blocking manner). Remember that this was the time when spinning rust was still the norm, so it did make a difference to startup time if you got it right (something like halfing the boot time).</p>\n<p>After migrating to systemd, I remember my boot times regressed a bit, but I think this was mostly because most services still were using the old scripts to start. Once they started the migration to systemd service files everything started much faster, and except for one or another bug from the early days my experience with systemd was always solid.</p>\n<p>And this is only focusing in the init part. There are lots of other things that I think systemd does better (journal, cronjobs, user services, etc).</p>",
            "createdAt": "2025-07-09 06:48:26-0500",
            "replies": [
              {
                "author": "seachub",
                "content": "<p>I think people forget how hard booting was before.</p>",
                "createdAt": "2025-07-09 12:57:21-0500",
                "replies": []
              }
            ]
          },
          {
            "author": "viraptor",
            "content": "<p>Started way before systemd and I mostly like it. (Although I’m still grumpy about resolved being a thing)</p>\n<blockquote>\n<p>I wonder because I find systemd extremely opaque.</p>\n</blockquote>\n<p>What we had better was more opaque, because each service had it’s own special idea about how it should start/stop. Some even came with their own restart system (MySQL). Whatever’s opaque now is just a tiny bit of what we used to deal with.</p>\n<p>For the scope - you don’t have to learn all of it in one go. I’m sure there’s lots of features others use that I still haven’t seen, but that’s ok. Dependencies, supervision types, environment - those are the basics. Anything else is gravy.</p>",
            "createdAt": "2025-07-09 09:36:04-0500",
            "replies": []
          },
          {
            "author": "mccd",
            "content": "<p>I’m newer to sysadmin, being mostly a developer. I don’t think sysvinit is particularly good, I prefer systemd. That said even more so do I like runit.</p>\n<p>With runit, each time I’ve wondered how to do something, it has been exactly how I thought it was (too simple to be true), and its worked well. Same goes for cron over systemd timers, where I’ve had very weird errors (also many cron tools today have macros like <code>@daily</code>, so syntax is very minimal).</p>\n<p>There are some things I really like with systemd though like credential handling and sandboxing but they’ve been very hard IMO to work with.</p>\n<p>I don’t hate systemd, but I do feel that very few concept from Unix translate over to how it works. So it feels like what I learn from using systemd doesn’t transfer elsewhere whereas with runit/sysvinit I reuse the same skills. So it sticks out quite a bit, I actually wish it was more integrated into Linux itself along with dbus so we had less complexity and the system was more cohesive.</p>",
            "createdAt": "2025-07-09 08:58:27-0500",
            "replies": []
          },
          {
            "author": "cpurdy",
            "content": "<blockquote>\n<p>I wonder how many people who like systemd started using Linux before it came about.</p>\n</blockquote>\n<p>I’ve been using Linux since the mid 90s (maybe late ‘96 or ‘97), back when there were no “distributions”. I’m not actually sure now how I got anything to ever work, because I wasn’t an expert on UNIX or anything (although I also ran Solaris). I basically never used a GUI, and the few times I tried, I regretted it. As a result, I still don’t use (or even install) a GUI with my own installs of Linux.</p>\n<p>Back in the day, I read a few thousand pages of arguments online about systemd, and as far as I could tell, all of the criticisms were valid, as were many of the defenses of it 🤷‍♂️. But systemd never hurt me, which is more than I can say for init, but I didn’t particularly hate init either (except during the times when I was actively trying to figure out how to hack it into a working state, which somehow I managed to do despite having so little actual understanding of the overall system). OTOH, the old way of upgrading Linux was basically reinstalling (rebuilding) the OS, and that used to be a 30-40 hour process if you wanted to get things set up decently (e.g. get sound working 🤣) and have the install be relatively secure. I always felt like a complete moron doing it, because I’d spend 90% of my time reading long threads to try to understand what one tiny thing I was doing wrong, but I never felt (coming out of the other side of it) like I had gained any real knowledge … just scars 🤣.</p>\n<p>So in a way, I got to dodge any pain of switching over to systemd (I probably first picked it up with a debian-based build years ago), because the change never particularly impacted me. FWIW - 99% of my time on Linux is in SSH, configuring or managing servers, or deploying stuff, or diagnosing issues. And I really love Linux as a server now, which is a lot more than I can say for the Linux of 1996 – it really sucked as a server back then, but it had brilliantly good terminal support, which is the main reason I stuck with it!</p>",
            "createdAt": "2025-07-09 08:39:39-0500",
            "replies": []
          },
          {
            "author": "gcupc",
            "content": "<p>I started Linux sysadminning around ‘96, professionally in ‘98 to ‘06 and then hobbyist since then. I like systemd. The documentation is generally pretty good (though I do have to consult it every time I write a unit), and I’m pretty sure the site has an architectural overview. For my uses, it’s considerably better than anything I used before it. So much easier to write unit files than init scripts, sandboxing features are great for servers, and I’m coming to appreciate systemd user session management.</p>",
            "createdAt": "2025-07-10 07:38:36-0500",
            "replies": []
          },
          {
            "author": "atk",
            "content": "<p>I think systemd has a lot of problems and I have spent years passively working on what I think are better options but sysvinit based things are awful, even as implemented in BSDs.</p>\n<p>I would rather be writing an OpenBSD or FreeBSD rc script than an linux initscript (absolute fucking hot garbage and I don’t comprehend anyone who defends that stuff).</p>\n<p>I would rather be writing a systemd service file than a *BSD rc script (these are not the end of the world, but double forking and pidfiles are a stupid idea and shouldn’t even work in the first place).</p>\n<p>I would rather be writing a daemontools style run script (in most cases) than a systemd service file.</p>",
            "createdAt": "2025-07-09 10:22:45-0500",
            "replies": []
          },
          {
            "author": "zie",
            "content": "<p>Yes, I’ve been doing Linux since the 1990’s.</p>\n<p>Generally I think it’s better, but it does have warts. My biggest thing is like you said, it does A LOT and it’s not always very clear WTF it’s trying to do. I had a problem where I needed to debug something in systemd, I figured out how to turn on debug logs(man pages were not useful I don’t think in doing this) and the logs were not helpful at all at helping me figure out the problem, so I resorted to strace. I figured out it was trying to reach some particular host, google.com I think, and basing network decisions on it. I filed a bug report about my experience, that went nowhere.</p>\n<p>I have no idea why debug level logs would not include network connection attempts or decisions it made based on the results.</p>\n<p>So it definitely needs more work, but overall I think it’s a net positive. It’s frustratingly obscure/obtuse at times though.</p>",
            "createdAt": "2025-07-09 09:51:17-0500",
            "replies": []
          },
          {
            "author": "alper",
            "content": "<p>I used Linux way before but then recently got back and used <code>systemctl</code> and never realised that was the “much hated” systemd. I thought it was just a new way of doing things and adapted. Total nothingburger. Could be documented better.</p>\n<p>New ways of doing things <em>are</em> possible.</p>",
            "createdAt": "2025-07-10 04:46:54-0500",
            "replies": []
          },
          {
            "author": "adam_d_ruppe",
            "content": "<p>I’ve still almost never used systemd…. I’m a Slackware user. tbh I’ve also never really used whatever slackware’s init thing is called though either, almost been a full year since I last even rebooted this computer, so <code>init</code> isn’t something I think about really at all.</p>\n<p>A work system had systemd though and it seemed ok. I guess what I like most is that there isn’t so much random differences now between distros and services so that is nice.</p>",
            "createdAt": "2025-07-09 11:33:51-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "cyberia",
        "content": "<p>It would just be nice if it wasn’t so actively hostile to musl.</p>\n<p><a href=\"https://github.com/systemd/systemd/issues/37779\" rel=\"ugc\">https://github.com/systemd/systemd/issues/37779</a></p>",
        "createdAt": "2025-07-09 04:49:32-0500",
        "replies": []
      },
      {
        "author": "duncan_bayne",
        "content": "<p>Tell that to, say, people with a desire to run GNOME on non-Linux operating systems.</p>\n<p>Yes, systemd is a perfectly workable … I wouldn’t just call it an init system these days. Wikipedia describes it as providing “an array of system components”, which sounds about right.</p>\n<p>But systemd becoming a defacto standard on Linux is hurting the portability of software from Linux to other Unix-like OSs. Or perhaps another way of putting it: Linux is becoming sufficiently non-Unix-like that software written to target it doesn’t port well?</p>\n<p>Sure, it’s reasonable to argue that Linux users are in a very significant majority, and there’s no point in maintaining compatibility with other standards as it’s a bunch more work. But as someone who’s been using Linux since ~ 1995, that position <em>does</em> remind me somewhat of the company we all used to hate at the time ;)</p>",
        "createdAt": "2025-07-09 08:05:56-0500",
        "replies": [
          {
            "author": "mort",
            "content": "<p>GNOME doesn’t have a goal of working on non-Linux systems. In general, you should probably pick major system components which have a goal of working on your operating system. I wouldn’t, say, try to hack the Windows Explorer desktop environment to work on my Linux system. Even if GNOME didn’t depend on systemd, it would be weird to try to get it to work on e.g FreeBSD when that’s such a clearly stated non-goal.</p>",
            "createdAt": "2025-07-09 08:51:27-0500",
            "replies": [
              {
                "author": "duncan_bayne",
                "content": "<blockquote>\n<p>GNOME doesn’t have a goal of working on non-Linux systems.</p>\n</blockquote>\n<p>Yeah that’s … kinda my point about what’s wrong here. systemd perhaps isn’t the main cause of it, maybe it’s even a symptom?</p>",
                "createdAt": "2025-07-09 08:57:43-0500",
                "replies": [
                  {
                    "author": "mort",
                    "content": "<p>I 100% think it’s a “symptom”, as you call it. GNOME used to make sure that they have multiple code paths when something has to be OS specific. They don’t do that anymore. And it has been like this for a while; it has had a hard dependency on systemd-logind for a long time. They clearly don’t care that it’s Linux-specific, they clearly value the decreased maintenance burden of just assuming a Linux system running systemd.</p>\n<p>Hard dependencies on systemd are probably easier for FreeBSD people to deal with than other kinds of hard dependencies on Linux. Adding a shim dbus service which provides the functionality GNOME requires is probably one of the easier ways to fix an OS dependency.</p>",
                    "createdAt": "2025-07-09 09:11:51-0500",
                    "replies": [
                      {
                        "author": "gcupc",
                        "content": "<p>I’m not sure “a hard dependency on systemd-logind” is true. I <em>think</em> you can run GNOME with elogind, though pretty soon there are going to be more systemd dependencies that will have to be shimmed/re-implemented.</p>",
                        "createdAt": "2025-07-10 07:48:39-0500",
                        "replies": [
                          {
                            "author": "mort",
                            "content": "<p>I thought the only reason GNOME can run with elogind is that elogind pretends to be systemd-logind by exposing all the same services as logind under the same names? It is the case with all “hard dependencies” that they can be shimmed by something which exposes an identical API to the dependency (and “hard dependencies” on systemd services via dbus provide a rather clean way to expose alternate implementations). I think you’re right that the systemd dbus services which GNOME depends on will get non-systemd API-compatible alternatives.</p>",
                            "createdAt": "2025-07-10 08:00:00-0500",
                            "replies": [
                              {
                                "author": "gcupc",
                                "content": "<p>I think the only thing we disagree on here is the semantics of “hard dependency”. I feel like if you can replace something with a compatible implementation it’s not a hard dependency, but I can accept that you mean something different.</p>",
                                "createdAt": "2025-07-10 09:30:58-0500",
                                "replies": [
                                  {
                                    "author": "mort",
                                    "content": "<p>Under your definition of “hard dependency”, I have a hard time imagining something that’s <em>not</em> a hard dependency. Any dependency necessarily has some sort of interface boundary, and that interface could always in principle be replicated by an alternative implementation.</p>\n<p>Do you have a concrete example of something you would consider an <em>actual</em> hard dependency?</p>",
                                    "createdAt": "2025-07-10 09:45:16-0500",
                                    "replies": [
                                      {
                                        "author": "kwas",
                                        "content": "<blockquote>\n<p>Do you have a concrete example of something you would consider an actual hard dependency?</p>\n</blockquote>\n<p>glibc for systemd. It’s linked somewhere in this thread; systemd imported glibc headers wholesale, and started using private symbols included therein. Which I am obliged to admit is a really stupid move.</p>",
                                        "createdAt": "2025-07-10 09:48:29-0500",
                                        "replies": []
                                      }
                                    ]
                                  }
                                ]
                              }
                            ]
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "author": "yawaramin",
                    "content": "<p>I don’t think anything is ‘wrong’ here though, the GNOME Project people have the right to decide what platforms they support. It’s free software–anyone who wants to support anything else is free to do so.</p>",
                    "createdAt": "2025-07-09 16:05:48-0500",
                    "replies": []
                  }
                ]
              },
              {
                "author": "hoistbypetard",
                "content": "<blockquote>\n<p>Even if GNOME didn’t depend on systemd, it would be weird to try to get it to work on e.g FreeBSD when that’s such a clearly stated non-goal.</p>\n</blockquote>\n<p>I don’t think it’s particularly weird to run GNOME on FreeBSD. <a href=\"https://docs.freebsd.org/en/books/handbook/desktop/\" rel=\"ugc\">It’s documented as a supported desktop environment in the handbook</a>.</p>",
                "createdAt": "2025-07-09 10:16:45-0500",
                "replies": [
                  {
                    "author": "ThinkChaos",
                    "content": "<p>The point is it’s supported by FreeBSD, not upstream GNOME. So the maintenance burden is shifted.</p>\n<p>I think it’s fair for GNOME to say they don’t want to do specific work. Although unfortunate for others.</p>",
                    "createdAt": "2025-07-09 11:39:19-0500",
                    "replies": []
                  },
                  {
                    "author": "mort",
                    "content": "<p>Unfortunately, which platforms GNOME supports is dictated by GNOME, not by FreeBSD. It’s fine that FreeBSD tries to keep GNOME working on FreeBSD, but I would be wary of using something as complex as GNOME on an operating system which upstream has no intention to support.</p>",
                    "createdAt": "2025-07-09 11:48:36-0500",
                    "replies": [
                      {
                        "author": "hoistbypetard",
                        "content": "<p>I see what you mean, and I think I’d look at the ports tree pretty carefully to see how hard they’re needing to fight upstream before making any important system depend on it, to be sure.</p>\n<p>But I don’t think it’s all that odd for GNOME to decide “we don’t want to take on this work” nor for FreeBSD to say “we want to maintain and support this,” and I definitely don’t think it’s unreasonable for a user to feel comfortable using a thing their OS distribution takes on and supports.</p>\n<p>Oddly enough, I wouldn’t be surprised if GNOME’s embrace of systemd actually reduced the number of patches FreeBSD needs to maintain in their ports tree. It’s better specified, IMO, than other sorts of Linux-isms that tend to sneak into software that doesn’t get tested on non-Linux platforms by its authors.</p>",
                        "createdAt": "2025-07-09 12:37:29-0500",
                        "replies": []
                      }
                    ]
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "author": "mort",
        "content": "<p>There are some legitimate questions about whether some other replacement to sysvinit would’ve been better than systemd. But the world we got, where everything is using systemd, is pretty great, honestly. It has significantly improved my computing life, across my roles as end user, system administrator, developer, and guy in charge of Yocto-based custom Linux distros for embedded. I can just <em>write a service file</em>. I never wrote sysvinit scripts because I always thought it was too hard/arcane.</p>\n<p>I think systemd-the-init-system is great, and I also think systemd-the-project is pretty great. I like systemd-resolved and its stub resolver functionality and solid DBus interface. I love journald and its unification of logging on all my systems, I almost never have to go digging in /var/log to see what some random service outputs anymore. I haven’t had the chance to work with networkd too much as it has always either Just Worked or not been flexible enough for my particular needs in very specific embedded situations, but it seems nicer to work with than ifupdown scripts.</p>",
        "createdAt": "2025-07-09 01:52:00-0500",
        "replies": []
      },
      {
        "author": "atk",
        "content": "<p>I think that posting about how much better systemd is than OpenRC, rc, and old linux initscripts is really a tired straw man. It is a lot better, but so is literally anything which doesn’t rely on double-forking as a start.</p>\n<p>And I know this post isn’t <em>just</em> saying that, but it doesn’t really address the fact that with e.g. runit and a bunch of <em>separate</em> utilities you can reach 100% feature parity of systemd (although some of the utilities don’t exist yet). (Although I am not sure if you want to use runit for that purpose, and not something slightly different.)</p>\n<p>There’s a lot of “systemd critics” out there who can’t even see why double-forking being a thing, never mind working as a concept, is the most bizarre design choice in all of Unix (I would put PID leaks at the same level as memory leaks).</p>\n<p>There are genuine criticisms of systemd, but the most vocal critics who just want their OpenRC nonsense aren’t stating them, and definitely nobody is addressing them.</p>\n<p>I am not going to sit here enumerating the issues, it’s a tiring process and best left for a blog post, I just would like to say that in 2025 I don’t think you need to be giving systemd free advertising.</p>",
        "createdAt": "2025-07-09 05:31:33-0500",
        "replies": [
          {
            "author": "tonyfinn",
            "content": "<blockquote>\n<p>I think that posting about how much better systemd is than OpenRC, rc, and old linux initscripts is really a tired straw man. It is a lot better, but so is literally anything which doesn’t rely on double-forking as a start.</p>\n</blockquote>\n<p>Those are however the alternatives on offer. If you want to argue people should stop using systemd then it should be clear what system or systems they should move to.</p>\n<p>If there was an alternative, people would move to to it. People largely moved to systemd, and the Linux desktop world has largely just finished replacing pulseaudio with pipewire. Pulseaudio is even from the same authors and backers as systemd and it still got dumped.</p>",
            "createdAt": "2025-07-09 11:11:47-0500",
            "replies": [
              {
                "author": "atk",
                "content": "<blockquote>\n<p>Those are however the alternatives on offer. If you want to argue people should stop using systemd then it should be clear what system or systems they should move to.</p>\n</blockquote>\n<p>I don’t want to argue people should stop using systemd. I don’t actually care what people and distributions use. What I do care about is that the software I use doesn’t stop working fully when you don’t run it with systemd. The problems I am complaining about are e.g. that there is software out there which when built on a non-systemd system (with no functioning libsystemd (the elogind libsystemd is non-functional last I checked)) then useful features which are not systemd originated or systemd specific like readiness notifications and socket activation just get dropped.</p>\n<p>I am arguing that systemd has benefited greatly from an organised marketing campaign which has resulted in actual damage to projects, software diversity, and interoperability. And I think at this point even Lennart has said that this wasn’t the intention. I don’t know if I trust him on that given his actual statements about these kinds of things in the past.</p>\n<p>There are areas where systemd interop is relatively trivial (socket activation and readiness notifications) but there are also other areas where systemd has eaten the cake (cgroups and namespaces) and not offered any good interoperability options (dbus is a garbage protocol, is over-engineered, and is in stark contrast to the simplicity of e.g. readiness notifications or socket activation).</p>\n<blockquote>\n<p>If there was an alternative, people would move to to it.</p>\n</blockquote>\n<p>Again, of no relevance to anything I said. But also: No guarantee of that, even if the alternative was strictly better across all metrics. It’s just kind of bizarre that in 2025 in a world where marketing is clearly the most important factor across the board that people still believe such things as “the better solution will win”.</p>\n<blockquote>\n<p>People largely moved to systemd, and the Linux desktop world has largely just finished replacing pulseaudio with pipewire. Pulseaudio is even from the same authors and backers as systemd and it still got dumped.</p>\n</blockquote>\n<p>I don’t see how this has anything to do with anything I said but I will point out that you are just making an appeal to popularity here with no other basis, just for your awareness.</p>",
                "createdAt": "2025-07-10 07:20:57-0500",
                "replies": []
              }
            ]
          }
        ]
      },
      {
        "author": "hwj",
        "content": "<blockquote>\n<p>systemd is replacing my plaintext logs with a binary format</p>\n</blockquote>\n<p>This is one of the reasons I don’t like systemd.</p>",
        "createdAt": "2025-07-09 01:04:26-0500",
        "replies": [
          {
            "author": "adrien",
            "content": "<p>My main issue with journald is rather how much I/O it causes. It uses huge amounts of space, default values are really high, can exhaust a machine’s storage space quickly if you spawn several containers, and certainly kill flash storage faster than what we should aim at.</p>\n<p>I find the default storage duration quite telling. The stated goal is to keep as much as possible which sounds like something from the previous century. Sure, you may want to keep everything stored but that’s probably not the default expectation (e.g. much more than 3 months of logs), and if you want to do that, you’re probably not doing that in journald anyway.</p>",
            "createdAt": "2025-07-09 05:47:40-0500",
            "replies": []
          },
          {
            "author": "gf0",
            "content": "<p>You can redirect the logs to have both plain text and binary logging, FWIW.</p>",
            "createdAt": "2025-07-09 01:16:34-0500",
            "replies": [
              {
                "author": "mort",
                "content": "<p>But also, it just hasn’t been a problem. I can still process logs with grep and awk and sed and all that. It’s just that the shell pipeline looks like <code>journalctl -u blah | grep</code> instead of <code>(zcat /var/logs/blah/*.log.gz; cat /var/logs/blah/*.log) | grep</code>, or whatever is appropriate for however the service rotates its logs. Which, honestly, is a good thing.</p>",
                "createdAt": "2025-07-09 04:18:27-0500",
                "replies": [
                  {
                    "author": "fanf",
                    "content": "<p>I am disappointed by how incredibly slow journalctl is compared to grepping plain text logs, since the point of the binary logs was to be faster. Tragic failure.</p>\n<p>Also journalctl overrides my $LESS settings which is deeply obnoxious.</p>",
                    "createdAt": "2025-07-09 05:13:32-0500",
                    "replies": [
                      {
                        "author": "david_chisnall",
                        "content": "<p>I have been pondering a syslog implementation that uses sqlite as a storage layer and exposes a read-only FUSE filesystem that pretends to be plain-text logs to pretend to be the old text files. I think it could be backwards compatible with syslog.conf and newsyslog.conf quite easily and also expose a rich search interface (and the ability to have saved searches that look like files).</p>",
                        "createdAt": "2025-07-09 09:26:19-0500",
                        "replies": [
                          {
                            "author": "fanf",
                            "content": "<p>Fancy log storage needs a trigram index <a href=\"https://swtch.com/~rsc/regexp/regexp4.html\" rel=\"ugc\">https://swtch.com/~rsc/regexp/regexp4.html</a> which might be able to compensate for the way splitting the log into separate lines stymies grep’s optimizations <a href=\"https://lists.freebsd.org/pipermail/freebsd-current/2010-August/019310.html\" rel=\"ugc\">https://lists.freebsd.org/pipermail/freebsd-current/2010-August/019310.html</a></p>",
                            "createdAt": "2025-07-09 11:29:41-0500",
                            "replies": [
                              {
                                "author": "bazzargh",
                                "content": "<p>sqlite already has this. <a href=\"https://www.sqlite.org/fts5.html#the_trigram_tokenizer\" rel=\"ugc\">https://www.sqlite.org/fts5.html#the_trigram_tokenizer</a> - it was added after a discussion of this feature with the ever-present simon willison <a href=\"https://simonwillison.net/2020/Sep/26/weeknotes-software-carpentry-sqlite/#sqlite-modules\" rel=\"ugc\">https://simonwillison.net/2020/Sep/26/weeknotes-software-carpentry-sqlite/#sqlite-modules</a></p>",
                                "createdAt": "2025-07-10 08:31:57-0500",
                                "replies": []
                              }
                            ]
                          },
                          {
                            "author": "zie",
                            "content": "<p>This is what journald should have done.</p>",
                            "createdAt": "2025-07-09 10:04:19-0500",
                            "replies": []
                          }
                        ]
                      },
                      {
                        "author": "gf0",
                        "content": "<p>It’s only slow because it has a questionable default of getting everything since the beginning of the universe.</p>\n<p>If you specify <code>-b</code> so that it only opens stuff since the current boot, it is basically instant under most conditions.</p>",
                        "createdAt": "2025-07-09 09:33:42-0500",
                        "replies": [
                          {
                            "author": "intelfx",
                            "content": "<p>TBF, it’s mainly slow because of the reader’s questionable implementation choices. It doesn’t have to be this way, nothing in the journal file format precludes writing a faster reader, nobody simply cared enough to do so.</p>",
                            "createdAt": "2025-07-09 16:35:33-0500",
                            "replies": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "author": "acatton",
                    "content": "<p>So many useful options are available on <code>journalctl</code> such as <code>--since=yesterday</code>, <code>--until=-2hours</code> or <code>--boot=-1</code>. I double dare anybody to do this with <code>zcat/cat</code>, <code>grep</code> and logrotated logs.</p>",
                    "createdAt": "2025-07-09 04:40:53-0500",
                    "replies": [
                      {
                        "author": "adrien",
                        "content": "<p>My favorite pager for such things is “view”, which is read-only vim. It definitely handles that very easily. I get your point and agree it’s an improvement but it’s quite modest. I’d say that the structure shines more (or at least has more potential to shine) with log content being separate from control content but human readers will only properly take advantage of that when using a structured output and a pager than can handle that format (maybe <a href=\"https://www.visidata.org/\" rel=\"ugc\">https://www.visidata.org/</a> or others which I can’t remember).</p>",
                        "createdAt": "2025-07-09 05:51:45-0500",
                        "replies": []
                      }
                    ]
                  },
                  {
                    "author": "gf0",
                    "content": "<p>Yeah absolutely. It’s just a common criticism and if someone is very adamant about the “old ways”, then they can have their cake.</p>\n<p>I personally see no evil in binary logging (especially that the code to parse/write it is available freely for anyone to see), plus it was definitely an improvement in the logging scene - we get logs properly from even before the kernel boots, for example.</p>",
                    "createdAt": "2025-07-09 04:57:25-0500",
                    "replies": []
                  }
                ]
              }
            ]
          },
          {
            "author": "singpolyma",
            "content": "<p>Does this really happen though? All my services still log to /var/log text files auto rotated just as they ever did…</p>",
            "createdAt": "2025-07-09 07:02:22-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "FeepingCreature",
        "content": "<p>Great. openrc works fine here though, so I’m good, thanks.</p>",
        "createdAt": "2025-07-09 03:40:38-0500",
        "replies": []
      },
      {
        "author": "alper",
        "content": "<p>I think the amount of wailing around systemd is probably a good indicator that it’s a good thing.</p>",
        "createdAt": "2025-07-09 01:38:02-0500",
        "replies": []
      },
      {
        "author": "ksynwa",
        "content": "<p>I love systemd but I don’t understand why there are things like systemd-networkd and systemd-resolved.</p>\n<p>Ok now that I think about it, the former might be helpful for cases when a service is network dependent. But not sure about the latter.</p>\n<p>Either way I use these services on my SBC and they work quite well.</p>",
        "createdAt": "2025-07-09 03:34:06-0500",
        "replies": [
          {
            "author": "tonyfinn",
            "content": "<p>You don’t need to use systemd-networkd, systemd-boot, systemd-resolved or even for as much criticism as it’s gotten in this thread, systemd-journald .</p>\n<p>There’s a lot less “all or nothing” in systemd components than people think, and I know in my case I moved to systemd-networkd because it was just better than the previous solutions I had used (network manager, netctl), etc.</p>",
            "createdAt": "2025-07-09 11:13:50-0500",
            "replies": [
              {
                "author": "srtcd424",
                "content": "<p>TBF you can’t disable journald, but you can obviously passthrough everything to an old style syslog. But yeah, everything else is optional. One of the things that’s always baffled me about many systemd critics is that “it’s too monolithic!” and “it installs too many binaries!” tend to occur in the same critiques, which feels rather self-contradictory.</p>",
                "createdAt": "2025-07-09 12:50:41-0500",
                "replies": []
              }
            ]
          },
          {
            "author": "bkhl",
            "content": "<p>One of the main points of making Systemd in the first place was better support for desktop/laptop scenarios where you want to manage services based on for example if network is available, or what network you are connected to.</p>\n<p>That you also may want to control how you look up hostnames based on that situation is also reasonable. It seems a lot easier to use a service included in Systemd for this than to write unit files that would then in turn control your DNS config or whatever.</p>",
            "createdAt": "2025-07-09 04:45:53-0500",
            "replies": []
          },
          {
            "author": "adrien",
            "content": "<p>Doing something makes sense because name resolution (especially on linux) is really stupid and limited. By default you have one resolver for everything, no way to match a hostname to a resolver (e.g. everything .company could go to a different resolver), configuration for various links fighting to set their resolver, …</p>\n<p>I hate the implementation however. Are you having troubles getting answers from your DNS resolver because it’s being DoS’ed or because the network is (temporarily) very lossy? Well, systemd-resolved will happily stop using that resolver. Even if it’s your only server. And it won’t use it again automatically, or at least not quickly at all. It won’t probe it again and you can’t force it directly: you have to disconnect and reconnect to trigger everything again.</p>",
            "createdAt": "2025-07-09 06:01:54-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "rtpg",
        "content": "<p>I do feel generally good about <code>systemd</code> in theory.</p>\n<p>I do wish I had something <code>lazydocker</code>-like for managing it. I’m constantly flipping arguments to <code>systemctl</code>, having to bounce between listing and then acting on it…. and I really want a TUI that feels as nice to use for filtering/selecting what to display/etc as, say, datadog or grafana (low bar, but!)</p>\n<p>There’s all this structure and information about what’s going on in our systems. We could have a great set of interactive tools to peruse this data!</p>",
        "createdAt": "2025-07-08 19:52:55-0500",
        "replies": [
          {
            "author": "asb",
            "content": "<p>I saw <a href=\"https://github.com/isd-project/isd\" rel=\"ugc\">isd</a> after looking through the recently announced <a href=\"https://cfp.all-systems-go.io/all-systems-go-2025/schedule/\" rel=\"ugc\">schedule for the All Systems Go 2025 conference</a>.</p>",
            "createdAt": "2025-07-08 22:33:47-0500",
            "replies": [
              {
                "author": "rtpg",
                "content": "<p>Thank you so much! This is essentially what I was looking for</p>",
                "createdAt": "2025-07-08 22:52:59-0500",
                "replies": []
              }
            ]
          },
          {
            "author": "lytedev",
            "content": "<p>I have found <a href=\"https://github.com/joehillen/sysz\" rel=\"ugc\">https://github.com/joehillen/sysz</a> useful as a “systemd TUI”</p>",
            "createdAt": "2025-07-08 20:11:36-0500",
            "replies": []
          },
          {
            "author": "junon",
            "content": "<p>This is what shell aliases are for, I suppose. Also I’d wager most people aren’t sitting in their systemd monitor enough to really warrant a TUI. Rust and Ratatui exist and there are codegen libraries for dbus - go write one! :)</p>",
            "createdAt": "2025-07-08 21:20:42-0500",
            "replies": []
          },
          {
            "author": "senekor",
            "content": "<p>Here’s another option that I quite like: <a href=\"https://github.com/rgwood/systemctl-tui\" rel=\"ugc\">https://github.com/rgwood/systemctl-tui</a></p>",
            "createdAt": "2025-07-09 02:55:14-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "classichasclass",
        "content": "<p>I’ve made my peace with <code>systemd</code>. I don’t like trying to match up unit files with services, but then I had that problem with <code>init.d</code> too and it doesn’t seem appreciably worse. That said, I use AIX or a BSD personally for server duties, so <code>systemd</code> on a client Linux box doesn’t rankle me much anyway.</p>",
        "createdAt": "2025-07-08 20:03:11-0500",
        "replies": [
          {
            "author": "strugee",
            "content": "<blockquote>\n<p>I don’t like trying to match up unit files with services</p>\n</blockquote>\n<p>What do you mean by this? If you mean figuring out what unit file caused a process to be spawned, you can pass <code>systemctl status</code> a PID and it will show you the corresponding unit.</p>",
            "createdAt": "2025-07-09 00:33:40-0500",
            "replies": [
              {
                "author": "runejuhl",
                "content": "<p>And it’ll helpfully show you the cgroup and list other processes in that same cgroup too, e.g. for processes that are running inside a Docker container.</p>\n<p>Here’s an example, this is a bunch of <code>sleep</code> processes running inside a Debian 12 Docker container:</p>\n<pre><code>$ pgrep -af 'sleep 9999' 319570 sh -c sleep 999999 319571 sleep 999999 319574 sh -c sleep 999999 319575 sleep 999999 319576 sh -c sleep 999999 319577 sleep 999999 319578 sh -c sleep 999999 319579 sleep 999999 $ systemctl status 319575 | cat ● docker-767d76cbfab8fd9c712d539742fbfb10e411a411e0e5bc3e4e2a095a49810b29.scope - libcontainer container 767d76cbfab8fd9c712d539742fbfb10e411a411e0e5bc3e4e2a095a49810b29 Loaded: loaded (/run/systemd/transient/docker-767d76cbfab8fd9c712d539742fbfb10e411a411e0e5bc3e4e2a095a49810b29.scope; transient) Transient: yes Drop-In: /run/systemd/transient/docker-767d76cbfab8fd9c712d539742fbfb10e411a411e0e5bc3e4e2a095a49810b29.scope.d └─50-DeviceAllow.conf, 50-DevicePolicy.conf Active: active (running) since Wed 2025-07-09 08:33:47 CEST; 1min 3s ago Invocation: cff50331d7ab4beba537d0191125e332 IO: 13.6M read, 0B written Tasks: 9 (limit: 37574) Memory: 6.9M (peak: 7.4M) CPU: 46ms CGroup: /system.slice/docker-767d76cbfab8fd9c712d539742fbfb10e411a411e0e5bc3e4e2a095a49810b29.scope ├─319493 bash ├─319570 sh -c \"sleep 999999\" ├─319571 sleep 999999 ├─319574 sh -c \"sleep 999999\" ├─319575 sleep 999999 ├─319576 sh -c \"sleep 999999\" ├─319577 sleep 999999 ├─319578 sh -c \"sleep 999999\" └─319579 sleep 999999 Jul 09 08:33:47 aegir systemd[1]: Started docker-767d76cbfab8fd9c712d539742fbfb10e411a411e0e5bc3e4e2a095a49810b29.scope - libcontainer container 767d76cbfab8fd9c712d539742fbfb10e411a411e0e5bc3e4e2a095a49810b29. </code></pre>",
                "createdAt": "2025-07-09 01:36:56-0500",
                "replies": [
                  {
                    "author": "classichasclass",
                    "content": "<p>Interesting, but what is the <code>| cat</code> for?</p>",
                    "createdAt": "2025-07-09 08:22:34-0500",
                    "replies": [
                      {
                        "author": "ehamberg",
                        "content": "<blockquote>\n<p>Interesting, but what is the | cat for?</p>\n</blockquote>\n<p>It’s a few keypresses saved compared to adding <code>--no-pager</code> :)</p>",
                        "createdAt": "2025-07-09 08:37:07-0500",
                        "replies": [
                          {
                            "author": "runejuhl",
                            "content": "<p>Yes, exactly this. Plus it works with more programs, and it often also serves to avoid ANSI escape codes in the output.</p>",
                            "createdAt": "2025-07-09 08:49:19-0500",
                            "replies": []
                          },
                          {
                            "author": "Psentee",
                            "content": "<p>This <a href=\"http://www.novosial.org/shell/useless-cat/\" rel=\"ugc\">“useless use of cat”</a> is actually useful, nice trick!</p>",
                            "createdAt": "2025-07-10 05:15:49-0500",
                            "replies": []
                          }
                        ]
                      },
                      {
                        "author": "pilif",
                        "content": "<p>Presumably to turn off the pager systemctl automatically invokes if the output is large</p>",
                        "createdAt": "2025-07-09 08:57:43-0500",
                        "replies": []
                      }
                    ]
                  }
                ]
              },
              {
                "author": "classichasclass",
                "content": "<p>No, I mean services like what I get from a blame list. Unless there’s a quick way to resolve those also.</p>",
                "createdAt": "2025-07-09 08:16:23-0500",
                "replies": [
                  {
                    "author": "intelfx",
                    "content": "<p><code>systemctl cat SERVICE-NAME</code>?</p>\n<p><code>systemctl show -pFragmentPath,DropInPaths --value SERVICE-NAME</code>?</p>",
                    "createdAt": "2025-07-09 16:31:25-0500",
                    "replies": [
                      {
                        "author": "classichasclass",
                        "content": "<p>Thanks, I’ll try those.</p>",
                        "createdAt": "2025-07-09 18:51:26-0500",
                        "replies": []
                      }
                    ]
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "author": "andrewrk",
        "content": "<p>I don’t really care but I did notice after my distro switched to systemd that rebooting sometimes waits for some service to time out for 3 minutes before eventually giving up and killing it which is annoying since I have to either wait 3 extra minutes to reboot or hold the power switch to override. I’ve observed this on multiple combinations of hardware and distros.</p>",
        "createdAt": "2025-07-09 17:53:59-0500",
        "replies": [
          {
            "author": "hoistbypetard",
            "content": "<p>Shot in the dark: I’ve seen exactly that behavior on multiple laptops using thunderbolt or USB docking stations. In all cases, the issue was that a device I was using in the dock (<strong>probably</strong> a gigabit ethernet adapter each time, but I don’t recall… it could’ve been a smart card reader also) was not behaving under TLP’s power management. Adding that device’s id to TLP’s <code>USB_BLACKLIST</code> fixed it each time. If you’re using TLP in a similar situation, it may be worth looking. I no longer recall how I narrowed it to TLP… it wasn’t anything TLP-related that was hanging.</p>",
            "createdAt": "2025-07-09 19:05:07-0500",
            "replies": [
              {
                "author": "andrewrk",
                "content": "<p>Thanks for the tip</p>",
                "createdAt": "2025-07-09 19:35:47-0500",
                "replies": [
                  {
                    "author": "wink",
                    "content": "<p>As someone who only used a dock for about 10-20% of the time using a linux laptop - that never was the cause for me, but it can be really individual to the machine/distro/version combo :(</p>",
                    "createdAt": "2025-07-10 09:42:21-0500",
                    "replies": []
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "author": "jwconway",
        "content": "<p>The way it was done was kind of hot garbage - it was foisted on a whole bunch of distros at once, without a good plan of transition into it.</p>\n<p>It cant be run outside of PID1.</p>\n<p>Tries to do everything. Does some stuff well, and other stuff it utterly fails on. resolverd blows donkeyballs.</p>\n<p>But it really is a good idea, just not too well implemented. <a href=\"https://www.youtube.com/watch?v=o_AIw9bGogo\" rel=\"ugc\">https://www.youtube.com/watch?v=o_AIw9bGogo</a></p>",
        "createdAt": "2025-07-09 08:22:31-0500",
        "replies": [
          {
            "author": "mort",
            "content": "<p>It wasn’t “foisted on” distros. Many distros individually elected to start using it at around the same time. Distros have the autonomy here, not Poettering.</p>\n<blockquote>\n<p>It cant be run outside of PID1.</p>\n</blockquote>\n<p>This isn’t true? Systemd user sessions let you run a non-pid-1 systemd for managing user session processes.</p>\n<blockquote>\n<p>Tries to do everything. Does some stuff well, and other stuff it utterly fails on. resolverd blows donkeyballs.</p>\n</blockquote>\n<p>I think you’re mixing up the systemd project, which is a collection of services, and the systemd init system. The systemd init system doesn’t try to do that much, just things which make sense for an init system to do. I’ve personally been happy with systemd-resolved, but you can use any other DNS resolver even if you use systemd as your PID 1.</p>",
            "createdAt": "2025-07-09 08:54:52-0500",
            "replies": [
              {
                "author": "dijit",
                "content": "<blockquote>\n<p>It wasn’t “foisted on” distros. Many distros individually elected to start using it at around the same time.</p>\n</blockquote>\n<p>I’m not interested in a flame war, and you’re partially right that many distros chose systemd for its own merits, but it was <em>additionally true</em> that large software (especially those in the orbit of RedHat) were only supporting systemd and you would have had to maintain your own forks or patchsets for something else.</p>\n<p>It felt heavy handed and that is valid criticism.</p>",
                "createdAt": "2025-07-10 07:04:25-0500",
                "replies": [
                  {
                    "author": "mxey",
                    "content": "<p>What software are you referring to? Gnome is starting to depend on systemd this year.</p>",
                    "createdAt": "2025-07-10 08:23:09-0500",
                    "replies": [
                      {
                        "author": "dijit",
                        "content": "<p>A handy list: <a href=\"https://wiki.gentoo.org/wiki/Hard_dependencies_on_systemd\" rel=\"ugc\">https://wiki.gentoo.org/wiki/Hard_dependencies_on_systemd</a></p>",
                        "createdAt": "2025-07-10 12:08:21-0500",
                        "replies": []
                      }
                    ]
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "author": "thufie",
        "content": "<p>I am very happy with all the new systemd-* projects in terms of ease of use as well. Comparing timers to cronjobs in terms of intuitive legibility is a great example of how much less alien things are nowadays.</p>\n<p>Weird that I see this blog post today, since I was just having a weird covid-fueled fever-dream/memory-nightmare of a real system administrator challenge I had actually taken at a Linux conference before the days of systemd (I forget who ran the booth but I won some “swag” from it at the time). I remember at the time I did passingly well, but there is very much a reason why now it has transfigured itself into a sweaty and sleepless nightmare where I feel completely lost without access to Linux’s lovely new system layer.</p>",
        "createdAt": "2025-07-09 11:21:16-0500",
        "replies": []
      },
      {
        "author": "jmmv",
        "content": "<p>I don’t understand systemd very well (I mean I know what it does and how to operate it, but I’ve have never looked under the covers in detail because at this point I’m just a “user” of desktop Linux). It feels complicated at first just by watching a Linux system boot, but still, I think it’s an overall good addition to the system. The features it provides for isolation, and the consistency it brings to how services operate, are great. One thing I wonder is if the problem people have with systemd is with systemd itself, or with the way Linux distributions configure it with a gazillion different units and default-on services.</p>\n<p>Anyway. I still use BSD systems and I like their rc simplicity as well. More specifically, NetBSD’s rc framework (<a href=\"https://man.netbsd.org/rc.8\" rel=\"ugc\">https://man.netbsd.org/rc.8</a>, later adopted by FreeBSD), which provides rudimentary dependency tracking and a library to implement scripts uniformly, is pretty neat. But even if I like how these work, I do not think they are in any way a sign of great design in this day and age.</p>\n<p>For example: one thing that bothers me about old rc systems is how system upgrades require manual merging of changes to /etc/rc.d/ files. WhyTH am I asked to review the script that starts inetd when I’m not asked to review changes to inetd themselves? It makes no sense. (More on this in <a href=\"https://jmmv.dev/2020/08/rcd-libexec-etc.html\" rel=\"ugc\">https://jmmv.dev/2020/08/rcd-libexec-etc.html</a>) Or: it’s painful to watch a BSD system boot and notice artificial delays along the way because the init system cannot parallelize services where possible.</p>\n<p>To conclude, a thought that crossed my mind as I read the article: systemd is the Bazel of init systems. These two pieces of software (systemd and Bazel) make it hard to do the wrong thing and offer advanced features that the systems they replace don’t even have—but that comes at a cost in code size and <em>apparent</em> complexity. And I say apparent because the complexity ends up solving problems in a centralized manner (both in terms of implementation and in terms of maintenance costs).</p>",
        "createdAt": "2025-07-09 15:17:00-0500",
        "replies": []
      },
      {
        "author": "johnklos",
        "content": "<p>I will continue to complain about it so long as 1) I’m forced to deal with it as it absorbs more functionality from the rest of the OS, and 2) it’s difficult to get straightforward answers on how to deal with it.</p>\n<p>People can swear up and down that it’s simple and easy, but when I ask how to do basic things, telling me that it’s simple and easy but not answering how comes across as fanboi behavior.</p>",
        "createdAt": "2025-07-09 09:53:11-0500",
        "replies": [
          {
            "author": "hauleth",
            "content": "<blockquote>\n<ol>\n<li>I’m forced to deal with it as it absorbs more functionality from the rest of the OS</li>\n</ol>\n</blockquote>\n<p>It does not. We need to differentiate between <code>systemd</code> umbrella project and <code>systemd</code> init system. The only thing required by <code>systemd</code> init system from the umbrella project is <code>journald</code>, everything else is optional.</p>\n<blockquote>\n<ol start=\"2\">\n<li>it’s difficult to get straightforward answers on how to deal with it</li>\n</ol>\n</blockquote>\n<p>From my experience - other init systems, especially SysV are <em>at best</em> as hard to get straightforward answers as <code>systemd</code>.</p>",
            "createdAt": "2025-07-10 03:40:00-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "fzakaria",
        "content": "<p>my main complaint is the binary logs…. it ruins the beauty of everything being UTF-8/ASCII and that I can just ripgrep my <code>/</code> and search for what I need. I don’t want to go through journalclt</p>",
        "createdAt": "2025-07-09 11:38:41-0500",
        "replies": [
          {
            "author": "hauleth",
            "content": "<p>At the same time parsing logs to fetch logs only from give time period with text-based logs is a pain (and you need tools anyway). Having built in structured data for logs is for me huge win from observability point of view. I can have my log message stored in a simple form and do not need to display all the cruft alongside it at once. I can have newlines in my log messages which is <strong>huge</strong> when you have stuff like stack traces logged.</p>",
            "createdAt": "2025-07-10 03:42:50-0500",
            "replies": [
              {
                "author": "wink",
                "content": "<p>It’s only a pain at scale, which can begin at 5 servers under load.</p>\n<p>For the person with exactly one system it’s 90% of the time less pain.</p>",
                "createdAt": "2025-07-10 09:44:12-0500",
                "replies": [
                  {
                    "author": "hauleth",
                    "content": "<p>I tend to disagree, as <code>journald</code> made my toy-server much easier to manage. Having single place for all logs, without any additional work, is a bliss. You do not need to update log rotate, manage directories, etc. It just is there and works, without doing anything.</p>",
                    "createdAt": "2025-07-10 11:54:31-0500",
                    "replies": []
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "author": "pm",
        "content": "<p>Meh, frankly speaking, the discussions concerned mostly other Linux developers and not so much the end user. Except for maybe the binary logs thingy which I find rather silly and never once have used or even bother to try.</p>\n<p>As an end user, I can write a simple text file with straightforward declarative syntax covering the functionality we need, and I am happy. Truth be told, others did this too, for example canonical’s upstart.</p>\n<p>This is objectively better than writing a mini program (rc script) just for starting maintaining a service. What’s under the hood doesn’t interfere with the millions of users. All we want is getting out application to execute on start up, restart and write to logs. Systemd does this. But all these heated discussions are not about this, are they?</p>",
        "createdAt": "2025-07-09 12:46:27-0500",
        "replies": []
      },
      {
        "author": "Aks",
        "content": "<p>Heh, i thought the systemd startup thing with the “OK” “FAIL” etc. was part of Linux for the longest time.</p>\n<p>I’ve never understood the complaints. If you don’t like it, use something else: in Linux (and BSD) world you actually have a choice on the matter.</p>",
        "createdAt": "2025-07-09 11:39:47-0500",
        "replies": [
          {
            "author": "hoistbypetard",
            "content": "<p>Red Hat used to display their sysv-style init output that way (and so did others who built off their work or wanted to be familiar to their user base). AFAICT, they kept it the same when they switched to systemd, and only the source of the information that caused the (<code>OK</code>, <code>FAIL</code>, etc. messages) to be printed changed.</p>",
            "createdAt": "2025-07-09 13:54:33-0500",
            "replies": [
              {
                "author": "wink",
                "content": "<p>pretty sure I first learned about shell colors from init scripts in the 90s, yes. (this was Red Hat, minus Enterprise, or SuSE)</p>",
                "createdAt": "2025-07-10 09:49:40-0500",
                "replies": []
              }
            ]
          },
          {
            "author": "wink",
            "content": "<blockquote>\n<p>I’ve never understood the complaints. If you don’t like it, use something else: in Linux (and BSD) world you actually have a choice on the matter.</p>\n</blockquote>\n<p>That’s not helpful or true. Sure, if your only criterion is “this is a distro/version of Linux or BSD” - but that’s usually not the case. Even more, the whole “pick what you want, mix and match” has seen a huge downturn since systemd has been introduced in most distros. Take an example of someone using (and contributing) to Debian for 20 years. “Use something else” helps exactly no one. Not the user in question and not the people who rely on their packages not being maintained anymore. Speaking of a hypothetical Debian dev who was against introducing systemd. I’m sure we could dig up the MLs at the time of the decision and zero of those existed ;)</p>\n<p>I’m not even saying that’s a bad thing per se, and maybe the combos that work together work together much better than 20 years ago, but it’s a fact that there is less choice now.</p>",
            "createdAt": "2025-07-10 09:48:52-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "robey",
        "content": "<p>It’s hard to judge if binary log files are better than normal text files, because they don’t work yet: <a href=\"https://github.com/systemd/systemd/issues/2460\" rel=\"ugc\">https://github.com/systemd/systemd/issues/2460</a></p>\n<p>The article quotes someone pointing out that openrc is at <em>least</em> as good at service management and doesn’t try to infiltrate the rest of the system, but the author has no counter-argument other than “shut up”. Using Alex Jones to finish is… certainly a choice. Not one I’d make if I wanted to be taken seriously, though.</p>\n<p>It’s too bad you can’t have a rational discussion about service management any more, because it always turns into this kind of team cheerleading, without admitting that everything has tradeoffs.</p>",
        "createdAt": "2025-07-10 12:37:49-0500",
        "replies": []
      },
      {
        "author": "singpolyma",
        "content": "<p>Honestly I’ve still almost never noticed systemd exists. It doesn’t change anything about my workflow and /etc/init.d/service restart still works on my system. I understand that’s because Debian maintainers wrote those to just call systemd. And every now and then someone didn’t and I have to look up how to restart the service. But while I have concerns about systemd as a developer, as a sysadmin it is not better or worse or anything. It’s just the same</p>",
        "createdAt": "2025-07-09 06:59:06-0500",
        "replies": []
      },
      {
        "author": "dubiouslittlecreature",
        "content": "<p>I agree with several criticisms of systemd’s design choices, and I don’t like the way it’s being treated as a mandatory dependency to the extent that compatibility layers are needed to run some software without it.</p>\n<p>That said, I do think at the very least they followed through on their design choices well. I can’t say they didn’t do their best most of the time.</p>\n<p>And also I really don’t care for the hyperbolic title of this blog post.</p>",
        "createdAt": "2025-07-10 04:29:16-0500",
        "replies": []
      },
      {
        "author": "ZicZacBee",
        "content": "<p>One reason systemd won in the init market place not often touched on is that it handles dependencies explicitly, easily and correctly. Dependency management is also why systemd-* exist. Many types of units are likely to need the network or a resolver before the start.</p>\n<p>It seems to me so many distros jumped to systemd at the same time because it became mature and solved very real deficiencies with the status quo.</p>\n<p>I definitely didn’t like systemd when I first saw it.</p>",
        "createdAt": "2025-07-10 09:51:16-0500",
        "replies": []
      }
    ]
  },
  {
    "postTitle": "The Tree Borrows paper is finally published",
    "comments": [
      {
        "author": "BD103",
        "content": "<p>Definitely also check out <a href=\"https://perso.crans.org/vanille/treebor/index.html\" rel=\"ugc\">https://perso.crans.org/vanille/treebor/index.html</a>, which was written by one of the authors of the paper to explain Tree Borrows. Pretty cool!</p>",
        "createdAt": "2025-07-09 15:03:49-0500",
        "replies": []
      },
      {
        "author": "xfbs",
        "content": "<p>I’m excited to give this one a read. It fixes some limitations that the Rust borrow checker has currently, and it has pretty graphics!</p>",
        "createdAt": "2025-07-09 05:17:09-0500",
        "replies": []
      },
      {
        "author": "ecmm",
        "content": "<p>Congrats to Prof. Jung (and co-authors)! I’m excited to read it.</p>",
        "createdAt": "2025-07-09 11:21:07-0500",
        "replies": []
      },
      {
        "author": "benl",
        "content": "<p>Congrats!</p>",
        "createdAt": "2025-07-09 23:50:49-0500",
        "replies": []
      },
      {
        "author": "casperin",
        "content": "<p>Am I the only one who got amazingly confused by the title? You can perhaps argue that humans borrow paper from trees. But trees borrowing paper?? No.</p>",
        "createdAt": "2025-07-10 12:48:10-0500",
        "replies": []
      }
    ]
  },
  {
    "postTitle": "Using fs_usage to see what files a process is using",
    "comments": [
      {
        "author": "zmitchell",
        "content": "<p>I suspect this is a DTrace script under the hood (it reports thread IDs instead of PIDs, which smells like DTrace/bpftrace). I noticed that my <code>fseventsd</code> daemon was using a ton of CPU/memory, ran the tool for a few seconds, and out of ~150k events about ~130k were <code>ps</code> processes doing who knows what. Seems like that shouldn’t be happening, but <code>fs_usage</code> worked pretty well for quickly inspecting filesystem events.</p>",
        "createdAt": "2025-07-09 19:34:19-0500",
        "replies": [
          {
            "author": "rtpg",
            "content": "<p>I love using dtrace for this stuff. I really am not a fan of the SIP stuff on MacOS around dtrace though. I get that you could poke giant holes through your security model with <code>dtrace</code> but I just want to see what my processes are doing, and sometimes I just don’t know which process to care about!</p>",
            "createdAt": "2025-07-10 00:40:06-0500",
            "replies": []
          },
          {
            "author": "lgerbarg",
            "content": "<p>It is not. It predates <code>DTrace</code>, and uses the <code>ktrace</code> interface.</p>",
            "createdAt": "2025-07-10 15:32:50-0500",
            "replies": []
          }
        ]
      },
      {
        "author": "wink",
        "content": "<p>Semi-related: I’ve deployed a copy of this standalone python script to all my servers for about 10 years: <a href=\"https://github.com/klausman/lib_users\" rel=\"ugc\">https://github.com/klausman/lib_users</a></p>\n<blockquote>\n<p>Lib_users is a Python script that goes through /proc/*/maps and finds all cases of libraries being mapped but marked as deleted. It then extracts the programs name and arguments from /proc/<!-- raw HTML omitted -->/cmdline. This information is presented to the user so that those processes can be restarted.</p>\n</blockquote>\n<p>Maybe there is a better or easier way, but it just works[tm] for identifying services I should manually restart after some package upgrades that did not restart them.</p>",
        "createdAt": "2025-07-10 04:19:08-0500",
        "replies": []
      },
      {
        "author": "simonw",
        "content": "<p>I’ve been trying to find a reliable recipe for running short-lived commands like <code>git status</code> and seeing which files they touch while they are running.</p>\n<p>The <code>fs_usage</code> trick needs a PID. I tried running a shell script that printed its PID and waited for me to hit a key before replacing itself with the command (preserving the same PID) but couldn’t get that to work properly.</p>",
        "createdAt": "2025-07-09 22:16:19-0500",
        "replies": [
          {
            "author": "llimllib",
            "content": "<p>You could do something like this with <code>eslogger</code> and <code>jq</code> to only print out files stat-ed by a process named “git”:</p>\n<pre><code>$ sudo eslogger stat | jq 'select(.process.executable.path | test(\"/git$\")) | .event.stat.target.path' </code></pre>\n<p>eslogger takes a list of actions to monitor, so you can use “open” or “write” in addition to or to replace “stat” there, and there’s lots more info in the event object that you could filter on.</p>\n<p>Use <code>--list-events</code> to list the events it can handle. From my note here: <a href=\"https://notes.billmill.org/computer_usage/mac_os/debugging_os_x.html\" rel=\"ugc\">https://notes.billmill.org/computer_usage/mac_os/debugging_os_x.html</a></p>",
            "createdAt": "2025-07-10 01:53:50-0500",
            "replies": []
          },
          {
            "author": "gpm",
            "content": "<p>This sounds like a job for strace… e.g. <code>strace -e openat git status</code>.</p>\n<p>Admittedly I don’t know a complete syscall list that gets every syscall that accesses a new file…</p>",
            "createdAt": "2025-07-09 23:46:59-0500",
            "replies": [
              {
                "author": "runejuhl",
                "content": "<blockquote>\n<p>strace -e openat git status</p>\n</blockquote>\n<p>I think you can just use the <code>-e file</code> flag instead. From the <code>strace(1)</code> man page:</p>\n<pre><code>-e trace=syscall_set -e t=syscall_set --trace=syscall_set Trace only the specified set of system calls. syscall_set is defined as [!]value[,value], and value can be one of the following: (...snip...) %file file Trace all system calls which take a file name as an argument. You can think of this as an abbreviation for -e trace=open,stat,chmod,un‐ link,... which is useful to seeing what files the process is referencing. Furthermore, using the abbreviation will ensure that you don't ac‐ cidentally forget to include a call like lstat(2) in the list. Betchya woulda forgot that one. The syntax without a preceding per‐ cent sign (\"-e trace=file\") is deprecated. </code></pre>\n<p>Here’s a truncated example from one of my own repos:</p>\n<pre><code>$ strace -e file git status execve(\"/usr/bin/git\", [\"git\", \"status\"], 0x7ffdfe6250f8 /* 68 vars */) = 0 access(\"/etc/ld.so.preload\", R_OK) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/lib/x86_64-linux-gnu/libpcre2-8.so.0\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/lib/x86_64-linux-gnu/libz.so.1\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/lib/x86_64-linux-gnu/libc.so.6\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/dev/null\", O_RDWR) = 3 openat(AT_FDCWD, \"/usr/lib/locale/locale-archive\", O_RDONLY|O_CLOEXEC) = 3 newfstatat(AT_FDCWD, \"/usr/share/locale\", {st_mode=S_IFDIR|0755, st_size=4096, ...}, 0) = 0 access(\"/etc/gitconfig\", R_OK) = -1 ENOENT (No such file or directory) access(\"/home/runejuhl/.config/git/config\", R_OK) = 0 openat(AT_FDCWD, \"/home/runejuhl/.config/git/config\", O_RDONLY) = 3 access(\"/home/runejuhl/.config/git/config.local\", R_OK) = 0 openat(AT_FDCWD, \"/home/runejuhl/.config/git/config.local\", O_RDONLY) = 4 access(\"/home/runejuhl/.gitconfig\", R_OK) = -1 ENOENT (No such file or directory) getcwd(\"/home/runejuhl/git/unusual-ansible\", 129) = 59 getcwd(\"/home/runejuhl/git/unusual-ansible\", 129) = 59 newfstatat(AT_FDCWD, \"/home/runejuhl/git/unusual-ansible\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, 0) = 0 newfstatat(AT_FDCWD, \"/home/runejuhl/git/unusual-ansible/.git\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, 0) = 0 newfstatat(AT_FDCWD, \"/home/runejuhl/git/unusual-ansible/.git/HEAD\", {st_mode=S_IFREG|0664, st_size=23, ...}, AT_SYMLINK_NOFOLLOW) = 0 openat(AT_FDCWD, \"/home/runejuhl/git/unusual-ansible/.git/HEAD\", O_RDONLY) = 3 newfstatat(AT_FDCWD, \"/home/runejuhl/git/unusual-ansible/.git/commondir\", 0x7ffcdfa11570, AT_SYMLINK_NOFOLLOW) = -1 ENOENT (No such file or directory) access(\"/home/runejuhl/git/unusual-ansible/.git/objects\", X_OK) = 0 access(\"/home/runejuhl/git/unusual-ansible/.git/refs\", X_OK) = 0 newfstatat(AT_FDCWD, \"/home/runejuhl/git/unusual-ansible\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"/home/runejuhl/git/unusual-ansible/.git\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \".git/commondir\", 0x7ffcdfa11640, AT_SYMLINK_NOFOLLOW) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \".git/config\", O_RDONLY) = 3 getcwd(\"/home/runejuhl/git/unusual-ansible\", 129) = 59 newfstatat(AT_FDCWD, \".git\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, 0) = 0 newfstatat(AT_FDCWD, \".git/commondir\", 0x7ffcdfa11640, AT_SYMLINK_NOFOLLOW) = -1 ENOENT (No such file or directory) newfstatat(AT_FDCWD, \"/home\", {st_mode=S_IFDIR|0755, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"/home/runejuhl\", {st_mode=S_IFDIR|0750, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"/home/runejuhl/git\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"/home/runejuhl/git/unusual-ansible\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 access(\"/etc/gitconfig\", R_OK) = -1 ENOENT (No such file or directory) access(\"/home/runejuhl/.config/git/config\", R_OK) = 0 openat(AT_FDCWD, \"/home/runejuhl/.config/git/config\", O_RDONLY) = 3 access(\"/home/runejuhl/.config/git/config.local\", R_OK) = 0 openat(AT_FDCWD, \"/home/runejuhl/.config/git/config.local\", O_RDONLY) = 4 getcwd(\"/home/runejuhl/git/unusual-ansible\", 129) = 59 newfstatat(AT_FDCWD, \"/home/runejuhl/git/unusual-ansible/.git\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 getcwd(\"/home/runejuhl/git/unusual-ansible\", 129) = 59 access(\"/home/runejuhl/.gitconfig\", R_OK) = -1 ENOENT (No such file or directory) access(\".git/config\", R_OK) = 0 openat(AT_FDCWD, \".git/config\", O_RDONLY) = 3 getcwd(\"/home/runejuhl/git/unusual-ansible\", 129) = 59 chdir(\"/home/runejuhl/git/unusual-ansible\") = 0 access(\"/etc/gitconfig\", R_OK) = -1 ENOENT (No such file or directory) access(\"/home/runejuhl/.config/git/config\", R_OK) = 0 openat(AT_FDCWD, \"/home/runejuhl/.config/git/config\", O_RDONLY) = 3 access(\"/home/runejuhl/.config/git/config.local\", R_OK) = 0 openat(AT_FDCWD, \"/home/runejuhl/.config/git/config.local\", O_RDONLY) = 4 getcwd(\"/home/runejuhl/git/unusual-ansible\", 129) = 59 newfstatat(AT_FDCWD, \"/home/runejuhl/git/unusual-ansible/.git\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 getcwd(\"/home/runejuhl/git/unusual-ansible\", 129) = 59 access(\"/home/runejuhl/.gitconfig\", R_OK) = -1 ENOENT (No such file or directory) access(\".git/config\", R_OK) = 0 openat(AT_FDCWD, \".git/config\", O_RDONLY) = 3 newfstatat(AT_FDCWD, \".git/commondir\", 0x7ffcdfa116a0, AT_SYMLINK_NOFOLLOW) = -1 ENOENT (No such file or directory) newfstatat(AT_FDCWD, \".git/HEAD\", {st_mode=S_IFREG|0664, st_size=23, ...}, AT_SYMLINK_NOFOLLOW) = 0 openat(AT_FDCWD, \".git/HEAD\", O_RDONLY) = 3 newfstatat(AT_FDCWD, \".git/refs/heads/public\", {st_mode=S_IFREG|0664, st_size=41, ...}, AT_SYMLINK_NOFOLLOW) = 0 openat(AT_FDCWD, \".git/refs/heads/public\", O_RDONLY) = 3 newfstatat(AT_FDCWD, \".git/MERGE_HEAD\", 0x7ffcdfa11890, AT_SYMLINK_NOFOLLOW) = -1 ENOENT (No such file or directory) newfstatat(AT_FDCWD, \".git/CHERRY_PICK_HEAD\", 0x7ffcdfa11660, AT_SYMLINK_NOFOLLOW) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \".git/packed-refs\", O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \".git/index\", O_RDONLY) = 3 newfstatat(AT_FDCWD, \".envrc\", {st_mode=S_IFREG|0664, st_size=59, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \".gitignore\", {st_mode=S_IFREG|0664, st_size=81, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"Dockerfile.de-fixed\", {st_mode=S_IFREG|0664, st_size=589, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"README.org\", {st_mode=S_IFREG|0664, st_size=5530, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"ansible.cfg\", {st_mode=S_IFREG|0664, st_size=184, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"decision_environments\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"decision_environments/fixed.yml\", {st_mode=S_IFREG|0664, st_size=659, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"decision_environments/minimal.yml\", {st_mode=S_IFREG|0664, st_size=470, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"ed25519.pub\", {st_mode=S_IFREG|0664, st_size=96, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"group_vars\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"group_vars/all\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"group_vars/all/common.yml\", {st_mode=S_IFREG|0664, st_size=98, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"group_vars/all/vault.yml\", {st_mode=S_IFREG|0664, st_size=6143, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"inventory\", {st_mode=S_IFREG|0664, st_size=139, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"pihole-mqtt\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"pihole-mqtt/Dockerfile.logstash\", {st_mode=S_IFREG|0664, st_size=98, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"pihole-mqtt/dns-pipeline.conf\", {st_mode=S_IFREG|0664, st_size=520, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"pihole-mqtt/docker-compose.yml\", {st_mode=S_IFREG|0664, st_size=452, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"playbooks\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"playbooks/dump_event.yml\", {st_mode=S_IFREG|0664, st_size=161, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"playbooks/hello-world.yml\", {st_mode=S_IFREG|0664, st_size=371, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"playbooks/loo_roll_empty.yml\", {st_mode=S_IFREG|0664, st_size=454, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"playbooks/make_drinks.yml\", {st_mode=S_IFREG|0664, st_size=331, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"playbooks/mqtt.yml\", {st_mode=S_IFREG|0664, st_size=304, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"playbooks/mqtt_clicker.yml\", {st_mode=S_IFREG|0664, st_size=872, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"playbooks/scare_kids.yml\", {st_mode=S_IFREG|0664, st_size=688, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"playbooks/send_notification.yml\", {st_mode=S_IFREG|0664, st_size=267, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"playbooks/unifi_throttle.yml\", {st_mode=S_IFREG|0664, st_size=1684, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"plugins\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"plugins/event_source\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"plugins/event_source/http_sse.py\", {st_mode=S_IFREG|0664, st_size=1176, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"plugins/event_source/mqtt.py\", {st_mode=S_IFREG|0664, st_size=1834, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"plugins/event_source/requirements.txt\", {st_mode=S_IFREG|0664, st_size=21, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"plugins/event_source/url_probe.py\", {st_mode=S_IFREG|0664, st_size=2309, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"presentations\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"presentations/2024-10-29 Red Hat Summit Connect 2024.pdf\", {st_mode=S_IFREG|0644, st_size=1015670, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"presentations/2024-12-05 Atea Tech Tour \\303\\205rhus - Rune Juhl.pdf\", {st_mode=S_IFREG|0644, st_size=1964746, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"requirements.txt\", {st_mode=S_IFREG|0664, st_size=725, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"requirements.yml\", {st_mode=S_IFREG|0664, st_size=74, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"rulebooks\", {st_mode=S_IFDIR|0775, st_size=4096, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"rulebooks/friday_afternoon_drinks-cli.yml\", {st_mode=S_IFREG|0664, st_size=547, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"rulebooks/friday_afternoon_drinks.yml\", {st_mode=S_IFREG|0664, st_size=1188, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"rulebooks/hass_websocket.yml\", {st_mode=S_IFREG|0664, st_size=524, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"rulebooks/hello-world.yml\", {st_mode=S_IFREG|0664, st_size=675, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"rulebooks/http_sse.yml\", {st_mode=S_IFREG|0664, st_size=343, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"rulebooks/http_sse_loo_roll.yml\", {st_mode=S_IFREG|0664, st_size=809, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"rulebooks/mqtt-cli.yml\", {st_mode=S_IFREG|0664, st_size=389, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"rulebooks/mqtt.yml\", {st_mode=S_IFREG|0664, st_size=447, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"rulebooks/mqtt_clicker.yml\", {st_mode=S_IFREG|0664, st_size=538, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"rulebooks/mqtt_dns.yml\", {st_mode=S_IFREG|0664, st_size=1140, ...}, AT_SYMLINK_NOFOLLOW) = 0 newfstatat(AT_FDCWD, \"scare_kids.yml\", {st_mode=S_IFREG|0664, st_size=688, ...}, AT_SYMLINK_NOFOLLOW) = 0 openat(AT_FDCWD, \"/usr/share/locale/locale.alias\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/usr/share/locale/en_US/LC_MESSAGES/git.mo\", O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \"/usr/share/locale/en/LC_MESSAGES/git.mo\", O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \"/usr/share/locale-langpack/en_US/LC_MESSAGES/git.mo\", O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \"/usr/share/locale-langpack/en/LC_MESSAGES/git.mo\", O_RDONLY) = -1 ENOENT (No such file or directory) readlink(\".git/index\", 0x568b7ba41840, 32) = -1 EINVAL (Invalid argument) getcwd(\"/home/runejuhl/git/unusual-ansible\", 129) = 59 openat(AT_FDCWD, \"/home/runejuhl/git/unusual-ansible/.git/index.lock\", O_RDWR|O_CREAT|O_EXCL|O_CLOEXEC, 0666) = 3 newfstatat(AT_FDCWD, \".git/HEAD\", {st_mode=S_IFREG|0664, st_size=23, ...}, AT_SYMLINK_NOFOLLOW) = 0 openat(AT_FDCWD, \".git/HEAD\", O_RDONLY) = 4 </code></pre>",
                "createdAt": "2025-07-10 03:38:02-0500",
                "replies": []
              },
              {
                "author": "wink",
                "content": "<p>Is there a way for it to not have a 30-50% speed penaly though?</p>\n<p>My first guess would have been dtrace, but surely the linux kernel by now would have something better and lightweight than strace? (or does -e not incur the full penalty?)</p>\n<p>Edit: this looks promising <a href=\"https://piware.de/post/2020-02-28-bpftrace/\" rel=\"ugc\">https://piware.de/post/2020-02-28-bpftrace/</a> - or fanotify</p>",
                "createdAt": "2025-07-10 04:27:22-0500",
                "replies": []
              }
            ]
          },
          {
            "author": "rtpg",
            "content": "<p>This might be SIP related, I don’t know if <code>fs_usage</code> is affected by it but <code>dtrace</code> on stuff like <code>/usr/bin/git</code> seems not allowed on MacOS</p>",
            "createdAt": "2025-07-10 00:44:57-0500",
            "replies": [
              {
                "author": "junon",
                "content": "<p>Yep, most likely. SIP crippled a bunch of debug tools when it was introduced.</p>",
                "createdAt": "2025-07-10 09:38:42-0500",
                "replies": []
              }
            ]
          },
          {
            "author": "jclulow",
            "content": "<p>Perhaps you could use a debugger to start the command with a breakpoint set on <code>main()</code>, which would presumably pause execution very early in the life of the program. You could then get the pid, attach the other tool, and have the debugger resume execution.</p>",
            "createdAt": "2025-07-10 01:15:42-0500",
            "replies": []
          },
          {
            "author": "zmitchell",
            "content": "<p>I suspect this is job for a Python script, and this is what I would try:</p>\n<ul>\n<li>Spawn short lived process. This typically returns an object that can report a PID.</li>\n<li>Immediately send a job control signal to pause the process.</li>\n<li>Start the fs_usage process pointed at the PID.</li>\n<li>Send another job control signal to resume the process.</li>\n</ul>\n<p>Definitely racy and will depend on just how short lived that process is, but maybe <em>more</em> reliable than other things you’ve tried?</p>\n<p>I’ve done stuff like the debugger trick mentioned in another comment, but that begins to feel unwieldy pretty quick.</p>",
            "createdAt": "2025-07-10 02:34:24-0500",
            "replies": []
          }
        ]
      }
    ]
  },
  {
    "postTitle": "Vibe Coding Casino: Rise of programming by slot machine",
    "comments": [
      {
        "author": "evrimzone",
        "content": "<p>“An ever-growing drive towards exploiting gambling instincts is being employed by AI coding agents and the companies developing them. The addictive nature of low-effort to high-output environment of LLM interfaces is being actively monetized, and incentives are once again stacked against you.”</p>\n<p>I decided to write about my experiences and observations about one of the less-discussed aspects of vibe coding: Its direct relationship to gambling, and how it’s being marketed to inflate the AI hype cycle to the MOON!</p>\n<p>I tried to be a little bit inflammatory, but please consider it seriously. Our relationship with AI agents is going to be an increasingly important one as they grow smarter. It’s up to us to keep our instincts in check, and stay cool.</p>\n<p>P.S. I would like to get in touch with people who think about this subject, are addicted to vibe coding and want to quit, and would like to provide more information/resources to substantiate this discussion, and expose the parties exploiting these addiction systems in the context of vibe coding.</p>",
        "createdAt": "2025-07-10 12:08:56-0500",
        "replies": []
      },
      {
        "author": "mbil",
        "content": "<p>I agree with the premise but not necessarily the conclusion that it’s bad. Personally, the itch that vibecoding scratches is not unlike what I felt when I first learned regular programming. I wrote about my own experiences here <a href=\"https://matthewbilyeu.com/blog/2025-06-14/vibecoding-s-allure-the-inventor-and-the-fiend\" rel=\"ugc\">https://matthewbilyeu.com/blog/2025-06-14/vibecoding-s-allure-the-inventor-and-the-fiend</a></p>",
        "createdAt": "2025-07-10 12:34:12-0500",
        "replies": [
          {
            "author": "evrimzone",
            "content": "<p>I think you can practice software curiosity safely while also practicing the mantras I listed. I also don’t think that my conclusion was that it’s <em>bad</em>, I’m simply pointing out that it’s ripe for exploitation and there’s Big Money involved in it now.</p>\n<p>As to the similarities between vibe coding and old school hacking like you used to: It’s pretty much the same reward mechanisms. I would extend the compulsion loop argument to the proliferation of computers altogether. We simply found them so very interesting and useful that in fifty years they went from offloading arithmetic work to being <em>the</em> infrastructure for the global economical, cultural, and political establishments.</p>\n<p>I’d thus argue that we wouldn’t have gotten there if <em>programming</em> as you describe it wasn’t addicting, in that dopaminergic manner. A lot of ‘work’ is like that, it’s interesting and makes us curious! You should always be wary when somebody exploits your curiosity for <em>their own</em> gain, however.</p>",
            "createdAt": "2025-07-10 13:04:46-0500",
            "replies": []
          }
        ]
      }
    ]
  },
  {
    "postTitle": "VibeTunnel - access terminal from the browser to command agents",
    "comments": [
      {
        "author": "dubiouslittlecreature",
        "content": "<p>Surely combining the complexity of a browser with access to a local shell won’t result in new and horrifying security issues.</p>\n<p>I wonder how long until someone gets pwned because they had this set up insecurely or got tricked into installing a sketchy extension or something.</p>",
        "createdAt": "2025-07-01 03:58:32-0500",
        "replies": [
          {
            "author": "mitsuhiko",
            "content": "<p>You have to go out of your way to run it without password protection. I’m however curious what exactly your concern is here in particular with the way we handle security.</p>",
            "createdAt": "2025-07-01 04:36:25-0500",
            "replies": [
              {
                "author": "mrexodia",
                "content": "<p>Just wanted to say: thank you for making this, I have been looking for something exactly like this!</p>\n<p>If someone exposes this to the internet with a weak password that’s on them 🤷‍♂️</p>",
                "createdAt": "2025-07-10 11:18:32-0500",
                "replies": []
              },
              {
                "author": "dubiouslittlecreature",
                "content": "<p>In general, my issue is that by allowing terminal access via a web browser, you’ve exposed the user’s local shell to all the attack surface of a browser.</p>\n<p>More specifically I am concerned about someone downloading a sketchy extension (e.g. “free AI coding tool!”) and that extension secretly checking for VibeTunnel windows. And then doing nefarious things in the background.</p>",
                "createdAt": "2025-07-01 13:00:06-0500",
                "replies": []
              }
            ]
          }
        ]
      }
    ]
  },
  {
    "postTitle": "Vim Fugitive in action (2021)",
    "comments": [
      {
        "author": "vimpostor",
        "content": "<p>This is a really well written article. I use fugitive in a similar way, the per-line staging feature is a much needed supplement to the horrible UX of <code>git add -p</code>.</p>\n<p>Regarding merge conflicts I reserve the full blown <code>git mergetool</code> for only the most complicated merge conflicts, the <a href=\"https://dzx.fr/blog/introduction-to-vim-fugitive/#with-fugitive\" rel=\"ugc\">:G mergetool</a> approach for just populating the quickfixlist with merge conflict markers (then manually editing) seems favorable to me.</p>\n<p>Actually coming to think about it, I almost always prefer this approach and I have solved my fair share of merge conflicts in the past years. Does anyone (using vim or not) even enjoy using 3-way diff solvers? To me it’s always so confusing which side is which, as it is inverted between merge and rebase, plus I can usually understand the context the fastest if I just see everything together.</p>",
        "createdAt": "2025-07-10 04:20:06-0500",
        "replies": [
          {
            "author": "dominicm",
            "content": "<p><a href=\"https://github.com/christoomey/vim-conflicted\" rel=\"ugc\">vim-conflicted</a> provides a very nice interface for resolving merge conflicts. It’s dependent on fugitive, and solves the problem of understanding what changed upstream that you need to incorporate into your patch’s changes.</p>",
            "createdAt": "2025-07-10 10:58:14-0500",
            "replies": []
          },
          {
            "author": "bhoot",
            "content": "<blockquote>\n<p>per-line staging feature is a much needed supplement to the horrible UX of git add -p</p>\n</blockquote>\n<p><code>git add -e</code> (or, of course, <code>git stage -e</code>) provides the real per-line staging – it opens the unstaged diffs in your editor. Of course, vim-fugitive probably has a better UX at least until one gets used to the interface provided by <code>git add -e</code>.</p>\n<p>I think of <code>git add -p</code> for per-hunk staging.</p>",
            "createdAt": "2025-07-10 05:19:10-0500",
            "replies": [
              {
                "author": "vimpostor",
                "content": "<blockquote>\n<p>git add -e provides the real per-line staging</p>\n</blockquote>\n<p>I would argue that <code>git add -e</code> has even worse UX than <code>git add -p</code> as you are literally editing the diff directly. There is a reason it got an entire warning section on its own in the man page.</p>\n<p>Granted, it’s faster than splitting a hunk repeatedly, but you gotta ask yourself if you are seriously considering it as an alternative to real per-line staging, when even kernel maintainers have to <a href=\"http://www.kroah.com/log/blog/2019/08/14/patch-workflow-with-mutt-2019/\" rel=\"ugc\">brag about being able to use the damn thing</a>:</p>\n<blockquote>\n<p>I want a Linkedin skill badge for “can edit diff files by hand and have them still work”</p>\n</blockquote>\n<p>Of course Greg hand-crafting additions is a bit more risky operation than just sculpting away lines from the staging diff, but man it still sounds like you are just constantly one typo away from fuzzy-testing a new CVE in git’s diff parser. Also you are really doing the inverse of per-line staging, you are removing lines until you are left with the lines you want to stage, which can take considerably longer than per-line staging.</p>",
                "createdAt": "2025-07-10 07:56:22-0500",
                "replies": [
                  {
                    "author": "bhoot",
                    "content": "<p>Oh I have no point in favour of the UX of <code>git add -e</code>.</p>\n<p>I was only pointing out that hunks quite often fail to achieve per-line granularity.</p>",
                    "createdAt": "2025-07-10 13:41:58-0500",
                    "replies": []
                  }
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  {
    "postTitle": "What's //go:nosplit for?",
    "comments": [
      {
        "author": "Riolku",
        "content": "<p>Very good writeup, thanks for sharing</p>",
        "createdAt": "2025-07-10 12:38:01-0500",
        "replies": []
      },
      {
        "author": "peterbourgon",
        "content": "<p><a href=\"https://github.com/golang/go/issues/74478#issuecomment-3046285808\" rel=\"ugc\">https://github.com/golang/go/issues/74478#issuecomment-3046285808</a></p>\n<blockquote>\n<p>I have smashed the stack by using a publicly-documented language feature that does not explicitly say it will produce memory errors.</p>\n</blockquote>\n<p><code>//go:nosplit</code> is a compiler directive that’s only documented in the context of compiler and runtime internals. It’s not part of the language spec, nor part of any kind of user-facing contract. It’s in no way intended or supported for general use, and AFAIU not a language feature in the sense that’s suggested here.</p>",
        "createdAt": "2025-07-10 14:38:42-0500",
        "replies": []
      }
    ]
  }
]